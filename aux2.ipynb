{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#üìö-Objetivos-de-la-clase-üìö\" data-toc-modified-id=\"üìö-Objetivos-de-la-clase-üìö-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>üìö Objetivos de la clase üìö</a></span></li><li><span><a href=\"#Motivaci√≥n\" data-toc-modified-id=\"Motivaci√≥n-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Motivaci√≥n</a></span><ul class=\"toc-item\"><li><span><a href=\"#El-gran-problema-de-Bag-of-Words\" data-toc-modified-id=\"El-gran-problema-de-Bag-of-Words-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>El gran problema de Bag of Words</a></span></li><li><span><a href=\"#Hip√≥tesis-Distribucional\" data-toc-modified-id=\"Hip√≥tesis-Distribucional-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Hip√≥tesis Distribucional</a></span></li><li><span><a href=\"#Word-Context-Matrices\" data-toc-modified-id=\"Word-Context-Matrices-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Word-Context Matrices</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Word Embeddings</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Word2vec-y-Skip-gram\" data-toc-modified-id=\"Word2vec-y-Skip-gram-2.4.0.1\"><span class=\"toc-item-num\">2.4.0.1&nbsp;&nbsp;</span>Word2vec y Skip-gram</a></span></li></ul></li><li><span><a href=\"#Detalles-del-Modelo\" data-toc-modified-id=\"Detalles-del-Modelo-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Detalles del Modelo</a></span></li></ul></li><li><span><a href=\"#La-capa-Oculta-y-los-Embeddings\" data-toc-modified-id=\"La-capa-Oculta-y-los-Embeddings-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>La capa Oculta y los Embeddings</a></span></li><li><span><a href=\"#Fuentes\" data-toc-modified-id=\"Fuentes-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Fuentes</a></span></li></ul></li><li><span><a href=\"#Entrenar-nuestros-Embeddings\" data-toc-modified-id=\"Entrenar-nuestros-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Entrenar nuestros Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset-y-limpiar\" data-toc-modified-id=\"Cargar-el-dataset-y-limpiar-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Cargar el dataset y limpiar</a></span></li><li><span><a href=\"#Extracci√≥n-de-Frases\" data-toc-modified-id=\"Extracci√≥n-de-Frases-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Extracci√≥n de Frases</a></span></li><li><span><a href=\"#Definir-el-modelo\" data-toc-modified-id=\"Definir-el-modelo-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Definir el modelo</a></span></li><li><span><a href=\"#Construir-el-vocabulario\" data-toc-modified-id=\"Construir-el-vocabulario-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Construir el vocabulario</a></span></li><li><span><a href=\"#Entrenar-el-Modelo\" data-toc-modified-id=\"Entrenar-el-Modelo-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Entrenar el Modelo</a></span></li><li><span><a href=\"#Guardar-y-cargar-el-modelo\" data-toc-modified-id=\"Guardar-y-cargar-el-modelo-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Guardar y cargar el modelo</a></span></li></ul></li><li><span><a href=\"#Tasks:-Palabras-mas-similares-y-Analog√≠as\" data-toc-modified-id=\"Tasks:-Palabras-mas-similares-y-Analog√≠as-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Tasks: Palabras mas similares y Analog√≠as</a></span><ul class=\"toc-item\"><li><span><a href=\"#Palabras-mas-similares\" data-toc-modified-id=\"Palabras-mas-similares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Palabras mas similares</a></span></li><li><span><a href=\"#Analog√≠as\" data-toc-modified-id=\"Analog√≠as-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Analog√≠as</a></span></li><li><span><a href=\"#Visualizar\" data-toc-modified-id=\"Visualizar-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Visualizar</a></span></li></ul></li><li><span><a href=\"#Word-Embeddings-como-caracter√≠sticas-para-clasificar\" data-toc-modified-id=\"Word-Embeddings-como-caracter√≠sticas-para-clasificar-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Word Embeddings como caracter√≠sticas para clasificar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset\" data-toc-modified-id=\"Cargar-el-dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Cargar el dataset</a></span></li><li><span><a href=\"#Dividir-el-dataset-en-training-y-test\" data-toc-modified-id=\"Dividir-el-dataset-en-training-y-test-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Dividir el dataset en training y test</a></span></li><li><span><a href=\"#Doc2vec\" data-toc-modified-id=\"Doc2vec-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Doc2vec</a></span></li><li><span><a href=\"#Definimos-el-pipeline\" data-toc-modified-id=\"Definimos-el-pipeline-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Definimos el pipeline</a></span></li></ul></li><li><span><a href=\"#Usandolo-con-BoW\" data-toc-modified-id=\"Usandolo-con-BoW-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Usandolo con BoW</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bias-en-Embeddings\" data-toc-modified-id=\"Bias-en-Embeddings-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Bias en Embeddings</a></span></li><li><span><a href=\"#Propuesto...\" data-toc-modified-id=\"Propuesto...-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Propuesto...</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T21:20:42.007937Z",
     "start_time": "2020-05-05T21:20:41.987638Z"
    }
   },
   "source": [
    "# Auxiliar 2 - Word Embeddings\n",
    "\n",
    "\n",
    "## üìö Objetivos de la clase üìö\n",
    "\n",
    "La clase auxiliar de esta semana tendr√° varios objetivos: \n",
    "\n",
    "- Motivaci√≥n y repaso de qu√© son los Word Embeddings. \n",
    "- Entrenar nuestros propios `word embeddings` usando el dataset de noticias de la radio biobio. \n",
    "- Experimentar y visualizar los embeddings entrenados. \n",
    "- Por √∫ltimo, resolver la misma tarea de la clase auxiliar 1, pero ahora usando los embeddings que hemos calculado. \n",
    "\n",
    "\n",
    "Para desarrollar ustedes mismos el auxiliar, necesitar√°n los siguientes paquetes: \n",
    "\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `gensim`\n",
    "- `plotly`\n",
    "- `sklearn`\n",
    "- `wefe`\n",
    "\n",
    "Una vez resuelto, pueden utilizar cualquier parte del c√≥digo que les parezca prudente para la tarea 1 (que tambi√©n es de clasificaci√≥n de texto! üòä).\n",
    "\n",
    "\n",
    "El notebook del auxiliar ya ejecutado se encuentra en el [github](https://github.com/dccuchile/CC6205/tree/master/tutorials) del curso (Recuerden dejar su Star ‚≠êüòâ!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivaci√≥n\n",
    "\n",
    "### El gran problema de Bag of Words\n",
    "\n",
    "Pensemos en estas 3 frases como documentos:\n",
    "\n",
    "- $doc_1$: `¬°Buen√≠sima la marraqueta!`\n",
    "- $doc_2$: `¬°Estuvo espectacular ese pan franc√©s!`\n",
    "- $doc_3$: `!Buen√≠sima esa pintura!`\n",
    "\n",
    "Sabemos $doc_1$ y $doc_2$ hablan de lo mismo üçûüçûüëå y que $doc_3$ üé® no tiene mucho que ver con los otros.\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "Para esto, generamos un modelo `Bag of Words` sobre el documento. Es decir, transformamos cada palabra a un vector one-hot y luego los sumamos por documento. Adem√°s, omitimos algunas stopwords y consideramos pan frances como un solo token.\n",
    "\n",
    "$$v = \\{buen√≠sima, marraqueta, estuvo, espectacular, pan\\ franc√©s, pintura\\}$$\n",
    "\n",
    "Entonces, el $\\vec{doc_1}$ quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "El $\\vec{doc_2}$ quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 0\\end{bmatrix} = \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el $\\vec{doc_3}$: \n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "**¬øCu√°l es el problema?**\n",
    "\n",
    "`buen√≠sima` $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan ideas muy similares. Por otra parte, sabemos que `marraqueta` $\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `pan franc√©s` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\0\\end{bmatrix}$ se refieren al mismo objeto. Pero en este modelo, estos **son totalmente distintos**. Es decir, los vectores de las palabras que `buen√≠sima` y `espectacular` son tan distintas como `marraqueta` y `pan franc√©s`. Esto evidentemente, repercute en la calidad de los modelos que creamos a partir de nuestro Bag of Words.\n",
    "\n",
    "Ahora, si queremos ver que documento es mas similar a otro usando distancia euclidiana, veremos que:\n",
    "\n",
    "$$d(doc_1, doc_2) = 2.236$$\n",
    "$$d(doc_1, doc_3) = 1.414$$\n",
    "\n",
    "Es decir, $doc_1$ se parece mas a $doc_3$ aunque nosotros sabemos que $doc_1$ y $doc_2$ nos est√°n diciendo lo mismo!\n",
    "\n",
    "\n",
    "Nos gustar√≠a que eso no sucediera. Que existiera alg√∫n m√©todo que nos permitiera hacer que palabras similares tengan representaciones similares. Y que con estas, representemos mejor a los documentos.\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hip√≥tesis Distribucional\n",
    "\n",
    "Estamos buscando alg√∫n enfoque que nos permita representar las palabras de forma no aislada, si no como algo que adem√°s capture el significado de esta.\n",
    "\n",
    "Pensemos un poco en la **hip√≥tesis distribucional**. Esta plantea que:\n",
    "\n",
    "    \"Palabras que ocurren en contextos iguales tienden a tener significados similares.\" \n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "    \"Una palabra es caracterizada por la compa√±√≠a que esta lleva.\"\n",
    "\n",
    "Esto nos puede hacer pensar que podr√≠amos usar los contextos de las palabras para generar vectores que describan mejor dichas palabras: en otras palabras, los `Distributional Vectors`.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Context Matrices\n",
    "\n",
    "Es una matriz donde cada celda $(i,j)$ representa la co-ocurrencia entre una palabra objetivo/centro $w_i$ y un contexto $c_j$. El contexto son las palabras dentro de ventana de tama√±o $k$ que rodean la palabra central. \n",
    "\n",
    "Cada fila representa a una palabra a trav√©s de su contexto. Como pueden ver, ya no es un vector one-hot, si no que ahora contiene mayor informaci√≥n.\n",
    "\n",
    "El tama√±o de la matriz es el tama√±o del vocabulario $V$ al cuadrado. Es decir $|V|*|V|$.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/pics/distributionalSocher.png\" alt=\"Word-context matrices\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Problema: Creada a partir de un corpus respetable, es gigantezca**. \n",
    "\n",
    "Por ejemplo, para $|v| = 100.000$, la matriz tendr√° $\\frac{100000 * 100000 * 4}{10^9} = 40gb $.\n",
    "\n",
    "- Es caro mantenerla en memoria \n",
    "- Los clasificadores no funcionan tan bien con tantas dimensiones (ver [maldici√≥n de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
    "\n",
    "¬øHabr√° una mejor soluci√≥n?\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "\n",
    "La idea principal de los Word Embeddings es crear representaciones vectoriales densas y de baja dimensionalidad $(d << |V|)$ de las palabras a partir de su contexto.  Para esto, se usan distintos modelos que emplean redes neuronales *shallow* o poco profundas.\n",
    "\n",
    "Volvamos a nuestro ejemplo anterior: `buen√≠sima` y `espectacular` ocurren muchas veces en el mismo contexto, por lo que los embeddings que los representan debiesen ser muy similares... (*ejemplos de mentira hechos a mano*):\n",
    "\n",
    "`buen√≠sima` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `marraqueta`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cu√°l es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¬øC√≥mo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "- Depender√° del modelo que utilizemos.\n",
    "\n",
    "\n",
    "##### Word2vec y Skip-gram\n",
    "\n",
    "Word2Vec es probablemente el paquete de software mas famoso para crear word embeddings. Este nos provee herramientas para crear distintos tipos de modelos, tales como `Skip-Gram` y `Continuous Bag of Word (CBOW)`. En este caso, solo veremos `Skip-Gram`.\n",
    "\n",
    "**Skip-gram** es una task auxiliar con la que crearemos nuestros embeddings. Esta consiste en que por cada palabra del dataset, predigamos las palabras de su contexto (las palabras presentes en ventana de alg√∫n tama√±o $k$).\n",
    "\n",
    "Para resolverla, usaremos una red de una sola capa oculta. Los pesos ya entrenados de esta capa ser√°n los que usaremos como embeddings.\n",
    "\n",
    "#### Detalles del Modelo\n",
    "\n",
    "- Como dijimos, el modelo ser√° una red de una sola capa. La capa oculta tendr√° una dimensi√≥n $d$ la cual nosotros determinaremos. Esta capa no tendr√° funci√≥n de activaci√≥n. Sin embargo, la de salida si, la cual ser√° una softmax.\n",
    "\n",
    "- El vector de entrada, de tama√±o $|V|$, ser√° un vector one-hot de la palabra que estemos viendo en ese momento.\n",
    "\n",
    "- La salida, tambi√©n de tama√±o $|V|$, ser√° un vector que contenga la distribuci√≥n de probabilidad de que cada palabra del vocabulario pertenezca al contexto de la palabra de entrada.\n",
    "\n",
    "- Al entrenar, se comparar√° la distribuci√≥n de los contextos con la suma de los vectores one-hot del contexto real.\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"Skip Gram\" style=\"width: 600px;\"/>\n",
    "\n",
    "Nota: Esto es computacionalmente una locura. Por cada palabra de entrada, debemos calcular la probabilidad de aparici√≥n de todas las otras. Imaginen el caso de un vocabulario de 100.000 de palabras y de 10000000 oraciones...\n",
    "\n",
    "La soluci√≥n a esto es modificar la task a *Negative Sampling*. Esta transforma este problema de $|V|$ clases a uno binario. Sin embargo, no lo veremos por el tiempo, pero est√°n muy bien explicado en el [video de la c√°tedra](https://www.youtube.com/watch?v=XDxzQ7JU95U&feature=youtu.be).\n",
    "\n",
    "\n",
    "### La capa Oculta y los Embeddings\n",
    "\n",
    "Al terminar el entrenamiento, ¬øQu√© nos queda en la capa oculta?\n",
    "\n",
    "Una matriz de $v$ filas por $d$ columnas, la cual contiene lo que buscabamos: Una representaci√≥n continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representaci√≥n continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" alt=\"Capa Oculta 1\" style=\"width: 400px;\"/>\n",
    "\n",
    "¬øC√≥mo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot de la entrada y las multiplicamos por la matriz:\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" alt=\"Skip Gram\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "Word2vec:\n",
    "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "Gensim: \n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "Nota: Las √∫ltimas 3 imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementaci√≥n de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:26:56.028758Z",
     "start_time": "2020-05-17T20:26:52.011925Z"
    }
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset y limpiar\n",
    "\n",
    "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.022904Z",
     "start_time": "2020-05-17T20:26:56.037686Z"
    }
   },
   "outputs": [],
   "source": [
    "# descargamos el dataset completo (~40mb)\n",
    "dataset = pd.read_json(\n",
    "    'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_clean.bz2',\n",
    "    encoding=\"utf-8\")\n",
    "\n",
    "dataset_r = dataset.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.136649Z",
     "start_time": "2020-05-17T20:27:38.050830Z"
    }
   },
   "outputs": [],
   "source": [
    "# unir titulo con contenido de la noticia\n",
    "content = dataset['title'] + dataset['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.183476Z",
     "start_time": "2020-05-17T20:27:38.175498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26413,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.227359Z",
     "start_time": "2020-05-17T20:27:38.221374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T21:21:12.916380Z",
     "start_time": "2020-05-07T21:21:04.087799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# limpiar puntuaciones y separar por tokens.\n",
    "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "\n",
    "\n",
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "\n",
    "    tokenized_doc = [\n",
    "        token for token in tokenized_doc if token.lower() not in stopwords\n",
    "    ]\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "print(punctuation)\n",
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T21:21:15.069544Z",
     "start_time": "2020-05-07T21:21:15.063560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de alguna noticia: ['Ministra', 'Cubillos', 'extensi√≥n', 'paro', 'docente', 'problema', 'propuesta', 'ministra', 'Educaci√≥n', 'Marcela', 'Cubillos', 'refiri√≥', 'a', 'votaci√≥n', 'realizada', 'interior', 'Colegio', 'Profesores', 'decidi√≥', 'mantener', 'paro', 'pese', 'llamado', 'presidente', 'gremio', 'Mario', 'Aguilar', 'a', 'replegarse', 'entrevista', 'programa', 'Expreso', 'B√≠o', 'B√≠o', 'Radio', 'ministra', 'valor√≥', 'dichos', 'Aguilar', 'previo', 'a', 'votaci√≥n', 'asegurando', 'efecto', 'positivo', 'respecto', 'vuelta', 'clases', 'Paro', 'profesores', 'extiende', 'semana', '255', 'votos', 'marcaron', 'diferencia', 'sufragio', 'seg√∫n', 'dijo', '95', 'colegios', 'funcionan', 'normalidad', 'ayer', 'bajaron', '490', 'colegios', 'quedaban', 'paro', 'Respecto', 'a', 'si', 'Ministerio', 'seguir√°', 'negociando', 'ofrecer√°', 'considerando', 'mayor√≠a', 'vot√≥', 'extender', 'movilizaci√≥n', 'ministra', 'limit√≥', 'a', 'decir', 'esperar√°n', 'resuelva', 'di√°logos', 'internos', 'llevando', 'a', 'cabo', 'interior', 'gremio', 'l√≠nea', 'sostuvo', 'decisi√≥n', 'presidente', 'colegio', 'punto', 'negociaci√≥n', 'llamando', 'a', 'aceptar', 'propuesta', 'problema', 'propuesta', 'esperamos', 'opiniones', 'interior', 'colegio', 'decanten', 'Magisterio', 'pide', 'a', 'docentes', 'cesar', 'paro', 'tras', 'oferta', '45', 'mil', 'trimestrales', 'a', 'educadoras', 'diferenciales', 'Consultada', 'entonces', 'd√≥nde', 'problema', 'a', 'parecer', 'jefa', 'cartera', 'solo', 'reiter√≥', 'di√°logo', 'interno', 'pendiente', 'mismos', 'reconocido', 'tener', 'Respecto', 'pago', 'menci√≥n', 'a', 'educadoras', 'diferenciales', 'p√°rvulos', 'Cubillos', 'adelant√≥', 'si', 'Gobierno', 'dispuesto', 'a', 'discutir', 'nuevamente', 'punto', 'se√±alando', 'solo', 'nunca', 'queda', 'conforme', 'planteado', 'petitorio', 'Cada', 'd√≠a', 'pasa', 'entrampamiento', 'hace', 'demoremos', 'empezar', 'a', 'avanzar', 'puntos', 'acuerdo', 'critic√≥', 'Cubillos', 'hizo', 'llamado', 'directo', 'a', 'colegio', '5', 'a√∫n', 'siguen', 'paro', 'a', 'ojal√°', 'depongan', 'Aguilar', 'alcance', 'paro', 'docente', 'Mostr√≥', 'organizaci√≥n', 'enfrenta', 'a', 'gobiernos', 'empresariales', 'parte', 'Ministerio', 'se√±al√≥', 'todas', 'energ√≠as', 'puestas', 'planes', 'recuperaci√≥n', 'reparar', 'da√±o', 'produce', 'a', 'ni√±os', 'educaci√≥n', 'publica', 'paralizaci√≥n', 'puntos', 'exig√≠a', 'Colegio', 'Profesores', 'primeras', 'semanas', 'movilizaci√≥n', 'ministra', 'participara', 'negociaciones', 'primera', 'instancia', 'lideradas', 'subsecretario', 'l√≠nea', 'descart√≥', 'mea', 'culpa', 'sumarse', 'a', 'mesas', 'trabajo', 'reci√©n', 'cuarta', 'semana', 'paro', 'gobierno', 'actuado', 'solo', 'siempre', 'misma', 'predisposici√≥n', 'posici√≥n', 'directiva', 'Colegio', 'Profesores', 'mismo', 'hablar', 'subsecrterario', 'gobierno', 'actuado', 'misma', 'opini√≥n', 'asegur√≥', 'A', 'agreg√≥', 'problema', 'conversado', 'Colegio', 'Profesores', 'Escucha', 'aqu√≠', 'entrevista', 'completa']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo de alguna noticia: {}\".format(cleaned_content[14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "### Extracci√≥n de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:18.064454Z",
     "start_time": "2020-05-07T19:18:03.208281Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:12:39: collecting all words and their counts\n",
      "INFO - 23:12:39: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:12:41: PROGRESS: at sentence #5000, processed 1239242 words and 878154 word types\n",
      "INFO - 23:12:42: PROGRESS: at sentence #10000, processed 2191881 words and 1395575 word types\n",
      "INFO - 23:12:44: PROGRESS: at sentence #15000, processed 3213105 words and 1891772 word types\n",
      "INFO - 23:12:46: PROGRESS: at sentence #20000, processed 4342156 words and 2517165 word types\n",
      "INFO - 23:12:48: PROGRESS: at sentence #25000, processed 5465003 words and 3095045 word types\n",
      "INFO - 23:12:49: collected 3264063 word types from a corpus of 5796971 words (unigram + bigrams) and 26413 sentences\n",
      "INFO - 23:12:49: using 3264063 counts as vocab in Phrases<0 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "#La condici√≥n para que sean considerados es que aparezcan por lo menos 100 veces repetidas.\n",
    "\n",
    "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:24.375161Z",
     "start_time": "2020-05-07T19:18:23.796702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {b'Colapsa': 1,\n",
       "             b'segmento': 125,\n",
       "             b'Colapsa_segmento': 1,\n",
       "             b'casa': 3376,\n",
       "             b'segmento_casa': 1,\n",
       "             b'derrumb\\xc3\\xb3': 76,\n",
       "             b'casa_derrumb\\xc3\\xb3': 5,\n",
       "             b'Valpara\\xc3\\xadso': 1986,\n",
       "             b'derrumb\\xc3\\xb3_Valpara\\xc3\\xadso': 4,\n",
       "             b'Noticia': 11,\n",
       "             b'Valpara\\xc3\\xadso_Noticia': 1,\n",
       "             b'Desarrollo': 833,\n",
       "             b'Noticia_Desarrollo': 1,\n",
       "             b'recopilando': 13,\n",
       "             b'Desarrollo_recopilando': 1,\n",
       "             b'antecedentes': 2185,\n",
       "             b'recopilando_antecedentes': 4,\n",
       "             b'noticia': 637,\n",
       "             b'antecedentes_noticia': 1,\n",
       "             b'qu\\xc3\\xa9date': 5,\n",
       "             b'noticia_qu\\xc3\\xa9date': 1,\n",
       "             b'atento': 63,\n",
       "             b'qu\\xc3\\xa9date_atento': 1,\n",
       "             b'a': 233657,\n",
       "             b'atento_a': 40,\n",
       "             b'actualizaciones': 19,\n",
       "             b'a_actualizaciones': 1,\n",
       "             b'Parte': 81,\n",
       "             b'actualizaciones_Parte': 1,\n",
       "             b'estructura': 442,\n",
       "             b'Parte_estructura': 1,\n",
       "             b'restante': 70,\n",
       "             b'estructura_restante': 1,\n",
       "             b'restante_casa': 1,\n",
       "             b'cay\\xc3\\xb3': 695,\n",
       "             b'casa_cay\\xc3\\xb3': 2,\n",
       "             b'ayer': 791,\n",
       "             b'cay\\xc3\\xb3_ayer': 1,\n",
       "             b'ayer_Valpara\\xc3\\xadso': 2,\n",
       "             b'colaps\\xc3\\xb3': 51,\n",
       "             b'Valpara\\xc3\\xadso_colaps\\xc3\\xb3': 1,\n",
       "             b'ma\\xc3\\xb1ana': 2764,\n",
       "             b'colaps\\xc3\\xb3_ma\\xc3\\xb1ana': 2,\n",
       "             b'mi\\xc3\\xa9rcoles': 5300,\n",
       "             b'ma\\xc3\\xb1ana_mi\\xc3\\xa9rcoles': 148,\n",
       "             b'hecho': 5912,\n",
       "             b'mi\\xc3\\xa9rcoles_hecho': 5,\n",
       "             b'produjo': 1013,\n",
       "             b'hecho_produjo': 56,\n",
       "             b'produjo_a': 64,\n",
       "             b'1015': 13,\n",
       "             b'a_1015': 3,\n",
       "             b'horas': 6417,\n",
       "             b'1015_horas': 3,\n",
       "             b'esquina': 203,\n",
       "             b'horas_esquina': 2,\n",
       "             b'Aldunate': 19,\n",
       "             b'esquina_Aldunate': 1,\n",
       "             b'Huito': 9,\n",
       "             b'Aldunate_Huito': 8,\n",
       "             b'Huito_ayer': 1,\n",
       "             b'murieron': 1100,\n",
       "             b'ayer_murieron': 3,\n",
       "             b'seis': 2408,\n",
       "             b'murieron_seis': 9,\n",
       "             b'personas': 13570,\n",
       "             b'seis_personas': 136,\n",
       "             b'Seg\\xc3\\xban': 7750,\n",
       "             b'personas_Seg\\xc3\\xban': 19,\n",
       "             b'informaci\\xc3\\xb3n': 3522,\n",
       "             b'Seg\\xc3\\xban_informaci\\xc3\\xb3n': 248,\n",
       "             b'preliminar': 400,\n",
       "             b'informaci\\xc3\\xb3n_preliminar': 127,\n",
       "             b'rescatistas': 137,\n",
       "             b'preliminar_rescatistas': 1,\n",
       "             b'lugar': 7472,\n",
       "             b'rescatistas_lugar': 2,\n",
       "             b'labores': 665,\n",
       "             b'lugar_labores': 3,\n",
       "             b'suspendido': 356,\n",
       "             b'labores_suspendido': 1,\n",
       "             b'peligro': 938,\n",
       "             b'suspendido_peligro': 1,\n",
       "             b'seguir': 1705,\n",
       "             b'peligro_seguir': 2,\n",
       "             b'trabajando': 1085,\n",
       "             b'seguir_trabajando': 64,\n",
       "             b'lesionados': 498,\n",
       "             b'trabajando_lesionados': 1,\n",
       "             b'Polic\\xc3\\xada': 1322,\n",
       "             b'busca': 2596,\n",
       "             b'Polic\\xc3\\xada_busca': 1,\n",
       "             b'busca_a': 52,\n",
       "             b'mujer': 5249,\n",
       "             b'a_mujer': 714,\n",
       "             b'acusada': 408,\n",
       "             b'mujer_acusada': 23,\n",
       "             b'matar': 410,\n",
       "             b'acusada_matar': 8,\n",
       "             b'matar_a': 229,\n",
       "             b'padre': 1770,\n",
       "             b'a_padre': 196,\n",
       "             b'discusi\\xc3\\xb3n': 862,\n",
       "             b'padre_discusi\\xc3\\xb3n': 2,\n",
       "             b'venta': 1118,\n",
       "             b'discusi\\xc3\\xb3n_venta': 1,\n",
       "             b'vivienda': 1202,\n",
       "             b'venta_vivienda': 2,\n",
       "             b'Pudahuel': 138,\n",
       "             b'vivienda_Pudahuel': 3,\n",
       "             b'Detectives': 41,\n",
       "             b'Pudahuel_Detectives': 1,\n",
       "             b'Detectives_Polic\\xc3\\xada': 7,\n",
       "             b'Investigaciones': 786,\n",
       "             b'Polic\\xc3\\xada_Investigaciones': 618,\n",
       "             b'realizan': 457,\n",
       "             b'Investigaciones_realizan': 1,\n",
       "             b'peritajes': 259,\n",
       "             b'realizan_peritajes': 7,\n",
       "             b'detener': 562,\n",
       "             b'peritajes_detener': 1,\n",
       "             b'detener_a': 137,\n",
       "             b'45': 787,\n",
       "             b'mujer_45': 11,\n",
       "             b'a\\xc3\\xb1os': 21728,\n",
       "             b'45_a\\xc3\\xb1os': 146,\n",
       "             b'presunta': 542,\n",
       "             b'a\\xc3\\xb1os_presunta': 8,\n",
       "             b'responsabilidad': 1345,\n",
       "             b'presunta_responsabilidad': 46,\n",
       "             b'ataque': 2691,\n",
       "             b'responsabilidad_ataque': 5,\n",
       "             b'arma': 1069,\n",
       "             b'ataque_arma': 20,\n",
       "             b'cortante': 15,\n",
       "             b'arma_cortante': 7,\n",
       "             b'propio': 1193,\n",
       "             b'cortante_propio': 1,\n",
       "             b'propio_padre': 14,\n",
       "             b'caus\\xc3\\xb3': 609,\n",
       "             b'padre_caus\\xc3\\xb3': 1,\n",
       "             b'muerte': 3677,\n",
       "             b'caus\\xc3\\xb3_muerte': 70,\n",
       "             b'comuna': 3290,\n",
       "             b'muerte_comuna': 2,\n",
       "             b'comuna_Pudahuel': 18,\n",
       "             b'Pudahuel_hecho': 4,\n",
       "             b'ocurri\\xc3\\xb3': 1774,\n",
       "             b'hecho_ocurri\\xc3\\xb3': 339,\n",
       "             b'calle': 1918,\n",
       "             b'ocurri\\xc3\\xb3_calle': 22,\n",
       "             b'Presidente': 1008,\n",
       "             b'calle_Presidente': 3,\n",
       "             b'Truman': 19,\n",
       "             b'Presidente_Truman': 1,\n",
       "             b'cerca': 4199,\n",
       "             b'Truman_cerca': 1,\n",
       "             b'intersecci\\xc3\\xb3n': 308,\n",
       "             b'cerca_intersecci\\xc3\\xb3n': 10,\n",
       "             b'Teniente': 53,\n",
       "             b'intersecci\\xc3\\xb3n_Teniente': 2,\n",
       "             b'Cruz': 656,\n",
       "             b'Teniente_Cruz': 5,\n",
       "             b'acorde': 127,\n",
       "             b'Cruz_acorde': 1,\n",
       "             b'acorde_a': 67,\n",
       "             b'declaraci\\xc3\\xb3n': 1590,\n",
       "             b'a_declaraci\\xc3\\xb3n': 65,\n",
       "             b'hijo': 1870,\n",
       "             b'declaraci\\xc3\\xb3n_hijo': 2,\n",
       "             b'v\\xc3\\xadctima': 2742,\n",
       "             b'hijo_v\\xc3\\xadctima': 10,\n",
       "             b'hermano': 623,\n",
       "             b'v\\xc3\\xadctima_hermano': 2,\n",
       "             b'victimaria': 11,\n",
       "             b'hermano_victimaria': 1,\n",
       "             b'ambos': 2174,\n",
       "             b'victimaria_ambos': 1,\n",
       "             b'sostuvieron': 148,\n",
       "             b'ambos_sostuvieron': 2,\n",
       "             b'enfrentamiento': 324,\n",
       "             b'sostuvieron_enfrentamiento': 2,\n",
       "             b'verbal': 93,\n",
       "             b'enfrentamiento_verbal': 2,\n",
       "             b'debido': 4077,\n",
       "             b'verbal_debido': 1,\n",
       "             b'debido_a': 3300,\n",
       "             b'intensi\\xc3\\xb3n': 7,\n",
       "             b'a_intensi\\xc3\\xb3n': 1,\n",
       "             b'Hernan': 12,\n",
       "             b'intensi\\xc3\\xb3n_Hernan': 1,\n",
       "             b'Silva': 906,\n",
       "             b'Hernan_Silva': 1,\n",
       "             b'P\\xc3\\xa9rez': 882,\n",
       "             b'Silva_P\\xc3\\xa9rez': 1,\n",
       "             b'vender': 349,\n",
       "             b'P\\xc3\\xa9rez_vender': 1,\n",
       "             b'vender_casa': 4,\n",
       "             b'Negocio': 5,\n",
       "             b'casa_Negocio': 1,\n",
       "             b'causado': 366,\n",
       "             b'Negocio_causado': 1,\n",
       "             b'molestia': 284,\n",
       "             b'causado_molestia': 6,\n",
       "             b'hija': 1497,\n",
       "             b'molestia_hija': 2,\n",
       "             b'Tania': 37,\n",
       "             b'hija_Tania': 1,\n",
       "             b'Tania_Silva': 2,\n",
       "             b'tras': 13937,\n",
       "             b'Silva_tras': 10,\n",
       "             b'tras_discusi\\xc3\\xb3n': 21,\n",
       "             b'acudido': 55,\n",
       "             b'discusi\\xc3\\xb3n_acudido': 1,\n",
       "             b'acudido_a': 32,\n",
       "             b'cocina': 242,\n",
       "             b'a_cocina': 15,\n",
       "             b'cocina_vivienda': 3,\n",
       "             b'volver': 930,\n",
       "             b'vivienda_volver': 1,\n",
       "             b'cuchillo': 300,\n",
       "             b'volver_cuchillo': 1,\n",
       "             b'apu\\xc3\\xb1alar': 52,\n",
       "             b'cuchillo_apu\\xc3\\xb1alar': 1,\n",
       "             b'apu\\xc3\\xb1alar_a': 37,\n",
       "             b'primeras': 678,\n",
       "             b'padre_primeras': 1,\n",
       "             b'diligencias': 579,\n",
       "             b'primeras_diligencias': 33,\n",
       "             b'realizaron': 699,\n",
       "             b'diligencias_realizaron': 8,\n",
       "             b'carabineros': 522,\n",
       "             b'realizaron_carabineros': 1,\n",
       "             b'45\\xc2\\xba': 7,\n",
       "             b'carabineros_45\\xc2\\xba': 1,\n",
       "             b'comisar\\xc3\\xada': 267,\n",
       "             b'45\\xc2\\xba_comisar\\xc3\\xada': 1,\n",
       "             b'tomaron': 365,\n",
       "             b'comisar\\xc3\\xada_tomaron': 1,\n",
       "             b'tomaron_declaraci\\xc3\\xb3n': 5,\n",
       "             b'\\xc3\\xbanico': 1519,\n",
       "             b'declaraci\\xc3\\xb3n_\\xc3\\xbanico': 1,\n",
       "             b'testigo': 524,\n",
       "             b'\\xc3\\xbanico_testigo': 4,\n",
       "             b'crimen': 1144,\n",
       "             b'testigo_crimen': 2,\n",
       "             b'interior': 2332,\n",
       "             b'crimen_interior': 2,\n",
       "             b'interior_vivienda': 64,\n",
       "             b'capit\\xc3\\xa1n': 519,\n",
       "             b'vivienda_capit\\xc3\\xa1n': 1,\n",
       "             b'Carlos': 2065,\n",
       "             b'capit\\xc3\\xa1n_Carlos': 10,\n",
       "             b'Lagos': 1560,\n",
       "             b'Carlos_Lagos': 3,\n",
       "             b'explic\\xc3\\xb3': 4735,\n",
       "             b'Lagos_explic\\xc3\\xb3': 5,\n",
       "             b'principal': 2106,\n",
       "             b'explic\\xc3\\xb3_principal': 6,\n",
       "             b'hip\\xc3\\xb3tesis': 249,\n",
       "             b'principal_hip\\xc3\\xb3tesis': 16,\n",
       "             b'apunta': 543,\n",
       "             b'hip\\xc3\\xb3tesis_apunta': 4,\n",
       "             b'apunta_a': 350,\n",
       "             b'a_discusi\\xc3\\xb3n': 47,\n",
       "             b'dinero': 2394,\n",
       "             b'discusi\\xc3\\xb3n_dinero': 1,\n",
       "             b'dinero_venta': 5,\n",
       "             b'inmueble': 352,\n",
       "             b'venta_inmueble': 2,\n",
       "             b'Luego': 1536,\n",
       "             b'inmueble_Luego': 1,\n",
       "             b'detectives': 111,\n",
       "             b'Luego_detectives': 1,\n",
       "             b'Brigada': 685,\n",
       "             b'detectives_Brigada': 15,\n",
       "             b'Homicidios': 344,\n",
       "             b'Brigada_Homicidios': 335,\n",
       "             b'PDI': 1322,\n",
       "             b'Homicidios_PDI': 122,\n",
       "             b'PDI_realizaron': 3,\n",
       "             b'realizaron_peritajes': 8,\n",
       "             b'corroborando': 4,\n",
       "             b'peritajes_corroborando': 1,\n",
       "             b'corroborando_v\\xc3\\xadctima': 1,\n",
       "             b'muri\\xc3\\xb3': 1527,\n",
       "             b'v\\xc3\\xadctima_muri\\xc3\\xb3': 20,\n",
       "             b'ser': 13537,\n",
       "             b'muri\\xc3\\xb3_ser': 1,\n",
       "             b'apu\\xc3\\xb1alado': 160,\n",
       "             b'ser_apu\\xc3\\xb1alado': 41,\n",
       "             b'ocasi\\xc3\\xb3n': 570,\n",
       "             b'apu\\xc3\\xb1alado_ocasi\\xc3\\xb3n': 1,\n",
       "             b'ocasi\\xc3\\xb3n_a': 10,\n",
       "             b'altura': 910,\n",
       "             b'a_altura': 451,\n",
       "             b't\\xc3\\xb3rax': 59,\n",
       "             b'altura_t\\xc3\\xb3rax': 10,\n",
       "             b'subcomisario': 104,\n",
       "             b't\\xc3\\xb3rax_subcomisario': 1,\n",
       "             b'Cristi\\xc3\\xa1n': 203,\n",
       "             b'subcomisario_Cristi\\xc3\\xa1n': 3,\n",
       "             b'Tur': 10,\n",
       "             b'Cristi\\xc3\\xa1n_Tur': 2,\n",
       "             b'asegur\\xc3\\xb3': 5705,\n",
       "             b'Tur_asegur\\xc3\\xb3': 1,\n",
       "             b'asegur\\xc3\\xb3_presunta': 1,\n",
       "             b'responsable': 1499,\n",
       "             b'presunta_responsable': 2,\n",
       "             b'identificada': 298,\n",
       "             b'responsable_identificada': 1,\n",
       "             b'raz\\xc3\\xb3n': 1031,\n",
       "             b'identificada_raz\\xc3\\xb3n': 1,\n",
       "             b'raz\\xc3\\xb3n_diligencias': 1,\n",
       "             b'polic\\xc3\\xada': 4365,\n",
       "             b'diligencias_polic\\xc3\\xada': 5,\n",
       "             b'enfocadas': 36,\n",
       "             b'polic\\xc3\\xada_enfocadas': 2,\n",
       "             b'dar': 2722,\n",
       "             b'enfocadas_dar': 1,\n",
       "             b'paradero': 404,\n",
       "             b'dar_paradero': 152,\n",
       "             b'paradero_detener': 1,\n",
       "             b'a_Tania': 3,\n",
       "             b'Herrera': 141,\n",
       "             b'Silva_Herrera': 1,\n",
       "             b'Herrera_45': 1,\n",
       "             b'a\\xc3\\xb1os_acorde': 1,\n",
       "             b'expresado': 168,\n",
       "             b'a_expresado': 11,\n",
       "             b'familiares': 1280,\n",
       "             b'expresado_familiares': 1,\n",
       "             b'vivir\\xc3\\xada': 5,\n",
       "             b'familiares_vivir\\xc3\\xada': 1,\n",
       "             b'situaci\\xc3\\xb3n': 6675,\n",
       "             b'vivir\\xc3\\xada_situaci\\xc3\\xb3n': 1,\n",
       "             b'situaci\\xc3\\xb3n_calle': 205,\n",
       "             b'buscada': 31,\n",
       "             b'calle_buscada': 1,\n",
       "             b'delito': 1866,\n",
       "             b'buscada_delito': 1,\n",
       "             b'parricidio': 67,\n",
       "             b'delito_parricidio': 29,\n",
       "             b'art\\xc3\\xadculo': 1464,\n",
       "             b'parricidio_art\\xc3\\xadculo': 1,\n",
       "             b'describe': 712,\n",
       "             b'art\\xc3\\xadculo_describe': 527,\n",
       "             b'proceso': 4762,\n",
       "             b'describe_proceso': 526,\n",
       "             b'judicial': 1892,\n",
       "             b'proceso_judicial': 630,\n",
       "             b'curso': 1078,\n",
       "             b'judicial_curso': 528,\n",
       "             b'Existe': 588,\n",
       "             b'curso_Existe': 526,\n",
       "             b'posibilidad': 2092,\n",
       "             b'Existe_posibilidad': 531,\n",
       "             b'cargos': 2011,\n",
       "             b'posibilidad_cargos': 526,\n",
       "             b'desestimados': 531,\n",
       "             b'cargos_desestimados': 526,\n",
       "             b'finalizar': 690,\n",
       "             b'desestimados_finalizar': 526,\n",
       "             b'investigaci\\xc3\\xb3n': 6261,\n",
       "             b'finalizar_investigaci\\xc3\\xb3n': 527,\n",
       "             b'debe': 4549,\n",
       "             b'investigaci\\xc3\\xb3n_debe': 534,\n",
       "             b'considerar': 1212,\n",
       "             b'debe_considerar': 548,\n",
       "             b'imputados': 998,\n",
       "             b'considerar_imputados': 526,\n",
       "             b'culpables': 717,\n",
       "             b'imputados_culpables': 526,\n",
       "             b'Justicia': 2252,\n",
       "             b'culpables_Justicia': 526,\n",
       "             b'dicte': 546,\n",
       "             b'Justicia_dicte': 526,\n",
       "             b'sentencia': 1531,\n",
       "             b'dicte_sentencia': 530,\n",
       "             b'Art\\xc3\\xadculo': 599,\n",
       "             b'sentencia_Art\\xc3\\xadculo': 526,\n",
       "             b'04': 587,\n",
       "             b'Art\\xc3\\xadculo_04': 526,\n",
       "             b'C\\xc3\\xb3digo': 780,\n",
       "             b'04_C\\xc3\\xb3digo': 526,\n",
       "             b'Procesal': 551,\n",
       "             b'C\\xc3\\xb3digo_Procesal': 539,\n",
       "             b'Penal': 1051,\n",
       "             b'Procesal_Penal': 550,\n",
       "             b'Dos': 906,\n",
       "             b'detenidos': 1825,\n",
       "             b'Dos_detenidos': 27,\n",
       "             b'Liceo': 251,\n",
       "             b'detenidos_Liceo': 1,\n",
       "             b'Aplicaci\\xc3\\xb3n': 21,\n",
       "             b'Liceo_Aplicaci\\xc3\\xb3n': 13,\n",
       "             b'protagonistas': 146,\n",
       "             b'Aplicaci\\xc3\\xb3n_protagonistas': 1,\n",
       "             b'incendiaron': 26,\n",
       "             b'protagonistas_incendiaron': 1,\n",
       "             b'ba\\xc3\\xb1o': 336,\n",
       "             b'incendiaron_ba\\xc3\\xb1o': 1,\n",
       "             b'quemar': 145,\n",
       "             b'ba\\xc3\\xb1o_quemar': 2,\n",
       "             b'capuchas': 13,\n",
       "             b'quemar_capuchas': 1,\n",
       "             b'overoles': 15,\n",
       "             b'capuchas_overoles': 1,\n",
       "             b'overoles_Dos': 1,\n",
       "             b'saldo': 345,\n",
       "             b'detenidos_saldo': 1,\n",
       "             b'serie': 2020,\n",
       "             b'saldo_serie': 1,\n",
       "             b'incidentes': 644,\n",
       "             b'serie_incidentes': 25,\n",
       "             b'ocurridos': 302,\n",
       "             b'incidentes_ocurridos': 21,\n",
       "             b'ocurridos_ma\\xc3\\xb1ana': 2,\n",
       "             b'ma\\xc3\\xb1ana_Liceo': 1,\n",
       "             b'Santiago': 3161,\n",
       "             b'Aplicaci\\xc3\\xb3n_Santiago': 1,\n",
       "             b'incluyeron': 63,\n",
       "             b'Santiago_incluyeron': 2,\n",
       "             b'fogata': 33,\n",
       "             b'incluyeron_fogata': 1,\n",
       "             b'fogata_interior': 2,\n",
       "             b'ba\\xc3\\xb1os': 110,\n",
       "             b'interior_ba\\xc3\\xb1os': 1,\n",
       "             b'A': 7682,\n",
       "             b'ba\\xc3\\xb1os_A': 1,\n",
       "             b'800': 361,\n",
       "             b'A_800': 3,\n",
       "             b'800_ma\\xc3\\xb1ana': 2,\n",
       "             b'grupo': 4053,\n",
       "             b'ma\\xc3\\xb1ana_grupo': 8,\n",
       "             b'encapuchados': 361,\n",
       "             b'grupo_encapuchados': 36,\n",
       "             b'realiz\\xc3\\xb3': 1582,\n",
       "             b'encapuchados_realiz\\xc3\\xb3': 2,\n",
       "             b'disturbios': 279,\n",
       "             b'realiz\\xc3\\xb3_disturbios': 1,\n",
       "             b'disturbios_intersecci\\xc3\\xb3n': 1,\n",
       "             b'Cumming': 13,\n",
       "             b'intersecci\\xc3\\xb3n_Cumming': 2,\n",
       "             b'Erasmo': 8,\n",
       "             b'Cumming_Erasmo': 1,\n",
       "             b'Escala': 9,\n",
       "             b'Erasmo_Escala': 1,\n",
       "             b'Escala_incluyeron': 1,\n",
       "             b'barricadas': 189,\n",
       "             b'incluyeron_barricadas': 2,\n",
       "             b'Evacuaci\\xc3\\xb3n': 8,\n",
       "             b'barricadas_Evacuaci\\xc3\\xb3n': 1,\n",
       "             b'espont\\xc3\\xa1nea': 31,\n",
       "             b'Evacuaci\\xc3\\xb3n_espont\\xc3\\xa1nea': 2,\n",
       "             b'Instituto': 1506,\n",
       "             b'espont\\xc3\\xa1nea_Instituto': 2,\n",
       "             b'Nacional': 5468,\n",
       "             b'Instituto_Nacional': 814,\n",
       "             b'efecto': 951,\n",
       "             b'Nacional_efecto': 2,\n",
       "             b'bombas': 407,\n",
       "             b'efecto_bombas': 2,\n",
       "             b'lacrim\\xc3\\xb3genas': 61,\n",
       "             b'bombas_lacrim\\xc3\\xb3genas': 50,\n",
       "             b'lacrim\\xc3\\xb3genas_carabineros': 2,\n",
       "             b'carabineros_A': 1,\n",
       "             b'llegada': 1121,\n",
       "             b'A_llegada': 24,\n",
       "             b'Fuerzas': 754,\n",
       "             b'llegada_Fuerzas': 2,\n",
       "             b'Especiales': 298,\n",
       "             b'Fuerzas_Especiales': 229,\n",
       "             b'Carabineros': 4288,\n",
       "             b'Especiales_Carabineros': 95,\n",
       "             b'manifestantes': 1376,\n",
       "             b'Carabineros_manifestantes': 5,\n",
       "             b'lanzaron': 295,\n",
       "             b'manifestantes_lanzaron': 12,\n",
       "             b'lanzaron_bombas': 30,\n",
       "             b'molotov': 214,\n",
       "             b'bombas_molotov': 115,\n",
       "             b'personal': 2798,\n",
       "             b'molotov_personal': 2,\n",
       "             b'luego': 7593,\n",
       "             b'personal_luego': 3,\n",
       "             b'huir': 349,\n",
       "             b'luego_huir': 28,\n",
       "             b'huir_interior': 1,\n",
       "             b'establecimiento': 904,\n",
       "             b'interior_establecimiento': 35,\n",
       "             b'seg\\xc3\\xban': 10659,\n",
       "             b'establecimiento_seg\\xc3\\xban': 4,\n",
       "             b'report\\xc3\\xb3': 385,\n",
       "             b'seg\\xc3\\xban_report\\xc3\\xb3': 40,\n",
       "             b'mayor': 4915,\n",
       "             b'report\\xc3\\xb3_mayor': 1,\n",
       "             b'Gonzalo': 441,\n",
       "             b'mayor_Gonzalo': 4,\n",
       "             b'Urbina': 22,\n",
       "             b'Gonzalo_Urbina': 4,\n",
       "             b'uniformado': 208,\n",
       "             b'Urbina_uniformado': 1,\n",
       "             b'coment\\xc3\\xb3': 1522,\n",
       "             b'uniformado_coment\\xc3\\xb3': 1,\n",
       "             b'coment\\xc3\\xb3_tras': 11,\n",
       "             b'ello': 2596,\n",
       "             b'tras_ello': 14,\n",
       "             b'ello_encapuchados': 1,\n",
       "             b'iniciaron': 345,\n",
       "             b'encapuchados_iniciaron': 1,\n",
       "             b'iniciaron_fogata': 1,\n",
       "             b'fogata_ba\\xc3\\xb1o': 1,\n",
       "             b'quemar_overoles': 1,\n",
       "             b'overoles_capuchas': 1,\n",
       "             b'mochilas': 92,\n",
       "             b'capuchas_mochilas': 1,\n",
       "             b'utilizaban': 33,\n",
       "             b'mochilas_utilizaban': 1,\n",
       "             b'utilizaban_personal': 1,\n",
       "             b'policial': 1704,\n",
       "             b'personal_policial': 201,\n",
       "             b'ingres\\xc3\\xb3': 450,\n",
       "             b'policial_ingres\\xc3\\xb3': 1,\n",
       "             b'recinto': 1578,\n",
       "             b'ingres\\xc3\\xb3_recinto': 10,\n",
       "             b'logr\\xc3\\xb3': 1387,\n",
       "             b'recinto_logr\\xc3\\xb3': 6,\n",
       "             b'logr\\xc3\\xb3_detener': 27,\n",
       "             b'dos': 13762,\n",
       "             b'a_dos': 1369,\n",
       "             b'j\\xc3\\xb3venes': 1533,\n",
       "             b'dos_j\\xc3\\xb3venes': 102,\n",
       "             b'portaban': 59,\n",
       "             b'j\\xc3\\xb3venes_portaban': 1,\n",
       "             b'portaban_bombas': 1,\n",
       "             b'incendiarias': 29,\n",
       "             b'bombas_incendiarias': 13,\n",
       "             b'incendiarias_Urbina': 1,\n",
       "             b'agreg\\xc3\\xb3': 4177,\n",
       "             b'Urbina_agreg\\xc3\\xb3': 1,\n",
       "             b'autoridades': 5387,\n",
       "             b'agreg\\xc3\\xb3_autoridades': 10,\n",
       "             b'liceo': 176,\n",
       "             b'autoridades_liceo': 1,\n",
       "             b'indicaron': 1294,\n",
       "             b'liceo_indicaron': 1,\n",
       "             b'indicaron_protagonistas': 2,\n",
       "             b'hechos': 2750,\n",
       "             b'protagonistas_hechos': 2,\n",
       "             b'estudiantes': 1890,\n",
       "             b'hechos_estudiantes': 2,\n",
       "             b'estudiantes_recinto': 7,\n",
       "             b'investiga': 540,\n",
       "             b'recinto_investiga': 1,\n",
       "             b'investiga_a': 29,\n",
       "             b'colegio': 835,\n",
       "             b'a_colegio': 30,\n",
       "             b'pertenec\\xc3\\xadan': 59,\n",
       "             b'colegio_pertenec\\xc3\\xadan': 1,\n",
       "             b'aprehendidos': 34,\n",
       "             b'pertenec\\xc3\\xadan_aprehendidos': 1,\n",
       "             b'alumnos': 741,\n",
       "             b'aprehendidos_alumnos': 1,\n",
       "             b'alumnos_colegio': 7,\n",
       "             b'denunciaron': 367,\n",
       "             b'colegio_denunciaron': 1,\n",
       "             b'uso': 2384,\n",
       "             b'denunciaron_uso': 3,\n",
       "             b'indiscriminado': 17,\n",
       "             b'uso_indiscriminado': 9,\n",
       "             b'gases': 355,\n",
       "             b'indiscriminado_gases': 1,\n",
       "             b'lacrim\\xc3\\xb3genos': 151,\n",
       "             b'gases_lacrim\\xc3\\xb3genos': 150,\n",
       "             b'parte': 12584,\n",
       "             b'lacrim\\xc3\\xb3genos_parte': 3,\n",
       "             b'parte_Carabineros': 27,\n",
       "             b'similar': 709,\n",
       "             b'Carabineros_similar': 1,\n",
       "             b'similar_a': 184,\n",
       "             b'ocurrido': 1741,\n",
       "             b'a_ocurrido': 82,\n",
       "             b'hoy': 2798,\n",
       "             b'ocurrido_hoy': 6,\n",
       "             b'hoy_Instituto': 1,\n",
       "             b'Rodrigo': 905,\n",
       "             b'Nacional_Rodrigo': 20,\n",
       "             b'Pino': 237,\n",
       "             b'Rodrigo_Pino': 94,\n",
       "             b'RBB': 1103,\n",
       "             b'Pino_RBB': 94,\n",
       "             b'Bomberos': 1120,\n",
       "             b'RBB_Bomberos': 2,\n",
       "             b'lleg\\xc3\\xb3': 2671,\n",
       "             b'Bomberos_lleg\\xc3\\xb3': 16,\n",
       "             b'lleg\\xc3\\xb3_lugar': 153,\n",
       "             b'controlar': 499,\n",
       "             b'lugar_controlar': 11,\n",
       "             b'controlar_situaci\\xc3\\xb3n': 45,\n",
       "             b'clases': 992,\n",
       "             b'situaci\\xc3\\xb3n_clases': 3,\n",
       "             b'suspendidas': 113,\n",
       "             b'clases_suspendidas': 20,\n",
       "             b'Apoyo': 68,\n",
       "             b'transversal': 138,\n",
       "             b'Apoyo_transversal': 1,\n",
       "             b'Senado': 1756,\n",
       "             b'transversal_Senado': 2,\n",
       "             b'aprueba': 295,\n",
       "             b'Senado_aprueba': 26,\n",
       "             b'general': 3530,\n",
       "             b'aprueba_general': 7,\n",
       "             b'proyecto': 5315,\n",
       "             b'general_proyecto': 36,\n",
       "             b'ley': 3839,\n",
       "             b'proyecto_ley': 708,\n",
       "             b'migraciones': 69,\n",
       "             b'ley_migraciones': 6,\n",
       "             b'Gobierno': 6215,\n",
       "             b'migraciones_Gobierno': 1,\n",
       "             b'sala': 689,\n",
       "             b'Gobierno_sala': 1,\n",
       "             b'sala_Senado': 21,\n",
       "             b'aprob\\xc3\\xb3': 578,\n",
       "             b'Senado_aprob\\xc3\\xb3': 30,\n",
       "             b'aprob\\xc3\\xb3_general': 14,\n",
       "             b'presentado': 910,\n",
       "             b'migraciones_presentado': 1,\n",
       "             b'presentado_Gobierno': 17,\n",
       "             b'Ahora': 734,\n",
       "             b'Gobierno_Ahora': 3,\n",
       "             b'iniciativa': 1822,\n",
       "             b'Ahora_iniciativa': 4,\n",
       "             b'entrar\\xc3\\xa1': 95,\n",
       "             b'iniciativa_entrar\\xc3\\xa1': 2,\n",
       "             b'entrar\\xc3\\xa1_a': 9,\n",
       "             b'fase': 312,\n",
       "             b'a_fase': 25,\n",
       "             b'indicaciones': 209,\n",
       "             b'fase_indicaciones': 1,\n",
       "             b'indicaciones_discusi\\xc3\\xb3n': 2,\n",
       "             b'particular': 1128,\n",
       "             b'discusi\\xc3\\xb3n_particular': 21,\n",
       "             b'particular_iniciativa': 3,\n",
       "             b'iniciativa_busca': 68,\n",
       "             b'modificar': 372,\n",
       "             b'busca_modificar': 29,\n",
       "             b'pol\\xc3\\xadtica': 4189,\n",
       "             b'modificar_pol\\xc3\\xadtica': 4,\n",
       "             b'migratoria': 537,\n",
       "             b'pol\\xc3\\xadtica_migratoria': 160,\n",
       "             b'pa\\xc3\\xads': 14521,\n",
       "             b'migratoria_pa\\xc3\\xads': 9,\n",
       "             b'reformada': 8,\n",
       "             b'pa\\xc3\\xads_reformada': 1,\n",
       "             b'\\xc3\\xbaltima': 1909,\n",
       "             b'reformada_\\xc3\\xbaltima': 1,\n",
       "             b'vez': 5877,\n",
       "             b'\\xc3\\xbaltima_vez': 172,\n",
       "             b'a\\xc3\\xb1o': 9948,\n",
       "             b'vez_a\\xc3\\xb1o': 48,\n",
       "             b'1975': 62,\n",
       "             b'a\\xc3\\xb1o_1975': 2,\n",
       "             b'votaci\\xc3\\xb3n': 1198,\n",
       "             b'1975_votaci\\xc3\\xb3n': 1,\n",
       "             b'resolvi\\xc3\\xb3': 179,\n",
       "             b'votaci\\xc3\\xb3n_resolvi\\xc3\\xb3': 2,\n",
       "             b'41': 382,\n",
       "             b'resolvi\\xc3\\xb3_41': 1,\n",
       "             b'votos': 1541,\n",
       "             b'41_votos': 7,\n",
       "             b'votos_a': 245,\n",
       "             b'favor': 1794,\n",
       "             b'a_favor': 1269,\n",
       "             b'0': 187,\n",
       "             b'favor_0': 2,\n",
       "             b'0_0': 24,\n",
       "             b'abstenciones': 95,\n",
       "             b'0_abstenciones': 2,\n",
       "             b'ahora': 4229,\n",
       "             b'abstenciones_ahora': 1,\n",
       "             b'abrir\\xc3\\xa1': 96,\n",
       "             b'ahora_abrir\\xc3\\xa1': 1,\n",
       "             b'periodo': 1156,\n",
       "             b'abrir\\xc3\\xa1_periodo': 1,\n",
       "             b'ingresarle': 1,\n",
       "             b'periodo_ingresarle': 1,\n",
       "             b'ingresarle_indicaciones': 1,\n",
       "             b'intervenci\\xc3\\xb3n': 1022,\n",
       "             b'indicaciones_intervenci\\xc3\\xb3n': 1,\n",
       "             b'senador': 1251,\n",
       "             b'intervenci\\xc3\\xb3n_senador': 2,\n",
       "             b'Partido': 2051,\n",
       "             b'senador_Partido': 22,\n",
       "             b'Socialista': 395,\n",
       "             b'Partido_Socialista': 360,\n",
       "             b'Jos\\xc3\\xa9': 2017,\n",
       "             b'Socialista_Jos\\xc3\\xa9': 1,\n",
       "             b'Miguel': 1044,\n",
       "             b'Jos\\xc3\\xa9_Miguel': 121,\n",
       "             b'Insulza': 68,\n",
       "             b'Miguel_Insulza': 46,\n",
       "             b'apoy\\xc3\\xb3': 144,\n",
       "             b'Insulza_apoy\\xc3\\xb3': 1,\n",
       "             b'medidas': 3327,\n",
       "             b'apoy\\xc3\\xb3_medidas': 2,\n",
       "             b'tomado': 553,\n",
       "             b'medidas_tomado': 11,\n",
       "             b'gobierno': 12239,\n",
       "             b'tomado_gobierno': 1,\n",
       "             b'torno': 792,\n",
       "             b'gobierno_torno': 1,\n",
       "             b'flujo': 369,\n",
       "             b'torno_flujo': 1,\n",
       "             b'migratorio': 257,\n",
       "             b'flujo_migratorio': 54,\n",
       "             b'producido': 245,\n",
       "             b'migratorio_producido': 3,\n",
       "             b'crisis': 4216,\n",
       "             b'producido_crisis': 2,\n",
       "             b'crisis_pol\\xc3\\xadtica': 311,\n",
       "             b'social': 1835,\n",
       "             b'pol\\xc3\\xadtica_social': 66,\n",
       "             b'Venezuela': 7055,\n",
       "             b'social_Venezuela': 11,\n",
       "             b'acuerdo': 10562,\n",
       "             b'Venezuela_acuerdo': 12,\n",
       "             b'fijar': 133,\n",
       "             b'acuerdo_fijar': 7,\n",
       "             b'visa': 279,\n",
       "             b'fijar_visa': 1,\n",
       "             b'cierto': 554,\n",
       "             b'visa_cierto': 1,\n",
       "             b'cierto_acuerdo': 3,\n",
       "             b'decisi\\xc3\\xb3n': 3614,\n",
       "             b'acuerdo_decisi\\xc3\\xb3n': 6,\n",
       "             b'tomada': 240,\n",
       "             b'decisi\\xc3\\xb3n_tomada': 77,\n",
       "             b'respecto': 4184,\n",
       "             b'tomada_respecto': 5,\n",
       "             b'consulados': 43,\n",
       "             b'respecto_consulados': 1,\n",
       "             b'Tacna': 85,\n",
       "             b'consulados_Tacna': 1,\n",
       "             b'Lima': 922,\n",
       "             b'Tacna_Lima': 2,\n",
       "             b'sostuvo': 2316,\n",
       "             b'Lima_sostuvo': 2,\n",
       "             b'sostuvo_parte': 17,\n",
       "             b'parte_senador': 32,\n",
       "             b'Renovaci\\xc3\\xb3n': 330,\n",
       "             b'senador_Renovaci\\xc3\\xb3n': 39,\n",
       "             b'Renovaci\\xc3\\xb3n_Nacional': 324,\n",
       "             b'Francisco': 1669,\n",
       "             b'Nacional_Francisco': 27,\n",
       "             b'Chahu\\xc3\\xa1n': 92,\n",
       "             b'Francisco_Chahu\\xc3\\xa1n': 66,\n",
       "             b'calific\\xc3\\xb3': 1013,\n",
       "             b'Chahu\\xc3\\xa1n_calific\\xc3\\xb3': 1,\n",
       "             b'nueva': 3504,\n",
       "             b'calific\\xc3\\xb3_nueva': 2,\n",
       "             b'legislaci\\xc3\\xb3n': 510,\n",
       "             b'nueva_legislaci\\xc3\\xb3n': 18,\n",
       "             b'moderna': 179,\n",
       "             b'legislaci\\xc3\\xb3n_moderna': 3,\n",
       "             b'asegurando': 720,\n",
       "             b'moderna_asegurando': 1,\n",
       "             b'equilibra': 3,\n",
       "             b'asegurando_equilibra': 1,\n",
       "             b'derechos': 2404,\n",
       "             b'equilibra_derechos': 1,\n",
       "             b'migrantes': 2301,\n",
       "             b'derechos_migrantes': 16,\n",
       "             b'migrantes_pa\\xc3\\xads': 14,\n",
       "             b'regular': 300,\n",
       "             b'pa\\xc3\\xads_regular': 2,\n",
       "             b'migraci\\xc3\\xb3n': 588,\n",
       "             b'regular_migraci\\xc3\\xb3n': 1,\n",
       "             b'migraci\\xc3\\xb3n_iniciativa': 1,\n",
       "             b'iniciativa_ahora': 5,\n",
       "             b'continuar\\xc3\\xa1': 272,\n",
       "             b'ahora_continuar\\xc3\\xa1': 2,\n",
       "             b'tr\\xc3\\xa1mite': 534,\n",
       "             b'continuar\\xc3\\xa1_tr\\xc3\\xa1mite': 1,\n",
       "             b'forma': 5244,\n",
       "             b'tr\\xc3\\xa1mite_forma': 2,\n",
       "             b'forma_particular': 15,\n",
       "             b'particular_Senado': 1,\n",
       "             b'carabineros_ma\\xc3\\xb1ana': 2,\n",
       "             b'mi\\xc3\\xa9rcoles_produjo': 2,\n",
       "             b'evacuaci\\xc3\\xb3n': 359,\n",
       "             b'produjo_evacuaci\\xc3\\xb3n': 1,\n",
       "             b'evacuaci\\xc3\\xb3n_espont\\xc3\\xa1nea': 1,\n",
       "             b'comunidad': 2113,\n",
       "             b'espont\\xc3\\xa1nea_comunidad': 1,\n",
       "             b'educativa': 100,\n",
       "             b'comunidad_educativa': 49,\n",
       "             b'educativa_Instituto': 1,\n",
       "             b'Nacional_debido': 4,\n",
       "             b'debido_efecto': 3,\n",
       "             b'efecto_gases': 1,\n",
       "             b'lacrim\\xc3\\xb3genos_carabineros': 1,\n",
       "             b'carabineros_interior': 3,\n",
       "             b'acci\\xc3\\xb3n': 1675,\n",
       "             b'establecimiento_acci\\xc3\\xb3n': 1,\n",
       "             b'acci\\xc3\\xb3n_policial': 20,\n",
       "             b'dio': 2803,\n",
       "             b'policial_dio': 9,\n",
       "             b'respuesta': 1915,\n",
       "             b'dio_respuesta': 12,\n",
       "             b'respuesta_a': 431,\n",
       "             b'a_acci\\xc3\\xb3n': 86,\n",
       "             b'acci\\xc3\\xb3n_grupo': 4,\n",
       "             b'menor': 2636,\n",
       "             b'grupo_menor': 4,\n",
       "             b'menor_encapuchados': 2,\n",
       "             b'enfrent\\xc3\\xb3': 146,\n",
       "             b'encapuchados_enfrent\\xc3\\xb3': 3,\n",
       "             b'enfrent\\xc3\\xb3_carabineros': 1,\n",
       "             b'techumbre': 56,\n",
       "             b'carabineros_techumbre': 1,\n",
       "             b'profesor': 729,\n",
       "             b'techumbre_profesor': 1,\n",
       "             b'Claudio': 462,\n",
       "             b'profesor_Claudio': 1,\n",
       "             b'Segovia': 27,\n",
       "             b'Claudio_Segovia': 3,\n",
       "             b'indic\\xc3\\xb3': 6791,\n",
       "             b'Segovia_indic\\xc3\\xb3': 2,\n",
       "             b'salida': 2070,\n",
       "             b'indic\\xc3\\xb3_salida': 1,\n",
       "             b'salida_recinto': 2,\n",
       "             b'recinto_dio': 2,\n",
       "             b'dio_calle': 1,\n",
       "             b'Alonso': 88,\n",
       "             b'calle_Alonso': 2,\n",
       "             b'Ovalle': 128,\n",
       "             b'Alonso_Ovalle': 9,\n",
       "             b'Ovalle_dio': 1,\n",
       "             b'manera': 3891,\n",
       "             b'dio_manera': 1,\n",
       "             b'manera_espont\\xc3\\xa1nea': 7,\n",
       "             b'orden': 1870,\n",
       "             b'espont\\xc3\\xa1nea_orden': 1,\n",
       "             b'rector\\xc3\\xada': 19,\n",
       "             b'orden_rector\\xc3\\xada': 1,\n",
       "             b'vieron': 459,\n",
       "             b'rector\\xc3\\xada_vieron': 1,\n",
       "             b'afectados': 1222,\n",
       "             b'vieron_afectados': 58,\n",
       "             b'afectados_gases': 7,\n",
       "             b'policiales': 637,\n",
       "             b'gases_policiales': 1,\n",
       "             b'acus\\xc3\\xb3': 1348,\n",
       "             b'policiales_acus\\xc3\\xb3': 1,\n",
       "             b'actuar': 807,\n",
       "             b'acus\\xc3\\xb3_actuar': 2,\n",
       "             b'protocolo': 436,\n",
       "             b'actuar_protocolo': 1,\n",
       "             b'protocolo_Fuerzas': 1,\n",
       "             b'advirtiendo': 132,\n",
       "             b'Especiales_advirtiendo': 1,\n",
       "             b'trat\\xc3\\xb3': 551,\n",
       "             b'advirtiendo_trat\\xc3\\xb3': 1,\n",
       "             b'trat\\xc3\\xb3_situaci\\xc3\\xb3n': 5,\n",
       "             b'amerita': 28,\n",
       "             b'situaci\\xc3\\xb3n_amerita': 3,\n",
       "             b'sumario': 489,\n",
       "             b'amerita_sumario': 1,\n",
       "             b'interno': 340,\n",
       "             b'sumario_interno': 32,\n",
       "             b'gran': 4286,\n",
       "             b'interno_gran': 1,\n",
       "             b'n\\xc3\\xbamero': 2090,\n",
       "             b'gran_n\\xc3\\xbamero': 73,\n",
       "             b'n\\xc3\\xbamero_estudiantes': 9,\n",
       "             b'encontraban': 519,\n",
       "             b'estudiantes_encontraban': 7,\n",
       "             b'encontraban_clases': 3,\n",
       "             b'clases_vieron': 1,\n",
       "             b'afectados_situaci\\xc3\\xb3n': 10,\n",
       "             b'situaci\\xc3\\xb3n_produjo': 13,\n",
       "             b'produjo_estudiantes': 1,\n",
       "             b'encararan': 1,\n",
       "             b'estudiantes_encararan': 1,\n",
       "             b'encararan_a': 1,\n",
       "             b'a_carabineros': 67,\n",
       "             b'carabineros_a': 9,\n",
       "             b'a_salida': 184,\n",
       "             b'salida_establecimiento': 3,\n",
       "             b'establecimiento_situaci\\xc3\\xb3n': 5,\n",
       "             b'deriv\\xc3\\xb3': 191,\n",
       "             b'situaci\\xc3\\xb3n_deriv\\xc3\\xb3': 2,\n",
       "             b'empujones': 20,\n",
       "             b'deriv\\xc3\\xb3_empujones': 1,\n",
       "             b'empujones_ambos': 1,\n",
       "             b'grupos': 1404,\n",
       "             b'ambos_grupos': 9,\n",
       "             b'Producto': 274,\n",
       "             b'grupos_Producto': 1,\n",
       "             b'ocho': 1317,\n",
       "             b'Producto_ocho': 1,\n",
       "             b'ocho_personas': 81,\n",
       "             b'resultaron': 826,\n",
       "             b'personas_resultaron': 204,\n",
       "             b'detenidas': 463,\n",
       "             b'resultaron_detenidas': 7,\n",
       "             b'Alcalde': 242,\n",
       "             b'Sharp': 134,\n",
       "             b'Alcalde_Sharp': 3,\n",
       "             b'lamenta': 156,\n",
       "             b'Sharp_lamenta': 1,\n",
       "             b'mortal': 180,\n",
       "             b'lamenta_mortal': 1,\n",
       "             b'derrumbe': 296,\n",
       "             b'mortal_derrumbe': 1,\n",
       "             b'afirma': 996,\n",
       "             b'derrumbe_afirma': 1,\n",
       "             b'afirma_Valpara\\xc3\\xadso': 1,\n",
       "             b'ciudad': 4564,\n",
       "             b'Valpara\\xc3\\xadso_ciudad': 7,\n",
       "             b'vive': 1002,\n",
       "             b'ciudad_vive': 2,\n",
       "             b'riesgo': 2193,\n",
       "             b'vive_riesgo': 1,\n",
       "             b'alcalde': 2054,\n",
       "             b'riesgo_alcalde': 1,\n",
       "             b'alcalde_Valpara\\xc3\\xadso': 28,\n",
       "             b'Jorge': 1645,\n",
       "             b'Valpara\\xc3\\xadso_Jorge': 51,\n",
       "             b'Jorge_Sharp': 74,\n",
       "             b'refiri\\xc3\\xb3': 1005,\n",
       "             b'Sharp_refiri\\xc3\\xb3': 4,\n",
       "             b'refiri\\xc3\\xb3_ma\\xc3\\xb1ana': 5,\n",
       "             b'mi\\xc3\\xa9rcoles_mortal': 1,\n",
       "             b'derrumbre': 1,\n",
       "             b'mortal_derrumbre': 1,\n",
       "             b'registrado': 876,\n",
       "             b'derrumbre_registrado': 1,\n",
       "             b'anoche': 82,\n",
       "             b'registrado_anoche': 1,\n",
       "             b'anoche_Aldunate': 1,\n",
       "             b'cerro': 173,\n",
       "             b'Huito_cerro': 1,\n",
       "             b'Bellavista': 52,\n",
       "             b'cerro_Bellavista': 1,\n",
       "             b'primer': 4553,\n",
       "             b'Bellavista_primer': 1,\n",
       "             b'primer_lugar': 248,\n",
       "             b'extendi\\xc3\\xb3': 251,\n",
       "             b'lugar_extendi\\xc3\\xb3': 2,\n",
       "             b'condolencias': 147,\n",
       "             b'extendi\\xc3\\xb3_condolencias': 2,\n",
       "             b'condolencias_familiares': 1,\n",
       "             b'v\\xc3\\xadctimas': 2806,\n",
       "             b'familiares_v\\xc3\\xadctimas': 76,\n",
       "             b'v\\xc3\\xadctimas_asegurando': 1,\n",
       "             b'asegurando_ciudad': 1,\n",
       "             b'conmocionada': 21,\n",
       "             b'ciudad_conmocionada': 1,\n",
       "             b'conmocionada_ocurrido': 1,\n",
       "             b'Derrumbe': 17,\n",
       "             b'ocurrido_Derrumbe': 1,\n",
       "             b'Derrumbe_vivienda': 8,\n",
       "             b'vivienda_cerro': 12,\n",
       "             b'cerro_Valpara\\xc3\\xadso': 18,\n",
       "             b'rescatan': 40,\n",
       "             b'Valpara\\xc3\\xadso_rescatan': 7,\n",
       "             b'2': 2612,\n",
       "             b'rescatan_2': 7,\n",
       "             b'cuerpos': 656,\n",
       "             b'2_cuerpos': 9,\n",
       "             b'4': 2661,\n",
       "             b'cuerpos_4': 7,\n",
       "             b'4_v\\xc3\\xadctimas': 9,\n",
       "             b'fatales': 214,\n",
       "             b'v\\xc3\\xadctimas_fatales': 145,\n",
       "             b'valpara\\xc3\\xadso': 1,\n",
       "             b'fatales_valpara\\xc3\\xadso': 1,\n",
       "             b'ustedes': 326,\n",
       "             b'valpara\\xc3\\xadso_ustedes': 1,\n",
       "             b'dolor': 589,\n",
       "             b'ustedes_dolor': 1,\n",
       "             b'dolor_hoy': 1,\n",
       "             b'd\\xc3\\xada': 5417,\n",
       "             b'hoy_d\\xc3\\xada': 299,\n",
       "             b'comparte': 149,\n",
       "             b'd\\xc3\\xada_comparte': 1,\n",
       "             b'toda': 2243,\n",
       "             b'comparte_toda': 1,\n",
       "             b'toda_ciudad': 45,\n",
       "             b'manifest\\xc3\\xb3': 1654,\n",
       "             b'ciudad_manifest\\xc3\\xb3': 2,\n",
       "             b'manifest\\xc3\\xb3_Luego': 1,\n",
       "             b'lament\\xc3\\xb3': 577,\n",
       "             b'Luego_lament\\xc3\\xb3': 1,\n",
       "             b'lament\\xc3\\xb3_Valpara\\xc3\\xadso': 1,\n",
       "             b'permanentemente': 87,\n",
       "             b'vive_permanentemente': 1,\n",
       "             ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usamos `Phraser` para re-tokenizamos el corpus con los bigramas encontrados. Es decir, juntamos los tokens separados que detectamos como frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:20:57.432220Z",
     "start_time": "2020-05-07T19:20:13.544388Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:13:11: source_vocab length 3264063\n",
      "INFO - 23:13:35: Phraser built with 1054 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:21:03.964097Z",
     "start_time": "2020-05-07T19:21:03.958113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ministro', 'Vivienda', 'enfrenta', 'pol√©mica', 'casitas', 'calle', 'oficina', 'ministro', 'Vivienda_Urbanismo', 'Cristian', 'Monckeberg', 'refiri√≥', 'a', 'pol√©mica', 'reflexi√≥n', 'integraci√≥n', 'social', 'd√©ficit', 'habitacional', 'pa√≠s', 'Comisi√≥n', 'Vivienda_Urbanismo', 'Senado', 'se√±al√≥', 'gran', 'mayor√≠a', 'propietarios', 'patrimonio', 'casita', 'dos', 'departamentos', 'ah√≠', 'radica', 'patrimonio', 'chilenos', 'Tras', 'episodio', 'entrevista', 'programa', 'Podr√≠a_Ser', 'Peor', 'Julio', 'C√©sar', 'Rodr√≠guez', 'remarc√≥', 'dichos', 'parte', 'intervenci√≥n', 'larga', 'pasado', '14', 'agosto', 'siquiera', 'sacaron', 'contexto', 'escucha', 'grabaci√≥n', 'efectivamente', 'suena', 'horrorosa', 'explicaci√≥n', 'larga', 'tema', 'tratando', 'Senado', 'hecho', 'a', 'ning√∫n', 'senador', 'llam√≥_atenci√≥n', 'asegur√≥', 'Ministro', 'Vivienda', 'gran', 'mayor√≠a', 'propietarios', 'casita', 'dos', 'departamentos', 'paso', 'explic√≥', 'instancia', 'analizando', 'hecho', 'chilenos', 'culturalmente', 'propietarios', 'aspiran', 'a', 'ser', 'propietarios', 'Si', 'alguien', 'sinti√≥', 'ofendido', 'pido', 'disculpas', 'caso', 'se√±al√≥', '¬øCu√°ntas', 'personas', 'vivienda', 'Chile', 'mismo_tiempo', 'refiri√≥', 'a', 'cifras', 'us√≥', 'defenderse', 'Admiti√≥', 'dentro', '604', 'hogares', 'chilenos', '35', 'millones', 'habita', 'vivienda', 'propia', 'acuerdo', 'a', 'encuesta', 'Casen', '2017', 'considerados', 'aquellos', 'a√∫n', 'pagando', 'propiedad', 'ejemplo', 'mediante', 'cr√©dito', 'hipotecario', '‚Äì', '¬øY', 'ah√≠', 'qui√©n', 'casa', '‚Äì', 'pronto', 'hablo', 'mismo', 'departamento', 'estacionamiento', 'bodega', 'casa', 'banco', 'suerte', 'due√±o', 'ba√±o', 'todav√≠a', 'debo', '16_a√±os', '‚Äì', 'Entonces', 'propietarios', '‚Äì', 'propietarios', 'inscrito', 'a', 'nombre', 'debemos', 'deuda', 'importante', 'paga', 'mensualmente', 'misma_l√≠nea', 'defendi√≥', 'qui√©nes', 'critican', 'desapegado', 'realidad', 'chilena', 'Llevo', 'visitado', '80', 'comunas', 'podido', 'palpar', 'conoc√≠a', 'drama', 'necesidad', 'vivienda', 'miles', 'familias', 'asegur√≥', 'calle', 'oficina', 'molesta', 'cr√≠tica', 'reconoci√≥', 'Escucha_entrevista', 'completa', 'a', 'continuaci√≥n']\n"
     ]
    }
   ],
   "source": [
    "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
    "print(sentences[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "### Definir el modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos par√°metros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada.\n",
    "- `window` : Tama√±o de la ventana. Usaremos 4.\n",
    "- `size` : El tama√±o de los embeddings que crearemos. Por lo general, el rendimiento sube cuando se usan mas dimensiones, pero despu√©s de 300 ya no se nota cambio. Ahora, usaremos solo 200.\n",
    "- `workers`: Cantidad de CPU que ser√°n utilizadas en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:29:43.525865Z",
     "start_time": "2020-05-07T15:29:43.521840Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v = Word2Vec(min_count=10,\n",
    "                      window=4,\n",
    "                      size=200,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se crear√° un conjunto que contendr√° (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:30:09.864087Z",
     "start_time": "2020-05-07T15:29:45.813793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:13:54: collecting all words and their counts\n",
      "INFO - 23:13:54: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:13:58: PROGRESS: at sentence #10000, processed 2090563 words, keeping 99620 word types\n",
      "INFO - 23:14:02: PROGRESS: at sentence #20000, processed 4142515 words, keeping 151977 word types\n",
      "INFO - 23:14:05: collected 186290 word types from a corpus of 5536916 raw words and 26413 sentences\n",
      "INFO - 23:14:05: Loading a fresh vocabulary\n",
      "INFO - 23:14:06: effective_min_count=10 retains 38094 unique words (20% of original 186290, drops 148196)\n",
      "INFO - 23:14:06: effective_min_count=10 leaves 5202906 word corpus (93% of original 5536916, drops 334010)\n",
      "INFO - 23:14:06: deleting the raw counts dictionary of 186290 items\n",
      "INFO - 23:14:06: sample=6e-05 downsamples 1099 most-common words\n",
      "INFO - 23:14:06: downsampling leaves estimated 4050149 word corpus (77.8% of prior 5202906)\n",
      "INFO - 23:14:06: estimated required memory for 38094 words and 200 dimensions: 79997400 bytes\n",
      "INFO - 23:14:06: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "biobio_w2v.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuaci√≥n, entenaremos el modelo. \n",
    "Los par√°metros que usaremos ser√°n: \n",
    "\n",
    "- `total_examples`: N√∫mero de documentos.\n",
    "- `epochs`: N√∫mero de veces que se iterar√° sobre el corpus.\n",
    "\n",
    "Es recomendable que tengan instalado `cpython` antes de continuar. Aumenta bastante la velocidad de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:34:44.255083Z",
     "start_time": "2020-05-07T15:30:12.141203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:14:38: training model with 8 workers on 38094 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
      "INFO - 23:14:39: EPOCH 1 - PROGRESS: at 3.98% examples, 177373 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 23:14:49: EPOCH 1 - PROGRESS: at 56.35% examples, 202135 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:14:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:14:58: EPOCH - 1 : training on 5536916 raw words (4050798 effective words) took 20.0s, 202864 effective words/s\n",
      "INFO - 23:14:59: EPOCH 2 - PROGRESS: at 4.29% examples, 188006 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:09: EPOCH 2 - PROGRESS: at 53.71% examples, 192522 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:15:19: EPOCH 2 - PROGRESS: at 93.11% examples, 178073 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:15:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:15:21: EPOCH - 2 : training on 5536916 raw words (4051494 effective words) took 23.1s, 175706 effective words/s\n",
      "INFO - 23:15:22: EPOCH 3 - PROGRESS: at 3.69% examples, 164049 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:32: EPOCH 3 - PROGRESS: at 34.64% examples, 126560 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:42: EPOCH 3 - PROGRESS: at 60.30% examples, 112694 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:52: EPOCH 3 - PROGRESS: at 97.53% examples, 126323 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:15:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:15:53: EPOCH - 3 : training on 5536916 raw words (4049937 effective words) took 31.8s, 127424 effective words/s\n",
      "INFO - 23:15:54: EPOCH 4 - PROGRESS: at 3.69% examples, 157945 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:04: EPOCH 4 - PROGRESS: at 48.16% examples, 173998 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:14: EPOCH 4 - PROGRESS: at 91.50% examples, 174842 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:16:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:16:16: EPOCH - 4 : training on 5536916 raw words (4051150 effective words) took 23.1s, 175153 effective words/s\n",
      "INFO - 23:16:17: EPOCH 5 - PROGRESS: at 4.00% examples, 170797 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:27: EPOCH 5 - PROGRESS: at 45.69% examples, 165659 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:37: EPOCH 5 - PROGRESS: at 73.75% examples, 139833 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:46: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:16:46: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:16:46: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:16:46: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:16:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:16:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:16:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:16:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:16:47: EPOCH - 5 : training on 5536916 raw words (4048722 effective words) took 30.4s, 132978 effective words/s\n",
      "INFO - 23:16:48: EPOCH 6 - PROGRESS: at 2.65% examples, 111958 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:16:58: EPOCH 6 - PROGRESS: at 32.10% examples, 119110 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:08: EPOCH 6 - PROGRESS: at 63.35% examples, 119615 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:18: EPOCH 6 - PROGRESS: at 92.53% examples, 119918 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:17:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:17:20: EPOCH - 6 : training on 5536916 raw words (4049430 effective words) took 33.8s, 119822 effective words/s\n",
      "INFO - 23:17:21: EPOCH 7 - PROGRESS: at 2.65% examples, 114503 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:31: EPOCH 7 - PROGRESS: at 31.90% examples, 118709 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:41: EPOCH 7 - PROGRESS: at 63.69% examples, 120489 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:51: EPOCH 7 - PROGRESS: at 91.50% examples, 118683 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:17:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:17:54: EPOCH - 7 : training on 5536916 raw words (4051213 effective words) took 34.1s, 118969 effective words/s\n",
      "INFO - 23:17:55: EPOCH 8 - PROGRESS: at 2.81% examples, 115759 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:06: EPOCH 8 - PROGRESS: at 31.90% examples, 117840 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:16: EPOCH 8 - PROGRESS: at 62.35% examples, 117317 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:26: EPOCH 8 - PROGRESS: at 91.86% examples, 118838 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:18:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:18:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:18:28: EPOCH - 8 : training on 5536916 raw words (4049903 effective words) took 34.0s, 118959 effective words/s\n",
      "INFO - 23:18:29: EPOCH 9 - PROGRESS: at 2.49% examples, 108346 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:39: EPOCH 9 - PROGRESS: at 36.85% examples, 135649 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:50: EPOCH 9 - PROGRESS: at 76.07% examples, 144594 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:18:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:18:58: EPOCH - 9 : training on 5536916 raw words (4050670 effective words) took 29.1s, 139122 effective words/s\n",
      "INFO - 23:18:59: EPOCH 10 - PROGRESS: at 3.54% examples, 154582 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:19:09: EPOCH 10 - PROGRESS: at 45.86% examples, 166668 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:19:19: EPOCH 10 - PROGRESS: at 80.34% examples, 153142 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:19:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:19:24: EPOCH - 10 : training on 5536916 raw words (4049580 effective words) took 26.1s, 155186 effective words/s\n",
      "INFO - 23:19:25: EPOCH 11 - PROGRESS: at 3.54% examples, 158108 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:19:35: EPOCH 11 - PROGRESS: at 44.92% examples, 163373 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:19:45: EPOCH 11 - PROGRESS: at 81.57% examples, 155730 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:19:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:19:49: EPOCH - 11 : training on 5536916 raw words (4050487 effective words) took 25.5s, 158634 effective words/s\n",
      "INFO - 23:19:50: EPOCH 12 - PROGRESS: at 3.69% examples, 165808 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:20:00: EPOCH 12 - PROGRESS: at 44.17% examples, 161185 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:20:10: EPOCH 12 - PROGRESS: at 82.02% examples, 156918 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:20:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:20:16: EPOCH - 12 : training on 5536916 raw words (4050584 effective words) took 26.5s, 152973 effective words/s\n",
      "INFO - 23:20:17: EPOCH 13 - PROGRESS: at 2.49% examples, 108412 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:20:27: EPOCH 13 - PROGRESS: at 40.74% examples, 148795 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:20:37: EPOCH 13 - PROGRESS: at 71.44% examples, 135190 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:20:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:20:45: EPOCH - 13 : training on 5536916 raw words (4049808 effective words) took 29.3s, 138139 effective words/s\n",
      "INFO - 23:20:46: EPOCH 14 - PROGRESS: at 3.24% examples, 144574 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:20:56: EPOCH 14 - PROGRESS: at 41.35% examples, 150572 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:06: EPOCH 14 - PROGRESS: at 80.51% examples, 153645 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:11: EPOCH - 14 : training on 5536916 raw words (4048526 effective words) took 25.9s, 156164 effective words/s\n",
      "INFO - 23:21:12: EPOCH 15 - PROGRESS: at 3.69% examples, 158755 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:22: EPOCH 15 - PROGRESS: at 44.92% examples, 162880 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:32: EPOCH 15 - PROGRESS: at 79.65% examples, 151571 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:39: EPOCH - 15 : training on 5536916 raw words (4050228 effective words) took 27.7s, 146287 effective words/s\n",
      "INFO - 23:21:39: training on a 83053740 raw words (60752530 effective words) took 420.6s, 144450 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 7.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. \n",
    "Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:34:46.358716Z",
     "start_time": "2020-05-07T15:34:46.331779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:25:07: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "biobio_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:23:19.548567Z",
     "start_time": "2020-05-07T19:23:18.572531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:26:43: saving Word2Vec object under ./pretrained_models/biobio_w2v.model, separately None\n",
      "INFO - 23:26:43: not storing attribute vectors_norm\n",
      "INFO - 23:26:43: not storing attribute cum_table\n",
      "INFO - 23:26:44: saved ./pretrained_models/biobio_w2v.model\n",
      "INFO - 23:26:44: loading Word2VecKeyedVectors object from ./pretrained_models/biobio_w2v.model\n",
      "INFO - 23:26:44: loading wv recursively from ./pretrained_models/biobio_w2v.model.wv.* with mmap=r\n",
      "INFO - 23:26:44: setting ignored attribute vectors_norm to None\n",
      "INFO - 23:26:44: loading vocabulary recursively from ./pretrained_models/biobio_w2v.model.vocabulary.* with mmap=r\n",
      "INFO - 23:26:44: loading trainables recursively from ./pretrained_models/biobio_w2v.model.trainables.* with mmap=r\n",
      "INFO - 23:26:44: setting ignored attribute cum_table to None\n",
      "INFO - 23:26:44: loaded ./pretrained_models/biobio_w2v.model\n"
     ]
    }
   ],
   "source": [
    "# Si entrenaste el modelo y lo quieres guardar, descomentar el siguiente bloque.\n",
    "if not os.path.exists('./pretrained_models'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
    "\n",
    "\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:05:18.639618Z",
     "start_time": "2020-05-07T17:02:10.981724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:27:05: loading Word2VecKeyedVectors object from ./pretrained_models/biobio_w2v.model\n",
      "INFO - 23:27:06: loading wv recursively from ./pretrained_models/biobio_w2v.model.wv.* with mmap=r\n",
      "INFO - 23:27:06: setting ignored attribute vectors_norm to None\n",
      "INFO - 23:27:06: loading vocabulary recursively from ./pretrained_models/biobio_w2v.model.vocabulary.* with mmap=r\n",
      "INFO - 23:27:06: loading trainables recursively from ./pretrained_models/biobio_w2v.model.trainables.* with mmap=r\n",
      "INFO - 23:27:06: setting ignored attribute cum_table to None\n",
      "INFO - 23:27:06: loaded ./pretrained_models/biobio_w2v.model\n"
     ]
    }
   ],
   "source": [
    "# descargar el modelo desde github\n",
    "def read_model_from_github(url):\n",
    "    if not os.path.exists('./pretrained_models'):\n",
    "        os.mkdir('./pretrained_models')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    filename = url.split('/')[-1]\n",
    "    with open('./pretrained_models/' + filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "\n",
    "[\n",
    "    read_model_from_github(file) for file in [\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model',\n",
    "    ]\n",
    "]\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Palabras mas similares y Analog√≠as\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la informaci√≥n contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matem√°tico, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:28:33.343259Z",
     "start_time": "2020-05-07T19:28:33.262476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:27:19: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gato', 0.7324815392494202),\n",
       " ('perrito', 0.7017662525177002),\n",
       " ('cachorro', 0.6726856231689453),\n",
       " ('canino', 0.6614428758621216),\n",
       " ('mascota', 0.6354753971099854),\n",
       " ('animal', 0.6341222524642944),\n",
       " ('gatito', 0.6259311437606812),\n",
       " ('felino', 0.622412919998169),\n",
       " ('perros', 0.6207761764526367),\n",
       " ('perra', 0.5834704041481018)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:28:59.154190Z",
     "start_time": "2020-05-07T19:28:59.146214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ecuador', 0.6255874633789062),\n",
       " ('peruana', 0.582399308681488),\n",
       " ('Paraguay', 0.5646696090698242),\n",
       " ('peruano', 0.5636229515075684),\n",
       " ('Colombia', 0.5616293549537659),\n",
       " ('Uruguay', 0.5335642099380493),\n",
       " ('Mart√≠n_Vizcarra', 0.5327112078666687),\n",
       " ('peruanos', 0.5254102945327759),\n",
       " ('Lima', 0.5234850645065308),\n",
       " ('Brasil', 0.5057570934295654)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Per√∫\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:49.464492Z",
     "start_time": "2020-05-07T19:29:49.432164Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'trump' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b4968b07cd1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbiobio_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trump\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'trump' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:56.028795Z",
     "start_time": "2020-05-07T19:29:56.014832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Casa_Blanca', 0.709988534450531),\n",
       " ('Donald_Trump', 0.7093803882598877),\n",
       " ('mandatario_estadounidense', 0.7046915292739868),\n",
       " ('inquilino', 0.678024411201477),\n",
       " ('administraci√≥n_Trump', 0.6302800178527832),\n",
       " ('Bolton', 0.6201831102371216),\n",
       " ('Unidos', 0.6197685599327087),\n",
       " ('Washington', 0.6131006479263306),\n",
       " ('presidente_estadounidense', 0.6014897227287292),\n",
       " ('Unidos_Donald', 0.5976207256317139)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:32:29.842609Z",
     "start_time": "2020-05-07T19:32:29.829644Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hut', 0.9047655463218689),\n",
       " ('Telepizza', 0.8243373036384583),\n",
       " ('ATT', 0.7196983098983765),\n",
       " ('Linio', 0.6911829113960266),\n",
       " ('Homecenter', 0.6828977465629578),\n",
       " ('Unimarc', 0.6740168333053589),\n",
       " ('Natura', 0.6694700717926025),\n",
       " ('Nestl√©', 0.6646097302436829),\n",
       " ('Coffee', 0.6622934937477112),\n",
       " ('Sodimac', 0.6596288681030273)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:32:39.355210Z",
     "start_time": "2020-05-07T19:32:39.348228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hamburguesas', 0.6893606185913086),\n",
       " ('cafeter√≠as', 0.6606760025024414),\n",
       " ('degustaciones', 0.6389386653900146),\n",
       " ('repartidor', 0.6389126777648926),\n",
       " ('pizzas', 0.6339085102081299),\n",
       " ('platos', 0.6290442943572998),\n",
       " ('arroz', 0.6215819716453552),\n",
       " ('hamburguesa', 0.6195734739303589),\n",
       " ('vodka', 0.6181467771530151),\n",
       " ('chocolates', 0.616828441619873)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:32:59.165524Z",
     "start_time": "2020-05-07T19:32:59.153557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cabify', 0.7779495716094971),\n",
       " ('Eats', 0.700801432132721),\n",
       " ('DiDi', 0.676181435585022),\n",
       " ('Rappi', 0.6054359078407288),\n",
       " ('Conductores', 0.5858488082885742),\n",
       " ('aplicaciones', 0.5700352787971497),\n",
       " ('Beat', 0.566635251045227),\n",
       " ('app', 0.5640519857406616),\n",
       " ('choferes', 0.5415352582931519),\n",
       " ('taxistas', 0.5351073741912842)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Uber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:34.899941Z",
     "start_time": "2020-05-07T19:33:34.885980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ZTE', 0.7025798559188843),\n",
       " ('telecomunicaciones', 0.639496922492981),\n",
       " ('Wanzhou', 0.617560863494873),\n",
       " ('5G', 0.6000876426696777),\n",
       " ('Meng', 0.5700865387916565),\n",
       " ('Pekin', 0.5595781803131104),\n",
       " ('Ren', 0.54143226146698),\n",
       " ('gigante_asi√°tico', 0.5363011956214905),\n",
       " ('sanciones_estadounidenses', 0.5254279375076294),\n",
       " ('China', 0.5246084928512573)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Huawei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:41.167456Z",
     "start_time": "2020-05-07T19:33:41.155482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Televisi√≥n', 0.6506631374359131),\n",
       " ('directorio', 0.5564517974853516),\n",
       " ('Orrego', 0.48779889941215515),\n",
       " ('C5N', 0.47816959023475647),\n",
       " ('Panorama', 0.47550541162490845),\n",
       " ('Chilevisi√≥n', 0.46353697776794434),\n",
       " ('cupr√≠fera', 0.4621236324310303),\n",
       " ('matinal', 0.45264914631843567),\n",
       " ('CHV', 0.4483409821987152),\n",
       " ('Mega', 0.44625139236450195)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"TVN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:47.881474Z",
     "start_time": "2020-05-07T19:33:47.871502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excapit√°n_Ej√©rcito', 0.7039800882339478),\n",
       " ('ultraderecha', 0.7019724249839783),\n",
       " ('ultraderechista_Jair', 0.661270022392273),\n",
       " ('extrema_derecha', 0.6610966324806213),\n",
       " ('Finlandeses', 0.659309983253479),\n",
       " ('Verdaderos', 0.6557667851448059),\n",
       " ('antiinmigraci√≥n', 0.6375353336334229),\n",
       " ('exmilitar', 0.636165201663971),\n",
       " ('Liga', 0.6324677467346191),\n",
       " ('Matteo_Salvini', 0.6320110559463501)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:55.044392Z",
     "start_time": "2020-05-07T19:33:55.033420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('penquista', 0.6831767559051514),\n",
       " ('Talcahuano', 0.6503714919090271),\n",
       " ('Chiguayante', 0.6319462060928345),\n",
       " ('Temuco', 0.6043856143951416),\n",
       " ('Santiago', 0.6022074222564697),\n",
       " ('Penco', 0.586882472038269),\n",
       " ('Hualqui', 0.5604559779167175),\n",
       " ('Hualp√©n', 0.5555928945541382),\n",
       " ('Lota', 0.5550185441970825),\n",
       " ('Chill√°n', 0.5531082153320312)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Concepci√≥n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analog√≠as\n",
    "\n",
    "Por otra parte, la analog√≠a consiste en comparar 3 terminos mediante una operaci√≥n del estilo: \n",
    "\n",
    "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
    "\n",
    "para encontrar relaciones entre estos.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| palabra 1 (pos) |  palabra 2 (neg) |\n",
    "|-----------------|------------------|\n",
    "|  macri          | pi√±era           |\n",
    "| chile           |  x               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:36:24.480150Z",
     "start_time": "2020-05-07T19:36:24.462198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brasil', 0.665826678276062),\n",
       " ('Jair_Bolsonaro', 0.5471663475036621),\n",
       " ('ultraderechista_Jair', 0.516360878944397),\n",
       " ('Paraguay', 0.4729083180427551),\n",
       " ('excapit√°n_Ej√©rcito', 0.47257742285728455),\n",
       " ('Ultraderechista', 0.4531181752681732),\n",
       " ('Colombia', 0.45093128085136414),\n",
       " ('nost√°lgico', 0.4260634183883667),\n",
       " ('presidente_electo', 0.42563045024871826),\n",
       " ('Per√∫', 0.4201669692993164)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Bolsonaro\", \"Argentina\"], negative=['Macri'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:36:10.444907Z",
     "start_time": "2020-05-07T19:36:10.435930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Falabella', 0.47030484676361084),\n",
       " ('portabilidad', 0.4300999641418457),\n",
       " ('Multicaja', 0.4203709363937378),\n",
       " ('CMR', 0.41495582461357117),\n",
       " ('Sodimac', 0.4064332842826843),\n",
       " ('Valledor', 0.3986559212207794),\n",
       " ('Telef√≥nica', 0.39562681317329407),\n",
       " ('Ciberseguridad', 0.39511701464653015),\n",
       " ('Subtel', 0.39427271485328674),\n",
       " ('retail', 0.39312079548835754)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Chile\", \"Huawei\"], negative=['China'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar\n",
    "\n",
    "Para visualizar, reduciremos las 200 dimensiones a 2. Para esto, usaremos [`T-SNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:38:06.260264Z",
     "start_time": "2020-05-07T19:38:06.208183Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EMBEDDINGS_TO_VISUALIZE = 10000\n",
    "\n",
    "word_counts = dict()\n",
    "for item in biobio_w2v.wv.vocab:\n",
    "    word_counts[item]=biobio_w2v.wv.vocab[item].count\n",
    "    \n",
    "sorted_word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:38:12.009463Z",
     "start_time": "2020-05-07T19:38:11.999489Z"
    }
   },
   "outputs": [],
   "source": [
    "words_to_visualize = list(sorted_word_counts.keys())[0:NUM_EMBEDDINGS_TO_VISUALIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:38:18.104614Z",
     "start_time": "2020-05-07T19:38:18.066715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 200)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wv_to_visualize = np.array([biobio_w2v.wv[word] for word in words_to_visualize])\n",
    "wv_to_visualize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:39:45.349695Z",
     "start_time": "2020-05-07T19:38:28.069987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 10000 samples in 0.200s...\n",
      "[t-SNE] Computed neighbors for 10000 samples in 22.416s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 10000\n",
      "[t-SNE] Mean sigma: 0.324630\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 97.095749\n",
      "[t-SNE] KL divergence after 1000 iterations: 2.501570\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos las transformaciones.\n",
    "tsne_components = TSNE(n_components=2, verbose=True,\n",
    "                  n_jobs=-1).fit_transform(wv_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la frecuencia de las palabras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos vectores que nos entrego TSNE mas el vocabulario y la frecuencia del vocabulario, formamos un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:39:50.210194Z",
     "start_time": "2020-05-07T19:39:50.202212Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = pd.DataFrame({\n",
    "    'x': tsne_components[:,0],\n",
    "    'y': tsne_components[:,1],\n",
    "    'vocab': words_to_visualize , \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la visualizaci√≥n, usaremos plotly. Este nos deja tener una barra interactiva que nos mostrar√° las 100 palabras mas parecidas a la que escribimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:39:55.187070Z",
     "start_time": "2020-05-07T19:39:55.171110Z"
    }
   },
   "outputs": [],
   "source": [
    "described_tsne = tsne.describe()\n",
    "min_y = described_tsne.y['min']\n",
    "max_y = described_tsne.y['max']\n",
    "\n",
    "min_x =  described_tsne.x['min']\n",
    "max_x =  described_tsne.x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:40:00.737661Z",
     "start_time": "2020-05-07T19:40:00.709715Z"
    }
   },
   "outputs": [],
   "source": [
    "textbox = widgets.Text(description='Palabra:')\n",
    "\n",
    "\n",
    "def validate(word):\n",
    "    if word in tsne.vocab.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def response(change):\n",
    "\n",
    "    word = textbox.value\n",
    "\n",
    "    if (word == ''):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = tsne.x.values\n",
    "            fig.data[0].y = tsne.y.values\n",
    "            fig.data[0].text = tsne.vocab.values\n",
    "    else:\n",
    "        if validate(word):\n",
    "\n",
    "            most_similar_words = [word]\n",
    "\n",
    "            for word_tuple in biobio_w2v.wv.most_similar(positive=[word],\n",
    "                                                         topn=100):\n",
    "                if word_tuple[0] in tsne.vocab.values:\n",
    "                    most_similar_words.append(word_tuple[0])\n",
    "\n",
    "            filtered_words = [\n",
    "                word in most_similar_words for word in tsne.vocab\n",
    "            ]\n",
    "            temp = tsne.loc[filtered_words]\n",
    "\n",
    "            with fig.batch_update():\n",
    "                fig.data[0].x = temp.x.values\n",
    "                fig.data[0].y = temp.y.values\n",
    "                fig.data[0].text = temp.vocab.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:27.345723Z",
     "start_time": "2020-05-07T19:44:26.863014Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4ad5df6873449894d854d6cfb9a310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Palabra:'),)), FigureWidget({\n",
       "    'data': [{'mode': ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget(data=[\n",
    "    go.Scatter(x=tsne.x,\n",
    "               y=tsne.y,\n",
    "               mode='markers',\n",
    "               text=tsne.vocab)\n",
    "],\n",
    "                      layout=go.Layout(title=go.layout.Title(\n",
    "                          text=\"Visualizaci√≥n 2D Embeddings Biobio\"),\n",
    "                                       yaxis=dict(range=[min_y, max_y]),\n",
    "                                       xaxis=dict(range=[min_x, max_x])))\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=800,\n",
    ")\n",
    "container = widgets.HBox([textbox])\n",
    "textbox.observe(response, names=\"value\")\n",
    "\n",
    "widgets.VBox([container, fig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:55:09.459852Z",
     "start_time": "2019-08-28T19:55:09.454865Z"
    }
   },
   "source": [
    "## Word Embeddings como caracter√≠sticas para clasificar\n",
    "\n",
    "\n",
    "En esta secci√≥n, veremos como utilizar los word embeddings como caracter√≠stica para **clasificar nuevamente el t√≥pico de las noticias de la radio biobio**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, obtendremos los datos y sus categor√≠as y dejamos solo las primeras 20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:39.200212Z",
     "start_time": "2020-05-07T19:44:39.176275Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_r.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:45.081609Z",
     "start_time": "2020-05-07T19:44:45.065652Z"
    }
   },
   "outputs": [],
   "source": [
    "# creamos una nueva columna titulo y contenido.\n",
    "content = dataset['content'] \n",
    "\n",
    "# obtenemos las clases\n",
    "subcategory = dataset.subcategory\n",
    "\n",
    "# dejamos en el dataset solo contenido de la noticia y categoria\n",
    "dataset = pd.DataFrame({'content': content, 'category': subcategory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:58.537303Z",
     "start_time": "2020-05-07T19:44:58.486439Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 250\n",
    "\n",
    "categorias_seleccionadas = [\n",
    "    'america-latina', 'eeuu', 'europa', 'chile', 'region-metropolitana',\n",
    "    'region-del-bio-bio', 'negocios-y-empresas', 'region-de-los-lagos',\n",
    "    'actualidad-economica', 'region-de-valparaiso', 'region-de-la-araucania',\n",
    "    'curiosidades', 'asia', 'region-de-los-rios', 'entrevistas', 'debates',\n",
    "    'mediooriente', 'viral', 'animales', 'tu-bolsillo'\n",
    "]\n",
    "\n",
    "# filtrar solo categorias seleccionadas\n",
    "dataset = dataset[dataset['category'].isin(categorias_seleccionadas)]\n",
    "\n",
    "# balancear clases\n",
    "g = dataset.groupby('category')\n",
    "dataset = pd.DataFrame(\n",
    "    g.apply(lambda x: x.sample(NUM_SAMPLES).reset_index(drop=True))\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, transformamos cada documento del dataset en el promedio de sus embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:25.817110Z",
     "start_time": "2019-09-23T20:43:25.797172Z"
    }
   },
   "source": [
    "### Dividir el dataset en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:45:05.304455Z",
     "start_time": "2020-05-07T19:45:05.296477Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
    "                                                    dataset.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, crearemos el Transformer con el cual convertiremos el documento a vector. (puede que les sirva para la tarea...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:42:22.105698Z",
     "start_time": "2020-05-17T20:42:22.094754Z"
    }
   },
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, aggregation_func):\n",
    "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
    "        self.model = model.wv \n",
    "        \n",
    "        # indicamos la funci√≥n de agregaci√≥n (np.min, np.max, np.mean, np.sum, ...)\n",
    "        self.aggregation_func = aggregation_func\n",
    "\n",
    "    def simple_tokenizer(self, doc, lower=False):\n",
    "        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y \n",
    "           separa el tweet por espacios.\n",
    "        \"\"\"\n",
    "        if lower:\n",
    "            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        doc_embeddings = []\n",
    "        \n",
    "        for doc in X:\n",
    "            # tokenizamos el documento. Se llevan todos los tokens a min√∫scula. \n",
    "            # ojo con esto, ya que puede que tokens con min√∫scula y may√∫scula tengan\n",
    "            # distintas representaciones\n",
    "            tokens = self.simple_tokenizer(doc, lower = True) \n",
    "            \n",
    "            selected_wv = []\n",
    "            for token in tokens:\n",
    "                if token in self.model.vocab:\n",
    "                    selected_wv.append(self.model[token])\n",
    "                    \n",
    "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo a√±adimos.\n",
    "            if len(selected_wv) > 0:\n",
    "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
    "                doc_embeddings.append(doc_embedding)\n",
    "            # si no, a√±adimos un vector de ceros que represente a ese documento.\n",
    "            else: \n",
    "                print('No pude encontrar ning√∫n embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
    "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
    "\n",
    "        return np.array(doc_embeddings)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos el pipeline\n",
    "\n",
    "\n",
    "Usaremos la transformaci√≥n que creamos antes mas una regresi√≥n log√≠stica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:50:34.393237Z",
     "start_time": "2020-05-07T19:50:34.387253Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "doc2vec_mean = Doc2VecTransformer(biobio_w2v, np.mean)\n",
    "doc2vec_sum = Doc2VecTransformer(biobio_w2v, np.sum)\n",
    "doc2vec_max = Doc2VecTransformer(biobio_w2v, np.max)\n",
    "\n",
    "pipeline = Pipeline([('doc2vec', doc2vec_sum), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:50:59.881521Z",
     "start_time": "2020-05-07T19:50:40.532331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('doc2vec',\n",
       "                 Doc2VecTransformer(aggregation_func=<function sum at 0x7f1f8c0a5b80>,\n",
       "                                    model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f1f1e299070>)),\n",
       "                ('clf', LogisticRegression(max_iter=1000000))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:47.337340Z",
     "start_time": "2019-09-23T20:43:47.317007Z"
    }
   },
   "source": [
    "**Predecimos y evaluamos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:07.108644Z",
     "start_time": "2020-05-07T19:51:05.799089Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:12.601957Z",
     "start_time": "2020-05-07T19:51:12.589989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0  0  9  0  1  2  0  0 15  2  0  0  0  0  0 12  0]\n",
      " [ 0 68  0  1  5  1  5  0  0  2  0  1  1  0  1  0  1  1  2]\n",
      " [ 0  0 53  0  0  5  1  0  2  0  0  0  1  0  1  0  0  0 12]\n",
      " [ 1  2  3 54  0  5  7  0  2  1  0  0  0  0  0  1  0  1  1]\n",
      " [ 4  0  0  1 56  1  0  2  1  0  2  7  3  1  3  3  8  9  0]\n",
      " [ 2  0  2  4  0 49  3  1  3  1  2  0  0  0  0  1  1  1 10]\n",
      " [ 2  3  2 11  0  1 52  0  7  4  1  0  0  0  0  0  0  0  3]\n",
      " [ 0  1  0  0  8  0  0 61  0  0  1  1  0  0  1  1  1  0  0]\n",
      " [ 1  2  1  3  0  3  4  0 70  3  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  1  0  5  0  1 12  0  6 60  0  0  0  0  0  0  0  0  0]\n",
      " [13  0  0  0  3  2  0  1  1  0 39  0  0  0  0  5  0 14  0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0  0 60  4  6  5  6  1  0  0]\n",
      " [ 1  0  0  0  3  0  0  0  0  0  0  7 52  5  4  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  3  8 63  1  2  2  0  0]\n",
      " [ 0  0  0  0  6  0  0  2  0  0  3  1  3  2 40 11  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  0  0  3  5  5  1  1 55  4  0  2]\n",
      " [ 1  0  0  0  8  0  0  2  0  0  4  2  0  0  7  1 55  1  2]\n",
      " [11  0  0  0  6  1  0  0  2  0 15  1  1  0  0  0  0 49  1]\n",
      " [ 0  1  8  1  0 14  2  0  1  0  0  0  0  0  0  0  0  0 47]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:18.863055Z",
     "start_time": "2020-05-07T19:51:18.805699Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "  actualidad-economica       0.58      0.55      0.56        91\n",
      "        america-latina       0.87      0.76      0.81        89\n",
      "              animales       0.77      0.71      0.74        75\n",
      "                  asia       0.68      0.69      0.68        78\n",
      "                 chile       0.52      0.55      0.54       101\n",
      "          curiosidades       0.59      0.61      0.60        80\n",
      "                  eeuu       0.60      0.60      0.60        86\n",
      "           entrevistas       0.82      0.81      0.82        75\n",
      "                europa       0.74      0.80      0.77        88\n",
      "          mediooriente       0.85      0.71      0.77        85\n",
      "   negocios-y-empresas       0.45      0.50      0.48        78\n",
      "region-de-la-araucania       0.67      0.70      0.68        86\n",
      "   region-de-los-lagos       0.67      0.68      0.67        77\n",
      "    region-de-los-rios       0.81      0.79      0.80        80\n",
      "  region-de-valparaiso       0.62      0.53      0.57        76\n",
      "    region-del-bio-bio       0.60      0.70      0.65        79\n",
      "  region-metropolitana       0.68      0.66      0.67        83\n",
      "           tu-bolsillo       0.56      0.56      0.56        87\n",
      "                 viral       0.58      0.64      0.61        74\n",
      "\n",
      "              accuracy                           0.66      1568\n",
      "             macro avg       0.67      0.66      0.66      1568\n",
      "          weighted avg       0.66      0.66      0.66      1568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:25.139392Z",
     "start_time": "2020-05-07T19:51:25.132412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['region-de-la-araucania'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(\n",
    "    [(\"Alguna noticia...\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:57:03.817026Z",
     "start_time": "2019-09-24T12:57:03.814008Z"
    }
   },
   "source": [
    "## Usandolo con BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:39:09.997634Z",
     "start_time": "2020-05-07T16:39:09.992647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf_2 = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Definimos el pipeline\n",
    "pipeline_2 = Pipeline([('features',\n",
    "                        FeatureUnion([('bow', CountVectorizer()),\n",
    "                                      ('doc2vec', doc2vec_sum)])), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:44:04.514995Z",
     "start_time": "2020-05-07T16:39:13.770150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('bow', CountVectorizer()),\n",
       "                                                ('doc2vec',\n",
       "                                                 Doc2VecTransformer(aggregation_func=<function sum at 0x7f1f8c0a5b80>,\n",
       "                                                                    model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f1f1e299070>))])),\n",
       "                ('clf', LogisticRegression(max_iter=1000000))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:44:08.965415Z",
     "start_time": "2020-05-07T16:44:07.164065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59  0  0  0  8  1  0  1  0  0 14  0  0  0  0  1  0  7  0]\n",
      " [ 0 69  1  2  4  1  5  0  0  1  2  1  0  0  2  0  0  0  1]\n",
      " [ 0  0 60  0  0  1  0  0  1  0  0  0  0  1  0  0  0  0 12]\n",
      " [ 0  1  4 57  0  6  5  0  3  0  0  0  0  0  0  0  0  0  2]\n",
      " [ 6  0  0  1 63  1  0  2  2  0  2  4  3  1  3  2  7  4  0]\n",
      " [ 1  0  0  5  0 51  0  1  2  0  0  0  0  0  1  0  0  2 17]\n",
      " [ 1  2  0 12  0  2 61  0  2  4  0  0  0  0  0  0  0  0  2]\n",
      " [ 0  1  0  0  1  0  0 69  0  0  1  0  0  0  2  0  1  0  0]\n",
      " [ 1  3  1  3  0  4  3  0 69  2  0  1  0  0  0  0  0  0  1]\n",
      " [ 0  1  0  3  0  2  6  0  3 69  0  0  0  0  0  0  0  0  1]\n",
      " [14  1  0  0  0  2  0  1  0  0 45  0  0  0  0  4  1 10  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0 74  2  5  1  1  2  0  0]\n",
      " [ 1  0  0  0  2  0  0  0  0  0  1  1 63  3  3  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  2  0 74  1  0  1  1  0]\n",
      " [ 2  0  0  0  6  0  0  0  0  0  3  2  0  1 49  8  5  0  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  0  2  2  3  2  2 61  4  1  0]\n",
      " [ 1  1  0  0  4  0  0  1  0  0  2  0  0  1  5  1 64  1  2]\n",
      " [ 9  0  0  0  5  4  0  0  0  0 12  1  0  0  0  0  0 56  0]\n",
      " [ 0  1  6  1  0 10  0  0  0  0  0  0  0  0  0  0  0  2 54]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_2 = pipeline_2.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_2)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:44:11.609183Z",
     "start_time": "2020-05-07T16:44:11.553969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "  actualidad-economica       0.61      0.65      0.63        91\n",
      "        america-latina       0.86      0.78      0.82        89\n",
      "              animales       0.83      0.80      0.82        75\n",
      "                  asia       0.68      0.73      0.70        78\n",
      "                 chile       0.68      0.62      0.65       101\n",
      "          curiosidades       0.59      0.64      0.61        80\n",
      "                  eeuu       0.76      0.71      0.73        86\n",
      "           entrevistas       0.91      0.92      0.91        75\n",
      "                europa       0.84      0.78      0.81        88\n",
      "          mediooriente       0.91      0.81      0.86        85\n",
      "   negocios-y-empresas       0.53      0.58      0.55        78\n",
      "region-de-la-araucania       0.84      0.86      0.85        86\n",
      "   region-de-los-lagos       0.89      0.82      0.85        77\n",
      "    region-de-los-rios       0.84      0.93      0.88        80\n",
      "  region-de-valparaiso       0.71      0.64      0.68        76\n",
      "    region-del-bio-bio       0.75      0.77      0.76        79\n",
      "  region-metropolitana       0.75      0.77      0.76        83\n",
      "           tu-bolsillo       0.67      0.64      0.65        87\n",
      "                 viral       0.59      0.73      0.65        74\n",
      "\n",
      "              accuracy                           0.74      1568\n",
      "             macro avg       0.75      0.75      0.75      1568\n",
      "          weighted avg       0.75      0.74      0.75      1568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias en Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:02:04.707045Z",
     "start_time": "2020-05-07T20:02:04.675157Z"
    }
   },
   "outputs": [],
   "source": [
    "from wefe.query import Query\n",
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "from wefe.metrics import RND, WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:06:09.985618Z",
     "start_time": "2020-05-07T20:06:09.979635Z"
    }
   },
   "outputs": [],
   "source": [
    "male_words = ['hombre', 'hombres', 'ni√±o', 'padre', 't√≠o', 'abuelo', 'sobrino']\n",
    "female_words = ['mujer', 'mujeres', 'ni√±a', 'madre', 't√≠a', 'abuela', 'sobrina']\n",
    "\n",
    "career = [\n",
    "    'ingeniero', 'ingeniera','abogado', 'm√©dico', 'astronauta', 'presidente',\n",
    "    'investigador', 'juez', 'acad√©mico'\n",
    "]\n",
    "\n",
    "science = [\n",
    "    'ciencia', 'tecnolog√≠a', 'f√≠sica', 'qu√≠mica', 'astronom√≠a', 'biolog√≠a',\n",
    "    'investigaci√≥n', 'computaci√≥n'\n",
    "]\n",
    "arts = [\n",
    "    'arte', 'm√∫sica', 'danza', 'cocina', 'cine', 'lectura', 'escritura',\n",
    "    'libros', 'libro', 'ficci√≥n', 'teatro'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:06:01.029061Z",
     "start_time": "2020-05-07T20:06:01.023079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ingeniera' in biobio_w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:02:16.765474Z",
     "start_time": "2020-05-07T20:02:16.758493Z"
    }
   },
   "outputs": [],
   "source": [
    "model = WordEmbeddingModel(biobio_w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:06:16.285136Z",
     "start_time": "2020-05-07T20:06:16.278154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Hombre and Mujer wrt Carrera',\n",
       " 'result': -0.08261020978291829,\n",
       " 'results_by_word': {'ingeniero': -0.13043869,\n",
       "  'astronauta': -0.12255514,\n",
       "  'acad√©mico': -0.114564896,\n",
       "  'abogado': -0.112694025,\n",
       "  'presidente': -0.104246974,\n",
       "  'investigador': -0.09778023,\n",
       "  'juez': -0.06920445,\n",
       "  'm√©dico': 0.0027772188,\n",
       "  'ingeniera': 0.005215287}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RND().run_query(\n",
    "    Query([male_words, female_words], [career], ['Hombre', 'Mujer'],\n",
    "          ['Carrera']), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:03:02.693925Z",
     "start_time": "2020-05-07T20:03:02.685941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Hombre and Mujer wrt Ciencia',\n",
       " 'result': -0.02750334143638611,\n",
       " 'results_by_word': {'biolog√≠a': -0.050181866,\n",
       "  'computaci√≥n': -0.045604825,\n",
       "  'f√≠sica': -0.031781554,\n",
       "  'investigaci√≥n': -0.026688933,\n",
       "  'astronom√≠a': -0.023275137,\n",
       "  'tecnolog√≠a': -0.022155404,\n",
       "  'qu√≠mica': -0.016297221,\n",
       "  'ciencia': -0.004041791}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RND().run_query(\n",
    "    Query([male_words, female_words], [science], ['Hombre', 'Mujer'],\n",
    "          ['Ciencia']), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:03:40.273279Z",
     "start_time": "2020-05-07T20:03:40.266298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Hombre and Mujer wrt Arte',\n",
       " 'result': -0.03800590471787886,\n",
       " 'results_by_word': {'teatro': -0.075146556,\n",
       "  'libros': -0.070794106,\n",
       "  'arte': -0.0655961,\n",
       "  'libro': -0.048403025,\n",
       "  'ficci√≥n': -0.041079164,\n",
       "  'm√∫sica': -0.03923595,\n",
       "  'danza': -0.03646016,\n",
       "  'cine': -0.033591866,\n",
       "  'escritura': -0.021597385,\n",
       "  'cocina': -0.00086045265,\n",
       "  'lectura': 0.014699817}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RND().run_query(\n",
    "    Query([male_words, female_words], [arts], ['Hombre', 'Mujer'],\n",
    "          ['Arte']), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propuesto...\n",
    "\n",
    "- Usar su modelo de embeddings favorito para ver si mejora la clasificaci√≥n: \n",
    "    \n",
    " - Fast y word2vec en espa√±ol, [cortes√≠a](https://github.com/dccuchile/spanish-word-embeddings) de los grandes del DCC\n",
    " - [Conceptnet](https://github.com/commonsense/conceptnet-numberbatch)\n",
    "\n",
    "\n",
    "- Visualizar los documentos usando `doc2vec`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
