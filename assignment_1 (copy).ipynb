{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    },
    "colab_type": "text",
    "id": "PkfIPpmVXK1J"
   },
   "source": [
    "# Tarea 1 NLP : Competencia de Clasificación de Texto\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssXRUJcBXK1L"
   },
   "source": [
    "- **Nombre:** Jou-Hui Ho Ku, Martín Valderrama\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** PibJou\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUUQEnHxXK1M"
   },
   "source": [
    "## Objetivo e Instrucciones:\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificación de tweets según su intensidad de emoción. Específicamente: \n",
    "\n",
    "Tendrán 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deberán crear un clasificador que indique la intensidad de dicha emoción en sus tweets (`low`, `medium`, `high`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVvANhT6XK1N"
   },
   "source": [
    "###  Fecha de Entrega: \n",
    "\n",
    "Por ser anunciada una vez termine el paro. Se publicará la fecha en ucursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:34:38.796217Z",
     "start_time": "2020-04-07T14:34:38.782255Z"
    },
    "colab_type": "text",
    "id": "CG1T7EWnXK1O"
   },
   "source": [
    "### Detalles e instrucciones de la competencia:\n",
    "\n",
    "- La competencia consiste en resolver 4 problemas de clasificación distintos, cada uno de tres clases. Por cada problema deberán crear un clasificador distinto. La evaluación de la competencia se realiza en base a 4 métricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada ítem serán los que ganen.\n",
    "\n",
    "- Para comenzar se les entregará en este notebook el baseline y la estructura del reporte. El baseline es el código que realiza creación de features y clasificación básica. Los puntajes de este serán ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
    "\n",
    "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
    "\n",
    "- **Es requisito entregar el reporte con el código y haber participado en la competencia para ser evaluado.**\n",
    "\n",
    "- Pueden hacer grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y después cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
    "\n",
    "- En total pueden hacer un **máximo de 4 envíos/submissions** (tanto para equipos como para envíos indivuales).\n",
    "\n",
    "- Hagan varios experimentos haciendo cross-validation o evaluación sobre una sub-partición antes de enviar sus predicciones a Codalab. Asegúrense que la distribución de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les será evaluado incorrectamente.\n",
    "\n",
    "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n",
    "\n",
    "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. ¡Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
    "\n",
    "- Todas las dudas escríbanlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUE0Y0f3XK1P"
   },
   "source": [
    "### Reporte\n",
    "\n",
    "Este debe cumplir la siguiente estructura:\n",
    "\n",
    "1.\t**Introducción**: Presentar brevemente el problema a resolver, los métodos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano. Mas abajo encontrarán una lista útil de estos que les podrá ser de utilidad. (1.5 puntos)\n",
    "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificación usados. (0.5 puntos)\n",
    "4.\t**Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación indicando que miden y su interpretación. (0.5 puntos)\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partición de evaluación que deben crear (o pueden usar cross-validation). Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    },
    "colab_type": "text",
    "id": "DuCEFL_aXK1Q"
   },
   "source": [
    "### Baseline\n",
    "\n",
    "Por último, el baseline contiene un código básico que:\n",
    "\n",
    "- Obtiene los dataset.\n",
    "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizadas.\n",
    "    - Transforma los dataset a bag of words (BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook. \n",
    "\n",
    "\n",
    "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendrá mas sentido cuando vean el código)\n",
    "\n",
    "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del el enunciado. Investigar también el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aquí les adjuntamos algunos ejemplos:\n",
    "    -\tWord n-grams.\n",
    "    -\tCharacter n-grams. \n",
    "    -\tPart-of-speech tags.\n",
    "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
    "        - Count the number of positive and negative words within a sentence.\n",
    "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
    "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
    "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
    "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
    "    -\tThe number of words with all characters in uppercase.\n",
    "    -\tThe presence and the number of positive or negative emoticons.\n",
    "    -\tThe number of individual negations.\n",
    "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
    "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
    "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
    "\n",
    "- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:25:19.677190Z",
     "start_time": "2020-04-07T15:25:19.671206Z"
    },
    "colab_type": "text",
    "id": "DdS9xmSWXK1Q"
   },
   "source": [
    "(Pueden eliminar cualquier celda con instrucciones...)\n",
    "\n",
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFhbF8WvXK1R"
   },
   "source": [
    "----------------------------------------\n",
    "\n",
    "(borrar este bloque despues)\n",
    "1.\t**Introducción**: Presentar brevemente el problema a resolver, los métodos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano. Mas abajo encontrarán una lista útil de estos que les podrá ser de utilidad. (1.5 puntos)\n",
    "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificación usados. (0.5 puntos)\n",
    "4.\t**Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación indicando que miden y su interpretación. (0.5 puntos)\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partición de evaluación que deben crear (o pueden usar cross-validation). Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    },
    "colab_type": "text",
    "id": "pmH9j-eYXK1S"
   },
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_d8xLu1XK1T"
   },
   "source": [
    "El problema a abordar en este trabajo es el análisis de sentimientos de tweets. Se tiene un dataset de tweets clasificados por sentimiento (joy, fear, sadness, anger), y en cada sentimiento, a su vez, los tweets se clasifican por intensidad (alto, medio o bajo).\n",
    "\n",
    "Para resolver este problema, se utilizan distintas representaciones de los datos, tales como Bag Of Words, la representación Tf-Idf, Word Embeddings, y también se extraen características relevantes de los textos, como por ejemplo la cantidad de signos de exclamación que contengan.\n",
    "\n",
    "Una vez convertidos los textos a su representación, se utilizan métodos de NaiveBayes, SVM, regresión logística y MLP para clasificar el nivel de sentimiento en cada clase.\n",
    "\n",
    "Para realizar tal tarea, se sigue el código entregado por el cuerpo docente a modo de marco de trabajo. En este se realiza la carga de datos, la creación de las transformaciones de los datos y la creación de los clasificadores, todo en utilizando las facultades de la librería sklearn con pipeline y feature union.\n",
    "\n",
    "Se realizan múltiples experimentos para obtener los modelos que mejor clasifiquen a los tweets y con ello después realizar una predicción sobre el conjunto de testeo que después es subido a la competencia de codalab.\n",
    "\n",
    "Con esta tarea se pretende abordar la metodología de trabajo para participar en competencias de Data Science donde se deben crear múltiples instancias de extracción de características y creación de clasificadores. Además, se deben aplicar los conocimientos adquiridos en el curso de Procesamiento de Lenguaje Natural para obtener los mejores rendimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    },
    "colab_type": "text",
    "id": "NJ2NBZFSXK1U"
   },
   "source": [
    "## 2. Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    },
    "colab_type": "text",
    "id": "Bl1LMOz6XK1W"
   },
   "source": [
    "Se implementan distintas representaciones de los textos, mediante las siguientes clases:\n",
    "\n",
    "   \n",
    "- **CountVectorizer** \\\n",
    "    Consiste en una representación de los textos en Bag of Words.\n",
    "    \n",
    "- **TfidfVectorizer**\\\n",
    "    Calcula la matriz Tf-Idf asociada al dataset. El algoritmo funciona sobre un tokenizer el cual por defecto se realiza separando solamente las palabras del tweet en este caso. Se puede utilizar con distintos Tokenizers, en esta tarea se prueba un caso con el TweetTokenizer de la librería nltk.\n",
    "    \n",
    "- **TweetTokenizer**\\\n",
    "    Es un tokenizer pre-definido de la librería nltk, el cual reconoce carácteres especiales de los mensajes de Twitter como emojis en símbolos o puntutación, incluso hashtags.\n",
    "    \n",
    "\n",
    "- **CharCountTransformer**: \\\n",
    "    Esta representación considera una serie de conteos \"manuales\" implementados a continuación. Esta incluye:\n",
    "    - **Caracteres relevantes dentro del texto**, tales como:\n",
    "        - Cantidad de signos de exclamación\n",
    "        - Cantidad de signos de interrogación\n",
    "        - Cantidad de letras repetidas más de tres veces\n",
    "        - Cantidad de letras en mayúscula\n",
    "        - Cantidad de puntos seguidos \n",
    "    - Se utiliza la librería _emoji_ para codificar los emojis de los tweets, y la librería _emosent_ para extraer un sentiment score del emoji, además de la polaridad del sentimiento. Cabe destacar que existen algunos emojis que no están en la librería _emosent_, por lo que fue necesario evaluar su polaridad bajo inspección, y evaluar si el nombre del emoji contiene palabras positivas o negativas.\n",
    "    - Además, se marcan previamente las palabras que siguen a una negación, concatenando el string \\_NEG luego de la palabra, para que sea considerada diferente a la misma palabra, pero en un contexto positivo.\n",
    " \n",
    "- **Doc2VecTransformer** \\\n",
    "    Word embeddings con el modelo _glove_twitter_ ya entrenado. Se considera el máximo de cada tweet, considerando que se quiere realizar un análisis de sentimientos, y otras operaciones como suma o promedio podrían causar una pérdida de información.\n",
    "    \n",
    "- **LiuFeatureExtractor**\\\n",
    "    Extrae la cantidad de palabras positivas y palabras negativas, usando el dataset de Bing Liu.\n",
    "\n",
    "- **VaderFeatureExtractor**\\\n",
    "    https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_H6tUAGPXK1Y"
   },
   "source": [
    "## 3. Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1P0yclvXK1Z"
   },
   "source": [
    "Los algoritmos utilizados en este trabajo son:\n",
    "\n",
    "- **Naive Bayes** es un algoritmo de búsqueda de pertenencia a una clase dada de acuerdo al contenido del documento. Compara las palabras presentes en el documento con las presentes en el vocabulario pertenenciente a la clase, obteniendo la verosimilitud de que dicho documento pertenezca a dicha clase. Compara cada una de las probabilidades para obtener la pertenencia.\n",
    "\n",
    "- **SVM** es un clasificador basado en vectores de soporte, son los que determinan el margen entre dos clases. La forma de separar las clases es maximizando el margen de la clase con el resto de los datos, esto con la aproximación del hiperplano de separación.\n",
    "\n",
    "- **Logistic Regression** \n",
    "\n",
    "- **MLP Classifier** es un clasificador basado en redes neuronales totalmente conectadas, puede tener múltiples capas ocultas, mientras más neuronas tenga el clasificador, más parámetros tendrá que entrenar con backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    },
    "colab_type": "text",
    "id": "KkGR_Y9qXK1a"
   },
   "source": [
    "## 4. Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvQGaiqaXK1b"
   },
   "source": [
    "- AUC: es una métrica que mide la capacidad del modelo de distinguir entre clases variando el threshold de separación, es decir, la probabilidad con la que se considera la pertenencia a una clase. En la imagen inferior obtenida de [towardsdatascience.com](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5), se muestra una separación entre las distribuciones de probabilidad de cada clase y la ROC resultante al mover el threshold.\n",
    "\n",
    "- Kappa: El coeficiente Cohen's Kappa es un estadístico de la capacidad del modelo de \"estar de acuerdo\" con otro anotador o clasificador, en este caso, con el anotador de los labels que consideramos reales. El coeficiente de Kappa se calcula como:\n",
    "\n",
    "$$K = \\frac{0A - AC}{1 - AC}$$\n",
    "\n",
    "Donde $0A$ es el accuracy obtenido, y $AC$ es la probabilida de haberle acertado por coincidencia. Un coeficiente de Cohen Kappa superior a 0.8 indica un buen rendimiento del modelo sobre haberle acertado por coincidencia. Mientras que un coeficiente inferior o negativo, indica un mal rendimiento del modelo.\n",
    "\n",
    "- Accuracy: El accuracy es el coeficiente de evaluación más usado para evaluar a un modelo. Con él se calcula el nivel de acuerdo que tiene el modelo con los labels considerados reales, se calcula como la suma de la diagonal de la matriz de confusión sobre el total de las muestras consideradas en la evaluación. Mientras más cercano a 1 quire decir que el modelo se parece más a los labels considerados reales, mientras que un accuracy 0, quiere decir que el modelo no pudo predecir correctamente ninguna etiqueta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBu_sP0KXK1b"
   },
   "source": [
    "## 5. Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGBQNys8XK1c"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    },
    "colab_type": "text",
    "id": "ul8vukH2XK1d"
   },
   "source": [
    "### Importar librerías y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.587160Z",
     "start_time": "2020-04-07T15:44:19.319386Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "68Lt-v5IXK1e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import opinion_lexicon, stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# !pip install -U emojis\n",
    "# !pip install emosent-py\n",
    "# !pip install emoji --upgrade\n",
    "import emoji\n",
    "import emojis\n",
    "from emosent import get_emoji_sentiment_rank\n",
    "\n",
    "import gensim.downloader as api\n",
    "import string\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('opinion_lexicon')\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPtqQdif8BC3"
   },
   "outputs": [],
   "source": [
    "negative_words = pd.read_csv('https://raw.githubusercontent.com/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/master/data_t1/opinion-lexicon-English/negative-words.csv',\n",
    "                             sep='\\n', header=None, error_bad_lines=False)\n",
    "positive_words = pd.read_csv('https://raw.githubusercontent.com/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/master/data_t1/opinion-lexicon-English/positive-words.csv',\n",
    "                             sep='\\n', header=None, error_bad_lines=False)\n",
    "\n",
    "positive_words = np.hstack((list(positive_words),\n",
    "                            'hugging_face',\n",
    "                            'popcorn'))\n",
    "negative_words = np.hstack((list(negative_words),\n",
    "                            'spider',\n",
    "                            'face_with_head-bandage',\n",
    "                            'upside-down_face',\n",
    "                            'face_with_rolling_eyes',\n",
    "                            'zipper-mouth_face',\n",
    "                            'slightly_frowning_face'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiar puntuaciones y separar por tokens.\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6631 | INFO | loading projection weights from /home/martin/gensim-data/glove-twitter-25/glove-twitter-25.gz\n",
      "6631 | INFO | loaded (1193514, 25) matrix from /home/martin/gensim-data/glove-twitter-25/glove-twitter-25.gz\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec Model\n",
    "glove_twitter = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "# glove_twitter.most_similar(\"cat\")\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sib6XRh6XK1u"
   },
   "source": [
    "### Definir métodos de evaluación\n",
    "\n",
    "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.604066Z",
     "start_time": "2020-04-07T15:44:20.589106Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "N4WVlJ3UXK1v"
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "#     print('\\nClassification Report:\\n')\n",
    "#     print(\n",
    "#         classification_report(y_test,\n",
    "#                               predicted_labels,\n",
    "#                               labels=['low', 'medium', 'high']))\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C622AbJeXK1z"
   },
   "source": [
    "### Datos\n",
    "\n",
    "Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.068137Z",
     "start_time": "2020-04-07T15:44:20.606061Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rne8BE61XK10"
   },
   "outputs": [],
   "source": [
    "# Datasets de entrenamiento.\n",
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "# Datasets que deberán predecir para la competencia.\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.088707Z",
     "start_time": "2020-04-07T15:44:21.069757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "X5x06HPdXK14",
    "outputId": "bfbf69ca-c292-4998-f5c3-85c92da0c52f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ejemplo de algunas filas aleatorias:\n",
    "low_len, med_len, high_len = [], [], []\n",
    "for index, row in train['fear'].iterrows():\n",
    "    if row.sentiment_intensity=='low':\n",
    "        low_len.append(len(row.tweet))\n",
    "    if row.sentiment_intensity=='medium':\n",
    "        med_len.append(len(row.tweet))\n",
    "    if row.sentiment_intensity=='high':\n",
    "        high_len.append(len(row.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12., 21., 23., 31., 28., 26., 27., 23., 38., 59.]),\n",
       " array([ 11. ,  24.3,  37.6,  50.9,  64.2,  77.5,  90.8, 104.1, 117.4,\n",
       "        130.7, 144. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(low_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 14.,  36.,  43.,  60.,  55.,  80.,  50.,  80., 174., 107.]),\n",
       " array([ 10. ,  24.2,  38.4,  52.6,  66.8,  81. ,  95.2, 109.4, 123.6,\n",
       "        137.8, 152. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQf0lEQVR4nO3dfYxldX3H8fenoDSiLeCOdAtsB+hqg0YXnFCMD6FalQcD0jR0N0ZRSVcTSbU1MYsmapuYYBVpTRVdyxZsEFEQJYAPlBpJk4ouiuvyJA8uZTfL7igWrRoq8O0f90y8zs52Hu6dvXd/vF/JyZzz+517z5cfM585+7vnnElVIUlqy2+NugBJ0vAZ7pLUIMNdkhpkuEtSgwx3SWrQgaMuAGDFihU1OTk56jIkab9y6623/qiqJubqG4twn5ycZPPmzaMuQ5L2K0ke2Fuf0zKS1CDDXZIaZLhLUoMMd0lq0LzhnmRTkt1Jtva1XZnktm7ZluS2rn0yyS/7+j6xnMVLkua2kKtlLgX+Cfj0TENV/cXMepILgUf69r+vqtYMq0BJ0uLNG+5VdXOSybn6kgQ4G3j5cMuSJA1i0Dn3lwK7quqevrajk3w3yTeSvHTA95ckLcGgNzGtA67o294JrKqqHyd5IfDFJM+tqp/OfmGS9cB6gFWrVg1YhiSp35LDPcmBwJ8BL5xpq6pHgUe79VuT3Ac8G9jj9tOq2ghsBJiamvIvhkhicsP1IznutgtOH8lxl9Mg0zJ/CtxVVdtnGpJMJDmgWz8GWA3cP1iJkqTFWsilkFcA/wk8J8n2JOd2XWv5zSkZgJcBW7pLI68C3lpVDw+zYEnS/BZytcy6vbS/cY62q4GrBy9LkjQI71CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatC84Z5kU5LdSbb2tb0/yY4kt3XLaX195ye5N8ndSV69XIVLkvZuIWfulwKnzNF+UVWt6ZYbAJIcB6wFntu95uNJDhhWsZKkhZk33KvqZuDhBb7fmcBnq+rRqvohcC9w4gD1SZKWYJA59/OSbOmmbQ7t2o4AHuzbZ3vXtock65NsTrJ5enp6gDIkSbMtNdwvBo4F1gA7gQsX+wZVtbGqpqpqamJiYollSJLmsqRwr6pdVfV4VT0BfIpfT73sAI7q2/XIrk2StA8tKdyTrOzbPAuYuZLmWmBtkoOSHA2sBr41WImSpMU6cL4dklwBnAysSLIdeB9wcpI1QAHbgLcAVNXtST4H3AE8Brytqh5fntIlSXszb7hX1bo5mi/5f/b/APCBQYqSJA3GO1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGjRvuCfZlGR3kq19bR9KcleSLUmuSXJI1z6Z5JdJbuuWTyxn8ZKkuS3kzP1S4JRZbTcCz6uq5wM/AM7v67uvqtZ0y1uHU6YkaTHmDfequhl4eFbb16rqsW7zm8CRy1CbJGmJhjHn/mbgy33bRyf5bpJvJHnp3l6UZH2SzUk2T09PD6EMSdKMgcI9yXuAx4DLu6adwKqqOh74G+AzSX5nrtdW1caqmqqqqYmJiUHKkCTNsuRwT/JG4DXA66qqAKrq0ar6cbd+K3Af8Owh1ClJWoQlhXuSU4B3AWdU1S/62ieSHNCtHwOsBu4fRqGSpIU7cL4dklwBnAysSLIdeB+9q2MOAm5MAvDN7sqYlwF/l+RXwBPAW6vq4TnfWJK0bOYN96paN0fzJXvZ92rg6kGLkiQNxjtUJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatC8z3OXpNZNbrh+ZMfedsHpy/K+nrlLUoMMd0lqkOEuSQ0y3CWpQYa7JDVoQeGeZFOS3Um29rUdluTGJPd0Xw/t2pPko0nuTbIlyQnLVbwkaW4LPXO/FDhlVtsG4KaqWg3c1G0DnAqs7pb1wMWDlylJWowFhXtV3Qw8PKv5TOCybv0y4LV97Z+unm8ChyRZOYxiJUkLM8ic++FVtbNbfwg4vFs/Aniwb7/tXdtvSLI+yeYkm6enpwcoQ5I021A+UK2qAmqRr9lYVVNVNTUxMTGMMiRJnUHCfdfMdEv3dXfXvgM4qm+/I7s2SdI+Mki4Xwuc062fA3ypr/0N3VUzJwGP9E3fSJL2gQU9OCzJFcDJwIok24H3ARcAn0tyLvAAcHa3+w3AacC9wC+ANw25ZknSPBYU7lW1bi9dr5hj3wLeNkhRkqTBeIeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGLejP7ElPZpMbrh/JcbddcPpIjguj+2/W8HjmLkkNWvKZe5LnAFf2NR0DvBc4BPhLYLprf3dV3bDkCiVJi7bkcK+qu4E1AEkOAHYA1wBvAi6qqg8PpUJJ0qINa1rmFcB9VfXAkN5PkjSAYYX7WuCKvu3zkmxJsinJoUM6hiRpgQYO9yRPBc4APt81XQwcS2/KZidw4V5etz7J5iSbp6en59pFkrREwzhzPxX4TlXtAqiqXVX1eFU9AXwKOHGuF1XVxqqaqqqpiYmJIZQhSZoxjHBfR9+UTJKVfX1nAVuHcAxJ0iIMdBNTkoOBVwJv6Wv++yRrgAK2zeqTJO0DA4V7Vf0ceOasttcPVJEkaWDeoSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBg304DA9+UxuuH4kx912wekjOa60v/LMXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQV4to/3CqK7SkfZXnrlLUoMMd0lq0MDTMkm2AT8DHgceq6qpJIcBVwKTwDbg7Kr6yaDHkiQtzLDO3P+kqtZU1VS3vQG4qapWAzd125KkfWS5pmXOBC7r1i8DXrtMx5EkzWEYV8sU8LUkBXyyqjYCh1fVzq7/IeDw2S9Ksh5YD7Bq1aohlCG1xSuENIhhhPtLqmpHkmcBNya5q7+zqqoLfma1bwQ2AkxNTe3RL0lauoGnZapqR/d1N3ANcCKwK8lKgO7r7kGPI0lauIHCPcnBSZ4xsw68CtgKXAuc0+12DvClQY4jSVqcQadlDgeuSTLzXp+pqq8k+TbwuSTnAg8AZw94HEnSIgwU7lV1P/CCOdp/DLxikPeWJC2dd6hKUoMMd0lqkOEuSQ3ykb/7IW9ukTQfz9wlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ17kPwOvNJY0rz9wlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBSw73JEcl+XqSO5LcnuTtXfv7k+xIclu3nDa8ciVJCzHI4wceA95ZVd9J8gzg1iQ3dn0XVdWHBy9PkrQUSw73qtoJ7OzWf5bkTuCIYRUmSVq6ocy5J5kEjgdu6ZrOS7IlyaYkh+7lNeuTbE6yeXp6ehhlSJI6A4d7kqcDVwPvqKqfAhcDxwJr6J3ZXzjX66pqY1VNVdXUxMTEoGVIkvoMFO5JnkIv2C+vqi8AVNWuqnq8qp4APgWcOHiZkqTFGORqmQCXAHdW1Uf62lf27XYWsHXp5UmSlmKQq2VeDLwe+H6S27q2dwPrkqwBCtgGvGWgCiVJizbI1TL/AWSOrhuWXo4kaRi8Q1WSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYNchPT2JjccP2oS5CkseKZuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoOWLdyTnJLk7iT3JtmwXMeRJO1pWcI9yQHAx4BTgeOAdUmOW45jSZL2tFxn7icC91bV/VX1v8BngTOX6ViSpFmW6491HAE82Le9Hfjj/h2SrAfWd5v/k+TuZaplqVYAPxp1EQtkrctjf6l1f6kTrHUP+eBAL/+DvXWM7C8xVdVGYOOojj+fJJuramrUdSyEtS6P/aXW/aVOsNZ9abmmZXYAR/VtH9m1SZL2geUK928Dq5McneSpwFrg2mU6liRplmWZlqmqx5KcB3wVOADYVFW3L8exltHYThnNwVqXx/5S6/5SJ1jrPpOqGnUNkqQh8w5VSWqQ4S5JDXrSh3uSo5J8PckdSW5P8vau/bAkNya5p/t66KhrnZHkgCTfTXJdt310klu6Rz1c2X2IPXJJDklyVZK7ktyZ5EXjOq5J/rr7/781yRVJfntcxjXJpiS7k2zta5tzHNPz0a7mLUlOGINaP9R9D2xJck2SQ/r6zu9qvTvJq0dda1/fO5NUkhXd9kjHdSme9OEOPAa8s6qOA04C3tY9KmEDcFNVrQZu6rbHxduBO/u2PwhcVFV/CPwEOHckVe3pH4GvVNUfAS+gV/PYjWuSI4C/Aqaq6nn0LgJYy/iM66XAKbPa9jaOpwKru2U9cPE+qnHGpexZ643A86rq+cAPgPMBup+ztcBzu9d8vHt0yb5yKXvWSpKjgFcB/9XXPOpxXbyqculbgC8BrwTuBlZ2bSuBu0ddW1fLkfR+mF8OXAeE3l10B3b9LwK+OgZ1/i7wQ7oP7fvax25c+fUd1YfRu4LsOuDV4zSuwCSwdb5xBD4JrJtrv1HVOqvvLODybv184Py+vq8CLxp1rcBV9E5GtgErxmVcF7t45t4nySRwPHALcHhV7ey6HgIOH1FZs/0D8C7giW77mcB/V9Vj3fZ2emE1akcD08C/dFNI/5zkYMZwXKtqB/BhemdqO4FHgFsZz3GdsbdxnOvRH+NU95uBL3frY1drkjOBHVX1vVldY1frfAz3TpKnA1cD76iqn/b3Ve9X9civGU3yGmB3Vd066loW4EDgBODiqjoe+DmzpmDGaFwPpfdgu6OB3wcOZo5/ro+rcRnH+SR5D71p0MtHXctckjwNeDfw3lHXMgyGO5DkKfSC/fKq+kLXvCvJyq5/JbB7VPX1eTFwRpJt9J60+XJ689qHJJm5IW1cHvWwHdheVbd021fRC/txHNc/BX5YVdNV9SvgC/TGehzHdcbexnEsH/2R5I3Aa4DXdb+MYPxqPZbeL/jvdT9jRwLfSfJ7jF+t83rSh3uSAJcAd1bVR/q6rgXO6dbPoTcXP1JVdX5VHVlVk/Q+iPr3qnod8HXgz7vdxqXWh4AHkzyna3oFcAdjOK70pmNOSvK07vthptaxG9c+exvHa4E3dFd3nAQ80jd9MxJJTqE3lXhGVf2ir+taYG2Sg5IcTe/Dym+NokaAqvp+VT2rqia7n7HtwAnd9/LYjeu8Rj3pP+oFeAm9f9JuAW7rltPozWXfBNwD/Btw2KhrnVX3ycB13fox9H4o7gU+Dxw06vq6utYAm7ux/SJw6LiOK/C3wF3AVuBfgYPGZVyBK+h9FvAreoFz7t7Gkd4H7B8D7gO+T+8KoFHXei+9+eqZn69P9O3/nq7Wu4FTR13rrP5t/PoD1ZGO61IWHz8gSQ160k/LSFKLDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoP8DFqSTI/BSaQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(med_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4., 18., 21., 27., 28., 27., 29., 24., 60., 32.]),\n",
       " array([ 14. ,  27.8,  41.6,  55.4,  69.2,  83. ,  96.8, 110.6, 124.4,\n",
       "        138.2, 152. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAO/0lEQVR4nO3df6zdd13H8efLlYGA0pVdal2Ht0oDmcT9yM3cAjG68WP8yDYTsowQrdqk/6AOJcEOEhMS/9ii4YcJog2DNWbAZgHbbBGsZcSYmMItjLGtmyujkzbtetGNXyZC4e0f51t2vD239/Tee+45n/J8JCfn++v0+9on97763eec72mqCklSe35m3AEkSUtjgUtSoyxwSWqUBS5JjbLAJalRa1bzZBdeeGFNT0+v5iklqXkHDhz4VlVNzd++qgU+PT3N7Ozsap5SkpqX5MlB251CkaRGWeCS1CgLXJIaZYFLUqMscElqlAUuSY0aqsCTrE2yK8mjSQ4muTrJuiR7kzzePV8w6rCSpGcNewX+QeCzVfUK4FLgILAd2FdVm4F93bokaZUsWuBJXgT8BnAHQFX9oKqeAW4AdnaH7QRuHFVISdLphrkTcxMwB3wsyaXAAeAWYH1VHeuOOQ6sH/TiJNuAbQAvfelLlx1YUvumt983lvMevu1NYznvqAwzhbIGuAL4cFVdDnyfedMl1ftnfQb+0z5VtaOqZqpqZmrqtFv5JUlLNEyBHwGOVNX+bn0XvUJ/KskGgO75xGgiSpIGWbTAq+o48M0kL+82XQs8AuwBtnTbtgC7R5JQkjTQsN9G+EfAXUnOB54Afp9e+d+TZCvwJHDTaCJKkgYZqsCr6gFgZsCua1c2jiRpWN6JKUmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNWjPMQUkOA98FfgScrKqZJOuAu4Fp4DBwU1U9PZqYkqT5zuYK/Leq6rKqmunWtwP7qmozsK9blyStkuVModwA7OyWdwI3Lj+OJGlYwxZ4Af+c5ECSbd229VV1rFs+Dqwf9MIk25LMJpmdm5tbZlxJ0ilDzYEDr66qo0leAuxN8mj/zqqqJDXohVW1A9gBMDMzM/AYSdLZG+oKvKqOds8ngM8AVwJPJdkA0D2fGFVISdLpFi3wJC9I8nOnloHXAQ8Be4At3WFbgN2jCilJOt0wUyjrgc8kOXX8x6vqs0m+BNyTZCvwJHDT6GJKkuZbtMCr6gng0gHb/wu4dhShJEmL805MSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRq6wJOcl+QrSe7t1jcl2Z/kUJK7k5w/upiSpPnO5gr8FuBg3/rtwPur6mXA08DWlQwmSTqzoQo8yUbgTcBHuvUA1wC7ukN2AjeOIqAkabBhr8A/ALwL+HG3/mLgmao62a0fAS4a9MIk25LMJpmdm5tbVlhJ0rMWLfAkbwZOVNWBpZygqnZU1UxVzUxNTS3lj5AkDbBmiGNeBVyf5I3A84CfBz4IrE2yprsK3wgcHV1MSdJ8i16BV9WtVbWxqqaBm4HPV9XbgPuBt3SHbQF2jyylJOk0y/kc+J8Bf5rkEL058TtWJpIkaRjDTKH8RFV9AfhCt/wEcOXKR5IkDcM7MSWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIadVb/Kr0ktWx6+31jOe/h2940kj/XK3BJapQFLkmNssAlqVGLFniS5yX5YpKvJnk4yXu77ZuS7E9yKMndSc4ffVxJ0inDXIH/L3BNVV0KXAZcl+Qq4Hbg/VX1MuBpYOvoYkqS5lu0wKvne93qc7pHAdcAu7rtO4EbR5JQkjTQUHPgSc5L8gBwAtgLfB14pqpOdoccAS5a4LXbkswmmZ2bm1uJzJIkhizwqvpRVV0GbASuBF4x7AmqakdVzVTVzNTU1BJjSpLmO6tPoVTVM8D9wNXA2iSnbgTaCBxd4WySpDMY5lMoU0nWdss/C7wWOEivyN/SHbYF2D2qkJKk0w1zK/0GYGeS8+gV/j1VdW+SR4BPJvkL4CvAHSPMKZ2zzrXbu7V6Fi3wqnoQuHzA9ifozYdLksbAOzElqVF+G6EmitMJ0vC8ApekRlngktQoC1ySGuUcuE4zrnnocfpp/G9W+7wCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRnknpvRTyrtP2+cVuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNco7MSeYd8pJOpNFr8CTXJzk/iSPJHk4yS3d9nVJ9iZ5vHu+YPRxJUmnDDOFchJ4Z1VdAlwFvD3JJcB2YF9VbQb2deuSpFWyaIFX1bGq+nK3/F3gIHARcAOwsztsJ3DjqEJKkk53Vm9iJpkGLgf2A+ur6li36ziwfoHXbEsym2R2bm5uGVElSf2GLvAkLwQ+Bbyjqr7Tv6+qCqhBr6uqHVU1U1UzU1NTyworSXrWUAWe5Dn0yvuuqvp0t/mpJBu6/RuAE6OJKEkaZJhPoQS4AzhYVe/r27UH2NItbwF2r3w8SdJChvkc+KuA3wG+luSBbtu7gduAe5JsBZ4EbhpNREnSIIsWeFX9G5AFdl+7snEkScPyVnpJapS30i/C29klTSqvwCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1KIFnuSjSU4keahv27oke5M83j1fMNqYkqT5hrkCvxO4bt627cC+qtoM7OvWJUmraNECr6p/Bf573uYbgJ3d8k7gxhXOJUlaxFLnwNdX1bFu+TiwfqEDk2xLMptkdm5ubomnkyTNt+w3MauqgDrD/h1VNVNVM1NTU8s9nSSps9QCfyrJBoDu+cTKRZIkDWOpBb4H2NItbwF2r0wcSdKwhvkY4SeAfwdenuRIkq3AbcBrkzwOvKZblyStojWLHVBVb11g17UrnEWSdBa8E1OSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGLfp94JNievt9444gSRPFK3BJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjVpWgSe5LsljSQ4l2b5SoSRJi1tygSc5D/gQ8AbgEuCtSS5ZqWCSpDNbzhX4lcChqnqiqn4AfBK4YWViSZIWs5xvI7wI+Gbf+hHg1+cflGQbsK1b/V6Sx5ZxzrN1IfCtVTzfcpl39FrLbN7RWpW8uX3Zf8QvDdo48q+TraodwI5Rn2eQJLNVNTOOcy+FeUevtczmHa3W8s63nCmUo8DFfesbu22SpFWwnAL/ErA5yaYk5wM3A3tWJpYkaTFLnkKpqpNJ/hD4HHAe8NGqenjFkq2MsUzdLIN5R6+1zOYdrdby/j+pqnFnkCQtgXdiSlKjLHBJatQ5U+BJLk5yf5JHkjyc5JZu+7oke5M83j1fMO6s/ZKcl+QrSe7t1jcl2d99PcHd3RvEEyHJ2iS7kjya5GCSqyd5fJP8Sfez8FCSTyR53iSNb5KPJjmR5KG+bQPHMz1/3eV+MMkVE5L3L7ufhweTfCbJ2r59t3Z5H0vy+tXOu1Dmvn3vTFJJLuzWxz7GZ+ucKXDgJPDOqroEuAp4e3dr/3ZgX1VtBvZ165PkFuBg3/rtwPur6mXA08DWsaQa7IPAZ6vqFcCl9HJP5PgmuQj4Y2Cmql5J7432m5ms8b0TuG7etoXG8w3A5u6xDfjwKmXsdyen590LvLKqfg34D+BWgO5372bgV7vX/E339Rur7U5Oz0ySi4HXAf/Zt3kSxvjsVNU5+QB2A68FHgM2dNs2AI+NO1tfxo30fkmvAe4FQu+usDXd/quBz407Z5flRcA36N747ts+kePLs3cKr6P3aat7gddP2vgC08BDi40n8HfAWwcdN8688/b9NnBXt3wrcGvfvs8BV0/CGHfbdtG7CDkMXDhJY3w2j3PpCvwnkkwDlwP7gfVVdazbdRxYP6ZYg3wAeBfw4279xcAzVXWyWz9Cr4gmwSZgDvhYN+XzkSQvYELHt6qOAn9F7wrrGPBt4ACTO76nLDSeg766YtKy/wHwT93yxOZNcgNwtKq+Om/XxGZeyDlX4EleCHwKeEdVfad/X/X+Wp2Iz00meTNwoqoOjDvLkNYAVwAfrqrLge8zb7pkwsb3AnpfrrYJ+EXgBQz4X+lJNknjuZgk76E3jXnXuLOcSZLnA+8G/nzcWVbCOVXgSZ5Dr7zvqqpPd5ufSrKh278BODGufPO8Crg+yWF63+R4Db055rVJTt1gNUlfT3AEOFJV+7v1XfQKfVLH9zXAN6pqrqp+CHya3phP6viestB4TuxXVyT5PeDNwNu6v3RgcvP+Cr2/1L/a/e5tBL6c5BeY3MwLOmcKPEmAO4CDVfW+vl17gC3d8hZ6c+NjV1W3VtXGqpqm92bP56vqbcD9wFu6wyYp73Hgm0le3m26FniECR1felMnVyV5fvezcSrvRI5vn4XGcw/wu90nJa4Cvt031TI2Sa6jNw14fVX9T9+uPcDNSZ6bZBO9Nwa/OI6M/arqa1X1kqqa7n73jgBXdD/fEznGZzTuSfiVegCvpve/mw8CD3SPN9KbV94HPA78C7Bu3FkHZP9N4N5u+Zfp/aAfAv4BeO648/XlvAyY7cb4H4ELJnl8gfcCjwIPAX8PPHeSxhf4BL35+R/SK5KtC40nvTe4PwR8HfgavU/XTELeQ/TmjU/9zv1t3/Hv6fI+BrxhUsZ43v7DPPsm5tjH+Gwf3kovSY06Z6ZQJOmnjQUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGvV/DO/GXOENbf0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(high_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBfMKfpPXK17"
   },
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento. Noten que las clases están desbalanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.117633Z",
     "start_time": "2020-04-07T15:44:21.090703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "ZNibpIBYXK18",
    "outputId": "fdf38c5b-34d5-43fe-848a-2fc549353d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count(),\n",
    "          '\\n---------------------------------------\\n')\n",
    "for dataset_name in train:\n",
    "    get_group_dist(dataset_name, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0DXme6NXK2Q"
   },
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta carácteres relevantes ('!', '?', '#') en los tweets.\n",
    "2. Definios una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModelArchitecture:\n",
    "    \"\"\"\n",
    "    Basic architecture of transformer classes, contains fit method.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Returns `self` unless something different happens in train and test.\n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.128600Z",
     "start_time": "2020-04-07T15:44:21.119624Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CrNM4IewXK2R"
   },
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin, BasicModelArchitecture):\n",
    "    \"\"\"\n",
    "    Count relevant chars, emojis, repeated chars and dots, and do mark_negation.\n",
    "    \"\"\"\n",
    "    def __init(self):\n",
    "        super(CharsCountTransformer, self).__init__()\n",
    "    \n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count('#')\n",
    "        num_exclamations = tweet.count('!')\n",
    "        num_interrogations = tweet.count('?')\n",
    "        num_uppercases = sum(1 for c in tweet if c.isupper())  \n",
    "        num_repeated_chars = self.count_repeated_chars(tweet)\n",
    "        num_repeated_dots = self.count_repeated_dots(tweet)\n",
    "        len_tweet = len(tweet)\n",
    "        \n",
    "        return [num_hashtags, num_exclamations, num_interrogations,\n",
    "                num_uppercases, num_repeated_chars, num_repeated_dots, len_tweet]\n",
    "    \n",
    "    def get_emojis(self, tweet):\n",
    "        \"\"\"\n",
    "        Get number, sentiment_score, and polarity of emojis.\n",
    "        \"\"\"\n",
    "        emojis_list = [c for c in tweet if c in emoji.UNICODE_EMOJI]\n",
    "        positive, negative, neutral, sent_score, type_pos, type_neg = 0, 0, 0, 0, 0, 0\n",
    "        no_pesca = 0\n",
    "        \n",
    "        for emoj in emojis_list:\n",
    "            try:\n",
    "                em = get_emoji_sentiment_rank(emoj)\n",
    "#                 positive += em['positive']\n",
    "#                 negative += em['negative']\n",
    "#                 neutral += em['neutral']\n",
    "                sent_score += abs(em['sentiment_score'])\n",
    "                if em['sentiment_score']>0:\n",
    "                    type_pos += 1\n",
    "                else:\n",
    "                    type_neg += 1\n",
    "                    \n",
    "            except:  \n",
    "                emoji_name = emoji.UNICODE_EMOJI.get(emoj)[1:-1]\n",
    "                if emoji_name in positive_words:\n",
    "                    type_pos += 1\n",
    "                    continue\n",
    "                elif emoji_name in negative_words:\n",
    "                    type_neg += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # Evaluate by words\n",
    "                    for idx, w in enumerate(emoji_name.split('_')):\n",
    "                        if w in positive_words:\n",
    "                            type_pos += 1\n",
    "                            break\n",
    "                        elif w in negative_words:\n",
    "                            type_neg += 1\n",
    "                            break\n",
    "        return [len(emojis_list), sent_score, type_pos, type_neg]\n",
    "\n",
    "    def neg_transform(self, X):\n",
    "        \"\"\"\n",
    "        Append '_NEG' to all words after a negation.\n",
    "        \"\"\"\n",
    "        converted = []\n",
    "        counter = []\n",
    "        for tweet in X:\n",
    "            neg = mark_negation(tweet.split(' '))\n",
    "            count = sum(1 for word in neg if neg[-4:]=='_NEG')\n",
    "            sentence = ' '.join(word for word in neg)\n",
    "            converted.append(sentence)\n",
    "            counter.append(count)\n",
    "        return count, converted\n",
    "    \n",
    "    def count_repeated_chars(self, tweet):\n",
    "        \"\"\"\n",
    "        Count number of more than twice repeated chars ( >= three times).\n",
    "        \"\"\"\n",
    "        c_prev = tweet[0]\n",
    "        counter = 0\n",
    "        twice = False\n",
    "        for c in tweet[1:]:\n",
    "            if c==c_prev:\n",
    "                if twice:\n",
    "                    counter += 1\n",
    "                twice = True\n",
    "            else:\n",
    "                twice = False\n",
    "            c_prev = c\n",
    "        return counter\n",
    "    \n",
    "    def count_repeated_dots(self, tweet):\n",
    "        \"\"\"\n",
    "        Count number of contiguous dots.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        prev_is_dot = False\n",
    "        for c in tweet[1:]:\n",
    "            if c == '.':\n",
    "                if prev_is_dot:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    prev_is_dot = True\n",
    "            else:\n",
    "                prev_is_dot = False\n",
    "        return counter\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        num_negs, X_neg = self.neg_transform(X)\n",
    "        for tweet in X_neg:\n",
    "            features = np.hstack((self.get_relevant_chars(tweet),\n",
    "                                  self.get_emojis(tweet),\n",
    "                                  num_negs))\n",
    "            chars.append(features)\n",
    "        return np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiuFeatureExtractor(BaseEstimator, TransformerMixin, BasicModelArchitecture):\n",
    "    \"\"\"\n",
    "    Takes in a corpus of tweets and calculates features using Bing Liu's lexicon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        super(LiuFeatureExtractor, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pos_set = set(opinion_lexicon.positive())\n",
    "        self.neg_set = set(opinion_lexicon.negative())\n",
    "\n",
    "    def liu_score(self,sentence):\n",
    "        \"\"\"\n",
    "        Calculates the number of positive and negative words in the sentence using Bing Liu's Lexicon.\n",
    "        \"\"\" \n",
    "        tokenized_sent = self.tokenizer.tokenize(sentence)\n",
    "        pos_words = 0\n",
    "        neg_words = 0\n",
    "        for word in tokenized_sent:\n",
    "            if word in self.pos_set:\n",
    "                pos_words += 1\n",
    "            elif word in self.neg_set:\n",
    "                neg_words += 1\n",
    "        return [pos_words,neg_words]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Applies liu_score on a data.frame containing tweets.\n",
    "        \"\"\"\n",
    "        values = []\n",
    "        for tweet in X:\n",
    "            values.append(self.liu_score(tweet))\n",
    "\n",
    "        return(np.array(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaderFeatureExtractor(BaseEstimator, TransformerMixin, BasicModelArchitecture):\n",
    "    \"\"\"\n",
    "    Takes in a corpus of tweets and calculates features using the Vader method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VaderFeatureExtractor, self).__init__()\n",
    "        self.sid = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Calculates sentiment scores for a sentence using the Vader method.\n",
    "        \"\"\"\n",
    "        values = []\n",
    "        for tweet in X:\n",
    "            values.append(list(self.sid.polarity_scores(tweet).values()))\n",
    "\n",
    "        return(np.array(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4404}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_analyser = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "nl_analyser.polarity_scores(':(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /home/martin/anaconda3/envs/nlp/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/martin/anaconda3/envs/nlp/lib/python3.8/site-packages (from vaderSentiment) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/martin/anaconda3/envs/nlp/lib/python3.8/site-packages (from requests->vaderSentiment) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/martin/anaconda3/envs/nlp/lib/python3.8/site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/martin/anaconda3/envs/nlp/lib/python3.8/site-packages (from requests->vaderSentiment) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/martin/anaconda3/envs/nlp/lib/python3.8/site-packages (from requests->vaderSentiment) (2020.4.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound': -0.4585}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores('i am not happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.    , 0.083 , 0.917 , 0.9022],\n",
       "       [0.    , 0.    , 1.    , 0.8126]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaderFeatureExtractor().transform(['i am happy happy happy', 'happy happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBlobTransformer(BaseEstimator, TransformerMixin, BasicModelArchitecture):\n",
    "    \"\"\"\n",
    "    Sentiment Analysis using TextBlob.\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        super(TextBlobTransformer, self).__init__()\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        sent_pol = [TextBlob(doc).sentiment.polarity for doc in X]\n",
    "        return np.array([sent_pol]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin, BasicModelArchitecture):\n",
    "    \"\"\" Transforma tweets a representaciones vectoriales usando algún modelo de Word Embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, aggregation_func, tokenizer, do_stem=False, do_lemmatize=False,\n",
    "                 remove_stopwords=False):\n",
    "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
    "        self.model = model.wv \n",
    "        self.tokenizer = tokenizer\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.do_stem = do_stem\n",
    "        self.do_lemmatize = do_lemmatize\n",
    "        \n",
    "        # indicamos la función de agregación (np.min, np.max, np.mean, np.sum, ...)\n",
    "        self.aggregation_func = aggregation_func\n",
    "\n",
    "    def simple_tokenizer(self, doc, remove_stopwords=False, lower=False):\n",
    "        \"\"\"Tokenizador. Elimina signos de puntuación, lleva las letras a minúscula(opcional) y \n",
    "           separa el tweet por espacios.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(doc)\n",
    "        final_tokens = []\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                if t[0]=='#':  # Convert hashtags\n",
    "                    t = t[1:]\n",
    "            except:\n",
    "                print('Except', doc)\n",
    "            if remove_stopwords:\n",
    "                if t.lower() in stopwords:\n",
    "                    continue\n",
    "            final_tokens.append(t)\n",
    "        \n",
    "        return final_tokens\n",
    "    \n",
    "    def stem(self, doc):  \n",
    "        doc = stemmer.stem(doc)\n",
    "        return doc\n",
    "    \n",
    "    def lemmatize(self, doc):\n",
    "        \n",
    "        doc = lemmatizer.lemmatize(doc)\n",
    "        return doc\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        doc_embeddings = []\n",
    "        no_embeddings = False\n",
    "        for doc in X:\n",
    "            \n",
    "            tokens = self.simple_tokenizer(doc, self.remove_stopwords) \n",
    "    \n",
    "            selected_wv = []\n",
    "            for token in tokens:\n",
    "                token = token.lower()\n",
    "                \n",
    "                # Stem or lemmatize\n",
    "                if self.do_stem:\n",
    "                    token = self.stem(token)\n",
    "                elif self.do_lemmatize:\n",
    "                    token = self.lemmatize(token)\n",
    "                \n",
    "                if token in self.model.vocab:\n",
    "                    selected_wv.append(self.model[token])\n",
    "                    \n",
    "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo añadimos.\n",
    "            if len(selected_wv) > 0:\n",
    "#                 doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
    "                doc_embedding += np.array(selected_wv)\n",
    "                doc_embeddings.append(doc_embedding)\n",
    "            # si no, añadimos un vector de ceros que represente a ese documento.\n",
    "            else: \n",
    "                if not no_embeddings:\n",
    "                    print('Couldnt find any embedding for tweet:')\n",
    "                    no_embeddings = True\n",
    "                print(doc)\n",
    "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
    "\n",
    "        return np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4mLu9iYXK2Z"
   },
   "source": [
    "### Definir la representación y el clasificador\n",
    "\n",
    "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n",
    "\n",
    "El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
    "\n",
    "```python\n",
    "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                        ('chars_count',CharsCountTransformer())])),\n",
    "              ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6zo8kmUXK2a"
   },
   "source": [
    "Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-5b3e502dfb57>:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  self.model = model.wv\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(tokenizer=TweetTokenizer().tokenize,\n",
    "                        ngram_range=(1, 4))\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=TweetTokenizer().tokenize)\n",
    "\n",
    "chars_count = CharsCountTransformer()\n",
    "\n",
    "# doc2vec_mean = Doc2VecTransformer(glove_twitter, np.mean)\n",
    "# doc2vec_sum = Doc2VecTransformer(glove_twitter, np.sum, TweetTokenizer())\n",
    "doc2vec_max_remove_stop = Doc2VecTransformer(glove_twitter, np.max, TweetTokenizer(), remove_stopwords=True)\n",
    "doc2vec_max = Doc2VecTransformer(glove_twitter, np.max, TweetTokenizer())\n",
    "doc2vec_min = Doc2VecTransformer(glove_twitter, np.min, TweetTokenizer())\n",
    "\n",
    "doc2vec_max_stem = Doc2VecTransformer(glove_twitter, np.max, TweetTokenizer(), do_stem=True)\n",
    "doc2vec_max_lem = Doc2VecTransformer(glove_twitter, np.max, TweetTokenizer(), do_lemmatize=True)\n",
    "\n",
    "liu = LiuFeatureExtractor(tokenizer=TweetTokenizer())\n",
    "vader = VaderFeatureExtractor()\n",
    "textblob = TextBlobTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "F_qyFk1JXK2b"
   },
   "outputs": [],
   "source": [
    "def get_experiment_0_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer + NB\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count)\n",
    "                                    ])), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "F_qyFk1JXK2b"
   },
   "outputs": [],
   "source": [
    "def get_experiment_01_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer + NB\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count)\n",
    "                                    ])), ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_1_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + TweetTokenizer + CharsCountTransformer + NB\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count)\n",
    "                                    ])), ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_2_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + TweetTokenizer + NB\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count)])),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(30)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_21_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + TweetTokenizer + NB\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_4_pipeline():\n",
    "    \"\"\"\n",
    "    TfidfVectorizer + TweetTokenizer + CharsCountTransformer + NB\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', tfidf),\n",
    "                                    ('chars_count', chars_count)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_5_pipeline():\n",
    "    \"\"\"\n",
    "    TfidfVectorizer + TweetTokenizer + Doc2Vec (removing stopwords) + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', tfidf),\n",
    "                                    ('doc2vec', doc2vec_max_remove_stop)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_6_pipeline():\n",
    "    \"\"\"\n",
    "    TfidfVectorizer + TweetTokenizer + Doc2Vec (without removing stopwords) + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', tfidf),\n",
    "                                    ('doc2vec', doc2vec_max)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_61_pipeline():\n",
    "    \"\"\"\n",
    "    TfidfVectorizer + TweetTokenizer + Doc2Vec (without removing stopwords) + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', tfidf),\n",
    "                                    ('doc2vecmax', doc2vec_max),\n",
    "                                    ('doc2vecmin', doc2vec_min)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_62_pipeline():\n",
    "    \"\"\"\n",
    "    TfidfVectorizer + TweetTokenizer + Doc2Vec (without removing stopwords) + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', tfidf),\n",
    "                                    ('doc2vecmax_stem', doc2vec_max_stem)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_63_pipeline():\n",
    "    \"\"\"\n",
    "    TfidfVectorizer + TweetTokenizer + Doc2Vec (without removing stopwords) + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', tfidf),\n",
    "                                    ('doc2vecmax_lem', doc2vec_max_lem)])),\n",
    "                     ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_7_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader)\n",
    "                                    ])), ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_71_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader)\n",
    "                                    ])), ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_8_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader),\n",
    "                                    ('textblob', textblob)\n",
    "                                    ])), ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_9_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count),\n",
    "                                    ('doc2vecmax', doc2vec_max),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader),\n",
    "                                    ('doc2vec', textblob)\n",
    "                                    ])), ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_10_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader)\n",
    "                                    ])), ('clf', LogisticRegression(solver='liblinear',multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_101_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + SVM\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('chars_count', chars_count),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader)\n",
    "                                    ])), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_102_pipeline():\n",
    "    \"\"\"\n",
    "    CountVectorizer + CharsCountTransformer +  LiuFeatureExtractor + VaderFeatureExtractor + doc2vec +\n",
    "    featureSelection + RandomForestClassifier\n",
    "    \"\"\"\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', count),\n",
    "                                    ('liu', liu),\n",
    "                                    ('vader', vader),\n",
    "                                    ('doc2vec', textblob),\n",
    "                                    ])),\n",
    "                     ('feature_selection',\n",
    "                      SelectFromModel(LogisticRegression(solver='liblinear',multi_class='ovr'),\n",
    "                                      threshold='median')),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOQekJanXK2h"
   },
   "source": [
    "### Ejecutar el pipeline para algún dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(X, y):\n",
    "    \"\"\"\n",
    "    Realiza oversampling aleatorio de las clases con menos datos para igualar a la mayor\n",
    "    \"\"\"\n",
    "    X_over = []\n",
    "    y_over = []\n",
    "    \n",
    "    n_max = np.max([np.sum([y == c]) for c in np.unique(y)])\n",
    "    \n",
    "    for c in np.unique(y):\n",
    "        n = n_max - np.sum([y == c])\n",
    "        if n == 0: continue\n",
    "        indices = np.where(y == c)[0]\n",
    "        arr_sampling = np.random.randint(low=0,\n",
    "                                         high=len(indices),\n",
    "                                         size=int(n / 2))\n",
    "        indices = np.array(indices[arr_sampling])\n",
    "        \n",
    "        X_over = np.concatenate([X_over, [X.iloc[i] for i in indices]])\n",
    "        y_over = np.concatenate([y_over, [y.iloc[i] for i in indices]])\n",
    "    \n",
    "    X_over = np.concatenate([X, X_over])\n",
    "    y_over = np.concatenate([y, y_over])\n",
    "        \n",
    "    return X_over, y_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.167498Z",
     "start_time": "2020-04-07T15:44:21.157540Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V3gMwCchXK2i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name, pipeline):\n",
    "    \"\"\"\n",
    "    Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    dataset = dataset.sample(frac=1).reset_index()\n",
    "    print(len(dataset))\n",
    "    \n",
    "    for n in range(5):\n",
    "        \n",
    "#         # Dividimos el dataset en train y test.\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             dataset.tweet,\n",
    "#             dataset.sentiment_intensity,\n",
    "#             shuffle=True,\n",
    "#             test_size=0.33)\n",
    "\n",
    "        i = int(n * len(dataset) / 5)\n",
    "        j = int((n + 1) * len(dataset) / 5)\n",
    "        X_test = dataset.tweet.iloc[i:j]\n",
    "        y_test = dataset.sentiment_intensity.iloc[i:j]\n",
    "        print(len(y_test))\n",
    "        \n",
    "        X_train = pd.concat([dataset.tweet.iloc[:i], dataset.tweet.iloc[j:]])\n",
    "        y_train = pd.concat([dataset.sentiment_intensity.iloc[:i], dataset.sentiment_intensity.iloc[j:]])\n",
    "        \n",
    "        X_train, y_train = over_sampling(X_train, y_train)\n",
    "#         X_test, y_test = over_sampling(X_test, y_test)\n",
    "\n",
    "        # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "        predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "        # Obtenemos el orden de las clases aprendidas.\n",
    "        learned_labels = pipeline.classes_\n",
    "\n",
    "        # Evaluamos:\n",
    "        scores.append(evaluate(predicted_probabilities, y_test, learned_labels, dataset_name))\n",
    "    \n",
    "    return pipeline, learned_labels, np.array(scores).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2(dataset, dataset_name, pipeline):\n",
    "    \"\"\"\n",
    "    Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    dataset = dataset.sample(frac=1).reset_index()\n",
    "    print(len(dataset))\n",
    "    \n",
    "    for n in range(5):\n",
    "        \n",
    "#         # Dividimos el dataset en train y test.\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             dataset.tweet,\n",
    "#             dataset.sentiment_intensity,\n",
    "#             shuffle=True,\n",
    "#             test_size=0.33)\n",
    "\n",
    "        i = int(n * len(dataset) / 5)\n",
    "        j = int((n + 1) * len(dataset) / 5)\n",
    "        X_test = dataset.tweet.iloc[i:j]\n",
    "        y_test = dataset.sentiment_intensity.iloc[i:j]\n",
    "        print(len(y_test))\n",
    "        \n",
    "        X_train = pd.concat([dataset.tweet.iloc[:i], dataset.tweet.iloc[j:]])\n",
    "        y_train = pd.concat([dataset.sentiment_intensity.iloc[:i], dataset.sentiment_intensity.iloc[j:]])\n",
    "        \n",
    "        X_train, y_train = over_sampling(X_train, y_train)\n",
    "#         X_test, y_test = over_sampling(X_test, y_test)\n",
    "\n",
    "        # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "        predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "        # Obtenemos el orden de las clases aprendidas.\n",
    "        learned_labels = pipeline.classes_\n",
    "\n",
    "        # Evaluamos:\n",
    "        scores.append(evaluate(predicted_probabilities, y_test, learned_labels, dataset_name))\n",
    "    \n",
    "    return pipeline, learned_labels, np.array(scores).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    4\n",
       "2    3\n",
       "3    4\n",
       "1    2\n",
       "0    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2,3,4,4]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFZBa7emXK2l"
   },
   "source": [
    "### Ejecutar el sistema creado por cada train set\n",
    "\n",
    "Este código crea y entrena los 4 sistemas de clasificación y luego los evalua. Para los experimentos, pueden copiar este código variando el pipeline cuantas veces estimen conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(experiments):\n",
    "    best_scores = {\n",
    "        'anger': 0,\n",
    "        'fear': 0,\n",
    "        'joy': 0,\n",
    "        'sadness': 0\n",
    "    }\n",
    "    best_classifiers = {\n",
    "        'anger': [],\n",
    "        'fear': [],\n",
    "        'joy': [],\n",
    "        'sadness': []\n",
    "    }\n",
    "\n",
    "    for n, pipeline in enumerate(experiments):\n",
    "        classifiers = []\n",
    "        learned_labels_array = []\n",
    "        scores_array = []\n",
    "\n",
    "        # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "        for dataset_name, dataset in train.items():\n",
    "            print(dataset_name)\n",
    "            # ejecutamos el pipeline sobre el dataset\n",
    "            classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
    "\n",
    "            # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "            classifiers.append(classifier)\n",
    "\n",
    "            # guardamos las labels aprendidas por el clasificador\n",
    "            learned_labels_array.append(learned_labels)\n",
    "\n",
    "            # guardamos los scores obtenidos\n",
    "            scores_array.append(scores)\n",
    "            print(np.sum(scores))\n",
    "            \n",
    "            if np.sum(scores) > best_scores[dataset_name]:\n",
    "                best_scores[dataset_name] = np.sum(scores)\n",
    "                best_classifiers[dataset_name] = (classifier, n)\n",
    "\n",
    "        # print avg scores\n",
    "        print('Experiment', n,\n",
    "          \"Average scores:\\n\\n\",\n",
    "          \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "          .format(*np.array(scores_array).mean(axis=0)))\n",
    "    return best_classifiers, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.384119Z",
     "start_time": "2020-04-07T15:44:21.170488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GOIXPM_RXK2l",
    "outputId": "b3daaa37-d298-4c4a-d226-de08654bfb33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n",
      "941\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  1  24   1]\n",
      " [  4 120   5]\n",
      " [  0  26   7]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.607\tKappa: 0.123\tAccuracy: 0.681\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  9  19   1]\n",
      " [  8 111   9]\n",
      " [  0  21  10]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.673\tKappa: 0.268\tAccuracy: 0.691\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  6  31   0]\n",
      " [  9 105   5]\n",
      " [  0  26   6]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.625\tKappa: 0.119\tAccuracy: 0.622\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  29   0]\n",
      " [  3 108   7]\n",
      " [  0  24  10]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.674\tKappa: 0.231\tAccuracy: 0.665\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  5  28   0]\n",
      " [  5 111   7]\n",
      " [  0  25   8]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.653\tKappa: 0.166\tAccuracy: 0.656\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4908000000000001\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 18  38   2]\n",
      " [ 18 110  14]\n",
      " [  1  30  20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.712\tKappa: 0.234\tAccuracy: 0.59\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 21  33   3]\n",
      " [ 16 113  12]\n",
      " [  4  30  19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.697\tKappa: 0.278\tAccuracy: 0.61\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[  9  50   0]\n",
      " [ 14 108  14]\n",
      " [  3  35  19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.641\tKappa: 0.136\tAccuracy: 0.54\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 18  49   0]\n",
      " [ 15 105  16]\n",
      " [  2  30  16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.658\tKappa: 0.176\tAccuracy: 0.554\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[  9  33   5]\n",
      " [ 18 109  17]\n",
      " [  0  44  17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.613\tKappa: 0.108\tAccuracy: 0.536\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4165999999999999\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[18 28  1]\n",
      " [14 73 11]\n",
      " [ 0 16 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.687\tKappa: 0.313\tAccuracy: 0.611\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[19 29  2]\n",
      " [11 73  6]\n",
      " [ 1 20 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.722\tKappa: 0.339\tAccuracy: 0.617\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[14 22  0]\n",
      " [14 81 10]\n",
      " [ 0 20 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.73\tKappa: 0.323\tAccuracy: 0.635\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[14 33  0]\n",
      " [11 78  5]\n",
      " [ 1 22 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.734\tKappa: 0.274\tAccuracy: 0.6\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[13 26  0]\n",
      " [12 81  8]\n",
      " [ 1 23 17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.729\tKappa: 0.28\tAccuracy: 0.613\n",
      "------------------------------------------------------\n",
      "\n",
      "1.6414\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 7 35  2]\n",
      " [ 5 74 11]\n",
      " [ 4 22 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.641\tKappa: 0.158\tAccuracy: 0.541\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 6 29  2]\n",
      " [ 7 76  9]\n",
      " [ 1 30 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.636\tKappa: 0.143\tAccuracy: 0.547\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 4 28  1]\n",
      " [10 78  9]\n",
      " [ 1 28 13]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.635\tKappa: 0.124\tAccuracy: 0.552\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 7 34  5]\n",
      " [ 9 69  7]\n",
      " [ 1 22 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.699\tKappa: 0.208\tAccuracy: 0.547\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 8 42  0]\n",
      " [ 5 74 10]\n",
      " [ 2 19 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.633\tKappa: 0.166\tAccuracy: 0.547\n",
      "------------------------------------------------------\n",
      "\n",
      "1.3554\n",
      "Experiment 0 Average scores:\n",
      "\n",
      " Average AUC: 0.67\t Average Kappa: 0.208\t Average Accuracy: 0.598\n",
      "anger\n",
      "941\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 4 29  0]\n",
      " [ 8 96  8]\n",
      " [ 1 34  8]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.659\tKappa: 0.084\tAccuracy: 0.574\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  3  29   0]\n",
      " [  3 116  10]\n",
      " [  0  20   7]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.627\tKappa: 0.131\tAccuracy: 0.67\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  26   0]\n",
      " [ 10 105   8]\n",
      " [  1  25   6]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.628\tKappa: 0.129\tAccuracy: 0.628\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  5  30   1]\n",
      " [  6 108   9]\n",
      " [  0  26   3]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.575\tKappa: 0.06\tAccuracy: 0.617\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  8  19   0]\n",
      " [  5 117   8]\n",
      " [  1  23   8]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.668\tKappa: 0.253\tAccuracy: 0.704\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4013999999999998\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 12  44   3]\n",
      " [ 16 105  13]\n",
      " [  3  40  15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.633\tKappa: 0.123\tAccuracy: 0.526\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 15  36   4]\n",
      " [ 18 111  13]\n",
      " [  5  37  12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.675\tKappa: 0.146\tAccuracy: 0.55\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 26  36   2]\n",
      " [ 11 121   7]\n",
      " [  2  39   8]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.692\tKappa: 0.258\tAccuracy: 0.615\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 18  40   2]\n",
      " [ 14 112  12]\n",
      " [  3  29  21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.683\tKappa: 0.263\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 19  30   1]\n",
      " [ 22 101  23]\n",
      " [  3  34  19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.673\tKappa: 0.182\tAccuracy: 0.552\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4346\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[18 30  0]\n",
      " [ 8 77 12]\n",
      " [ 0 20 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.725\tKappa: 0.296\tAccuracy: 0.611\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[14 18  0]\n",
      " [16 85  6]\n",
      " [ 2 18 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.729\tKappa: 0.377\tAccuracy: 0.667\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[20 31  1]\n",
      " [14 74  9]\n",
      " [ 0 17 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.727\tKappa: 0.29\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[12 30  2]\n",
      " [17 66 10]\n",
      " [ 3 18 22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.72\tKappa: 0.239\tAccuracy: 0.556\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[14 29  0]\n",
      " [10 80  4]\n",
      " [ 1 27 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.743\tKappa: 0.287\tAccuracy: 0.608\n",
      "------------------------------------------------------\n",
      "\n",
      "1.6353999999999997\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 4 31  2]\n",
      " [ 3 81 11]\n",
      " [ 2 22 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.655\tKappa: 0.205\tAccuracy: 0.587\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 3 33  1]\n",
      " [12 79  7]\n",
      " [ 1 20 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.627\tKappa: 0.161\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 8 36  3]\n",
      " [ 6 70  8]\n",
      " [ 1 27 13]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.648\tKappa: 0.167\tAccuracy: 0.529\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 8 34  4]\n",
      " [ 9 63 13]\n",
      " [ 1 26 14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.683\tKappa: 0.121\tAccuracy: 0.494\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[11 30  2]\n",
      " [11 70 10]\n",
      " [ 3 22 13]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.615\tKappa: 0.185\tAccuracy: 0.547\n",
      "------------------------------------------------------\n",
      "\n",
      "1.3588\n",
      "Experiment 1 Average scores:\n",
      "\n",
      " Average AUC: 0.669\t Average Kappa: 0.198\t Average Accuracy: 0.59\n",
      "anger\n",
      "941\n",
      "188\n",
      "Couldnt find any embedding for tweet:\n",
      "I think they may be \n",
      "I think they may be \n",
      "I think they may be \n",
      "Couldnt find any embedding for tweet:\n",
      "I can't even right now #bb18 \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[13 22  4]\n",
      " [17 90 14]\n",
      " [ 2 15 11]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.666\tKappa: 0.224\tAccuracy: 0.606\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Couldnt find any embedding for tweet:\n",
      "I can't even right now #bb18 \n",
      "I think they may be \n",
      "I think they may be \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[11 17  1]\n",
      " [17 88 15]\n",
      " [ 2 25 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.663\tKappa: 0.186\tAccuracy: 0.59\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Couldnt find any embedding for tweet:\n",
      "I can't even right now #bb18 \n",
      "I think they may be \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think they may be \n",
      "I think they may be \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[14 19  1]\n",
      " [23 84 17]\n",
      " [ 0 11 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.677\tKappa: 0.292\tAccuracy: 0.622\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Couldnt find any embedding for tweet:\n",
      "I can't even right now #bb18 \n",
      "Couldnt find any embedding for tweet:\n",
      "I think they may be \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  9  17   0]\n",
      " [  7 111  10]\n",
      " [  1  20  13]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.661\tKappa: 0.319\tAccuracy: 0.707\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Couldnt find any embedding for tweet:\n",
      "I can't even right now #bb18 \n",
      "I think they may be \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[13 18  2]\n",
      " [17 85 22]\n",
      " [ 5 18  9]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.654\tKappa: 0.163\tAccuracy: 0.566\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5192\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "And here we go again 😓 \n",
      "That goes the \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 25  21   4]\n",
      " [ 21 100  28]\n",
      " [  6  19  27]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.717\tKappa: 0.319\tAccuracy: 0.606\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "And here we go again 😓 \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 29  24   6]\n",
      " [ 11 103  24]\n",
      " [  3  29  22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.72\tKappa: 0.326\tAccuracy: 0.614\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "And here we go again 😓 \n",
      "That goes the \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[27 26  7]\n",
      " [15 89 23]\n",
      " [ 2 36 27]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.67\tKappa: 0.277\tAccuracy: 0.567\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Couldnt find any embedding for tweet:\n",
      "And here we go again 😓 \n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[17 37  5]\n",
      " [19 94 23]\n",
      " [ 6 20 30]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.688\tKappa: 0.248\tAccuracy: 0.562\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "That goes the \n",
      "Couldnt find any embedding for tweet:\n",
      "And here we go again 😓 \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 21  36   3]\n",
      " [ 25 101  23]\n",
      " [  5  19  19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.65\tKappa: 0.207\tAccuracy: 0.56\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5462\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[17 18  1]\n",
      " [15 63 25]\n",
      " [ 1 15 25]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.722\tKappa: 0.298\tAccuracy: 0.583\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[25 18  0]\n",
      " [22 63 14]\n",
      " [ 3 12 23]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.758\tKappa: 0.368\tAccuracy: 0.617\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[30 16  4]\n",
      " [28 49 11]\n",
      " [ 2 16 25]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.739\tKappa: 0.334\tAccuracy: 0.575\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[19 15  6]\n",
      " [11 72 18]\n",
      " [ 0 16 23]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.741\tKappa: 0.374\tAccuracy: 0.633\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[29 17  4]\n",
      " [30 55 12]\n",
      " [ 3 12 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.7\tKappa: 0.305\tAccuracy: 0.569\n",
      "------------------------------------------------------\n",
      "\n",
      "1.6632\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[12 25  2]\n",
      " [19 64  9]\n",
      " [ 1 21 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.636\tKappa: 0.22\tAccuracy: 0.552\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[10 27  3]\n",
      " [17 61 21]\n",
      " [ 1 13 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.626\tKappa: 0.173\tAccuracy: 0.523\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[12 35  3]\n",
      " [12 62 16]\n",
      " [ 4 16 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.658\tKappa: 0.131\tAccuracy: 0.5\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "@chelseafc let them know it's the \n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[14 23  4]\n",
      " [20 52 16]\n",
      " [ 5 17 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.635\tKappa: 0.193\tAccuracy: 0.506\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 21  4]\n",
      " [15 60  9]\n",
      " [ 8 21 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.683\tKappa: 0.253\tAccuracy: 0.547\n",
      "------------------------------------------------------\n",
      "\n",
      "1.3672\n",
      "Experiment 2 Average scores:\n",
      "\n",
      " Average AUC: 0.683\t Average Kappa: 0.261\t Average Accuracy: 0.58\n",
      "anger\n",
      "941\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 8 24  6]\n",
      " [22 92  9]\n",
      " [ 1 17  9]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.611\tKappa: 0.134\tAccuracy: 0.58\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[12 24  0]\n",
      " [16 88 18]\n",
      " [ 1 15 14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.646\tKappa: 0.22\tAccuracy: 0.606\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 9 19  2]\n",
      " [10 99  8]\n",
      " [ 3 23 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.682\tKappa: 0.288\tAccuracy: 0.654\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[10 19  1]\n",
      " [22 89 19]\n",
      " [ 0 16 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.642\tKappa: 0.167\tAccuracy: 0.59\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 7 18  2]\n",
      " [11 90 24]\n",
      " [ 0 22 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.646\tKappa: 0.167\tAccuracy: 0.593\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4451999999999998\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[22 27  5]\n",
      " [20 95 20]\n",
      " [ 7 26 29]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.691\tKappa: 0.288\tAccuracy: 0.582\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 26  30   6]\n",
      " [ 26 108  14]\n",
      " [  3  24  14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.685\tKappa: 0.245\tAccuracy: 0.59\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 19  32   4]\n",
      " [ 19 105  16]\n",
      " [  4  29  24]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.673\tKappa: 0.261\tAccuracy: 0.587\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[25 28  5]\n",
      " [23 96 19]\n",
      " [ 6 30 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.65\tKappa: 0.231\tAccuracy: 0.558\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[21 33  5]\n",
      " [18 98 22]\n",
      " [ 2 28 25]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.669\tKappa: 0.25\tAccuracy: 0.571\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5062\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[14 17  6]\n",
      " [16 65 12]\n",
      " [ 3 27 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.699\tKappa: 0.238\tAccuracy: 0.55\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[22 18  1]\n",
      " [27 52 29]\n",
      " [ 2  7 22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.678\tKappa: 0.258\tAccuracy: 0.533\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[23 14  2]\n",
      " [18 71 11]\n",
      " [ 2 19 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.727\tKappa: 0.38\tAccuracy: 0.635\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[23 24  4]\n",
      " [11 65 17]\n",
      " [ 2 13 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.739\tKappa: 0.347\tAccuracy: 0.606\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[31 18  2]\n",
      " [24 56 14]\n",
      " [ 4  9 23]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.743\tKappa: 0.375\tAccuracy: 0.608\n",
      "------------------------------------------------------\n",
      "\n",
      "1.6232\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 17  2]\n",
      " [23 63 15]\n",
      " [ 6 13 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.657\tKappa: 0.249\tAccuracy: 0.558\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[20 22  4]\n",
      " [11 57 14]\n",
      " [ 6 17 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.705\tKappa: 0.304\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 25  4]\n",
      " [13 52 23]\n",
      " [ 4 20 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.65\tKappa: 0.146\tAccuracy: 0.483\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 24  3]\n",
      " [12 63 10]\n",
      " [ 1 29 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.679\tKappa: 0.214\tAccuracy: 0.541\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 25  4]\n",
      " [18 63 16]\n",
      " [ 4 15 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.642\tKappa: 0.169\tAccuracy: 0.523\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4180000000000001\n",
      "Experiment 3 Average scores:\n",
      "\n",
      " Average AUC: 0.676\t Average Kappa: 0.247\t Average Accuracy: 0.576\n",
      "anger\n",
      "941\n",
      "188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[14 14  0]\n",
      " [15 96 21]\n",
      " [ 0 15 13]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.669\tKappa: 0.285\tAccuracy: 0.654\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[11 17  1]\n",
      " [18 81 17]\n",
      " [ 1 24 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.656\tKappa: 0.219\tAccuracy: 0.585\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 9 28  1]\n",
      " [19 87  9]\n",
      " [ 4 20 11]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.642\tKappa: 0.148\tAccuracy: 0.569\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[11 23  2]\n",
      " [18 96 12]\n",
      " [ 0 12 14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.699\tKappa: 0.262\tAccuracy: 0.644\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[10 18  2]\n",
      " [13 95 20]\n",
      " [ 2 20  9]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.591\tKappa: 0.165\tAccuracy: 0.603\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4782\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[21 42  1]\n",
      " [19 96 22]\n",
      " [ 1 27 22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.67\tKappa: 0.209\tAccuracy: 0.554\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 28  33   1]\n",
      " [ 17 101  15]\n",
      " [  7  27  22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.728\tKappa: 0.307\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 24  26   8]\n",
      " [ 18 111  19]\n",
      " [  3  18  25]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.697\tKappa: 0.348\tAccuracy: 0.635\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 25  24   7]\n",
      " [ 19 100  20]\n",
      " [  4  33  19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.679\tKappa: 0.252\tAccuracy: 0.574\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 20  24   4]\n",
      " [ 22 101  19]\n",
      " [  9  28  25]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.685\tKappa: 0.265\tAccuracy: 0.579\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5568\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[18 26  4]\n",
      " [23 63 14]\n",
      " [ 2 12 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.669\tKappa: 0.236\tAccuracy: 0.55\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[15 15  0]\n",
      " [25 59 13]\n",
      " [ 8 21 24]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.698\tKappa: 0.254\tAccuracy: 0.544\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[17 29  2]\n",
      " [16 67 13]\n",
      " [ 0 11 26]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.73\tKappa: 0.337\tAccuracy: 0.608\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[23 21  7]\n",
      " [15 58 18]\n",
      " [ 3 12 23]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.727\tKappa: 0.323\tAccuracy: 0.578\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[19 19  4]\n",
      " [18 74 12]\n",
      " [ 4 12 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.723\tKappa: 0.339\tAccuracy: 0.619\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5869999999999997\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[16 23  1]\n",
      " [13 67 17]\n",
      " [ 1 19 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.662\tKappa: 0.236\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[12 33  3]\n",
      " [13 64  7]\n",
      " [ 2 22 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.688\tKappa: 0.202\tAccuracy: 0.535\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[13 20  7]\n",
      " [16 60 17]\n",
      " [ 7 12 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.666\tKappa: 0.24\tAccuracy: 0.541\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[13 22  5]\n",
      " [16 53 19]\n",
      " [ 2 23 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.64\tKappa: 0.161\tAccuracy: 0.494\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[13 24  5]\n",
      " [13 62 16]\n",
      " [ 6 15 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.664\tKappa: 0.225\tAccuracy: 0.541\n",
      "------------------------------------------------------\n",
      "\n",
      "1.4129999999999998\n",
      "Experiment 4 Average scores:\n",
      "\n",
      " Average AUC: 0.679\t Average Kappa: 0.251\t Average Accuracy: 0.579\n",
      "anger\n",
      "941\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  3  29   0]\n",
      " [  8 113   6]\n",
      " [  0  17  12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.681\tKappa: 0.21\tAccuracy: 0.681\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 6 30  0]\n",
      " [10 99  7]\n",
      " [ 0 19 17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.709\tKappa: 0.259\tAccuracy: 0.649\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 10  19   1]\n",
      " [  7 111   8]\n",
      " [  1  24   7]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.67\tKappa: 0.242\tAccuracy: 0.681\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  25   0]\n",
      " [ 10 109  13]\n",
      " [  0  19   5]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.644\tKappa: 0.111\tAccuracy: 0.644\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  8  23   0]\n",
      " [  7 103   6]\n",
      " [  0  32  10]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.748\tKappa: 0.209\tAccuracy: 0.64\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5556\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 30  35   3]\n",
      " [ 18 106  17]\n",
      " [  2  25  15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.699\tKappa: 0.277\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 23  34   2]\n",
      " [ 19 105  10]\n",
      " [  3  37  18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.704\tKappa: 0.247\tAccuracy: 0.582\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 17  35   1]\n",
      " [ 14 111  17]\n",
      " [  3  37  17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.657\tKappa: 0.2\tAccuracy: 0.575\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 27  22   1]\n",
      " [ 17 109  12]\n",
      " [  3  34  26]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.734\tKappa: 0.369\tAccuracy: 0.645\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 23  31   4]\n",
      " [ 22 109  13]\n",
      " [  5  27  18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.683\tKappa: 0.262\tAccuracy: 0.595\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5661999999999998\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[19 23  0]\n",
      " [13 77 12]\n",
      " [ 1 11 24]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.778\tKappa: 0.413\tAccuracy: 0.667\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[18 21  1]\n",
      " [19 65 10]\n",
      " [ 3 22 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.678\tKappa: 0.287\tAccuracy: 0.578\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[23 23  3]\n",
      " [23 66  9]\n",
      " [ 0 22 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.705\tKappa: 0.23\tAccuracy: 0.558\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[13 26  0]\n",
      " [11 79 10]\n",
      " [ 1 19 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.752\tKappa: 0.321\tAccuracy: 0.628\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[22 27  0]\n",
      " [ 6 79  9]\n",
      " [ 0 20 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.76\tKappa: 0.394\tAccuracy: 0.657\n",
      "------------------------------------------------------\n",
      "\n",
      "1.6812\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 21  2]\n",
      " [10 70  5]\n",
      " [ 5 32 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.698\tKappa: 0.244\tAccuracy: 0.564\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[11 21  1]\n",
      " [10 76 12]\n",
      " [ 3 21 17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.672\tKappa: 0.269\tAccuracy: 0.605\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[20 25  2]\n",
      " [12 71  8]\n",
      " [ 2 18 14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.722\tKappa: 0.314\tAccuracy: 0.61\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[12 30  2]\n",
      " [ 6 76 10]\n",
      " [ 2 19 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.694\tKappa: 0.269\tAccuracy: 0.599\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[11 35  2]\n",
      " [ 9 72  6]\n",
      " [ 5 20 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.678\tKappa: 0.202\tAccuracy: 0.552\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5384\n",
      "Experiment 5 Average scores:\n",
      "\n",
      " Average AUC: 0.703\t Average Kappa: 0.266\t Average Accuracy: 0.616\n",
      "anger\n",
      "941\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  8  21   0]\n",
      " [  8 107  10]\n",
      " [  0  23  11]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.639\tKappa: 0.238\tAccuracy: 0.67\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  9  21   2]\n",
      " [  8 109   6]\n",
      " [  1  22  10]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.667\tKappa: 0.271\tAccuracy: 0.681\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[13 22  0]\n",
      " [10 97 13]\n",
      " [ 1 22 10]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.721\tKappa: 0.24\tAccuracy: 0.638\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 10  21   0]\n",
      " [ 14 103   5]\n",
      " [  1  23  11]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.704\tKappa: 0.252\tAccuracy: 0.66\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  6  28   0]\n",
      " [  7 110  10]\n",
      " [  0  20   8]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.668\tKappa: 0.165\tAccuracy: 0.656\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5739999999999998\n",
      "fear\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[28 37  3]\n",
      " [21 97 17]\n",
      " [ 3 29 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.7\tKappa: 0.227\tAccuracy: 0.562\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[25 30  4]\n",
      " [24 98 19]\n",
      " [ 7 32 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.693\tKappa: 0.176\tAccuracy: 0.538\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 23  39   1]\n",
      " [ 12 119  11]\n",
      " [  5  27  15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.72\tKappa: 0.284\tAccuracy: 0.623\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 21  24   1]\n",
      " [ 18 107  14]\n",
      " [  3  37  26]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.713\tKappa: 0.305\tAccuracy: 0.614\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 19  32   1]\n",
      " [ 23 102  17]\n",
      " [  9  29  20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.655\tKappa: 0.212\tAccuracy: 0.56\n",
      "------------------------------------------------------\n",
      "\n",
      "1.5164\n",
      "joy\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[29 20  2]\n",
      " [ 9 70 13]\n",
      " [ 0 17 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.784\tKappa: 0.432\tAccuracy: 0.661\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[21 31  2]\n",
      " [10 63 10]\n",
      " [ 0 20 23]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.742\tKappa: 0.335\tAccuracy: 0.594\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[23 20  0]\n",
      " [18 73 10]\n",
      " [ 0 18 19]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.751\tKappa: 0.362\tAccuracy: 0.635\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[18 16  0]\n",
      " [13 86 10]\n",
      " [ 2 15 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.797\tKappa: 0.421\tAccuracy: 0.689\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[17 19  1]\n",
      " [15 76 12]\n",
      " [ 2 18 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.737\tKappa: 0.343\tAccuracy: 0.63\n",
      "------------------------------------------------------\n",
      "\n",
      "1.7826\n",
      "sadness\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[10 24  3]\n",
      " [ 9 73  8]\n",
      " [ 5 23 17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.72\tKappa: 0.256\tAccuracy: 0.581\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[15 21  0]\n",
      " [20 65 11]\n",
      " [ 3 22 15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.682\tKappa: 0.212\tAccuracy: 0.552\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[16 23  4]\n",
      " [ 9 72  9]\n",
      " [ 4 22 13]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.717\tKappa: 0.273\tAccuracy: 0.587\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[19 28  3]\n",
      " [ 9 65 16]\n",
      " [ 1 17 14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.693\tKappa: 0.259\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[17 26  1]\n",
      " [ 5 66 16]\n",
      " [ 4 17 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.71\tKappa: 0.319\tAccuracy: 0.599\n",
      "------------------------------------------------------\n",
      "\n",
      "1.546\n",
      "Experiment 6 Average scores:\n",
      "\n",
      " Average AUC: 0.711\t Average Kappa: 0.279\t Average Accuracy: 0.615\n"
     ]
    }
   ],
   "source": [
    "experiments = [get_experiment_01_pipeline(),\n",
    "               get_experiment_1_pipeline(),\n",
    "#                get_experiment_3_pipeline(),\n",
    "#                get_experiment_4_pipeline(),               \n",
    "               get_experiment_5_pipeline(),\n",
    "               get_experiment_6_pipeline(),\n",
    "               get_experiment_63_pipeline(),\n",
    "               get_experiment_71_pipeline(),\n",
    "#                get_experiment_10_pipeline(),\n",
    "#                get_experiment_102_pipeline(),\n",
    "#                get_experiment_8_pipeline(),\n",
    "               get_experiment_9_pipeline()]\n",
    "# experiments = [get_experiment_0_pipeline(),\n",
    "#                get_experiment_001_pipeline()]\n",
    "\n",
    "# experiments = [get_experiment_101_pipeline()]\n",
    "\n",
    "best_classifiers, best_scores = run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////////anger////////////\n",
      "///////////1.5739999999999998////////////\n",
      "(Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer()),\n",
      "                                                ('doc2vecmax',\n",
      "                                                 Doc2VecTransformer(aggregation_func=<function amax at 0x7fc5941724c0>,\n",
      "                                                                    model=<gensim.model...rd2VecKeyedVectors object at 0x7fc54a950580>,\n",
      "                                                                    tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42220>)),\n",
      "                                                ('liu',\n",
      "                                                 LiuFeatureExtractor(tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42520>)),\n",
      "                                                ('vader',\n",
      "                                                 VaderFeatureExtractor()),\n",
      "                                                ('doc2vec',\n",
      "                                                 TextBlobTransformer())])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(multi_class='ovr', solver='liblinear'))]), 6)\n",
      "///////////fear////////////\n",
      "///////////1.5661999999999998////////////\n",
      "(Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer()),\n",
      "                                                ('liu',\n",
      "                                                 LiuFeatureExtractor(tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42520>)),\n",
      "                                                ('vader',\n",
      "                                                 VaderFeatureExtractor())])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(multi_class='ovr', solver='liblinear'))]), 5)\n",
      "///////////joy////////////\n",
      "///////////1.7826////////////\n",
      "(Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer()),\n",
      "                                                ('doc2vecmax',\n",
      "                                                 Doc2VecTransformer(aggregation_func=<function amax at 0x7fc5941724c0>,\n",
      "                                                                    model=<gensim.model...rd2VecKeyedVectors object at 0x7fc54a950580>,\n",
      "                                                                    tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42220>)),\n",
      "                                                ('liu',\n",
      "                                                 LiuFeatureExtractor(tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42520>)),\n",
      "                                                ('vader',\n",
      "                                                 VaderFeatureExtractor()),\n",
      "                                                ('doc2vec',\n",
      "                                                 TextBlobTransformer())])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(multi_class='ovr', solver='liblinear'))]), 6)\n",
      "///////////sadness////////////\n",
      "///////////1.546////////////\n",
      "(Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer()),\n",
      "                                                ('doc2vecmax',\n",
      "                                                 Doc2VecTransformer(aggregation_func=<function amax at 0x7fc5941724c0>,\n",
      "                                                                    model=<gensim.model...rd2VecKeyedVectors object at 0x7fc54a950580>,\n",
      "                                                                    tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42220>)),\n",
      "                                                ('liu',\n",
      "                                                 LiuFeatureExtractor(tokenizer=<nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b42520>)),\n",
      "                                                ('vader',\n",
      "                                                 VaderFeatureExtractor()),\n",
      "                                                ('doc2vec',\n",
      "                                                 TextBlobTransformer())])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(multi_class='ovr', solver='liblinear'))]), 6)\n"
     ]
    }
   ],
   "source": [
    "for k in best_classifiers.keys():\n",
    "    print('///////////{}////////////'.format(k))\n",
    "    print('///////////{}////////////'.format(best_scores[k]))\n",
    "    print(best_classifiers[k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  23   0]\n",
      " [ 11 105  15]\n",
      " [  0  16  11]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.627\tKappa: 0.198\tAccuracy: 0.654\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 11  22   1]\n",
      " [  9 105  11]\n",
      " [  0  20   9]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.681\tKappa: 0.245\tAccuracy: 0.665\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  16   0]\n",
      " [  9 105  10]\n",
      " [  1  28  12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.65\tKappa: 0.227\tAccuracy: 0.66\n",
      "------------------------------------------------------\n",
      "\n",
      "188\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 13  21   0]\n",
      " [ 11 110   3]\n",
      " [  0  21   9]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.694\tKappa: 0.313\tAccuracy: 0.702\n",
      "------------------------------------------------------\n",
      "\n",
      "189\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[ 8 31  1]\n",
      " [ 8 97  8]\n",
      " [ 0 24 12]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.717\tKappa: 0.206\tAccuracy: 0.619\n",
      "------------------------------------------------------\n",
      "\n",
      "1257\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 26  31   2]\n",
      " [ 16 109  12]\n",
      " [  2  33  20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.694\tKappa: 0.308\tAccuracy: 0.618\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 22  33   1]\n",
      " [ 25 108  10]\n",
      " [  5  31  16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.689\tKappa: 0.227\tAccuracy: 0.582\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 20  28   1]\n",
      " [ 21 107  15]\n",
      " [  4  32  24]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.724\tKappa: 0.273\tAccuracy: 0.599\n",
      "------------------------------------------------------\n",
      "\n",
      "251\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 25  35   4]\n",
      " [ 18 103  22]\n",
      " [  4  25  15]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.676\tKappa: 0.223\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "252\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 31  27   2]\n",
      " [ 23 100  10]\n",
      " [  2  35  22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.731\tKappa: 0.318\tAccuracy: 0.607\n",
      "------------------------------------------------------\n",
      "\n",
      "902\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[23 24  1]\n",
      " [14 70 12]\n",
      " [ 1 24 11]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.72\tKappa: 0.256\tAccuracy: 0.578\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[20 21  1]\n",
      " [ 8 81  6]\n",
      " [ 0 21 22]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.764\tKappa: 0.439\tAccuracy: 0.683\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[21 14  1]\n",
      " [14 73 16]\n",
      " [ 0 16 26]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.8\tKappa: 0.422\tAccuracy: 0.663\n",
      "------------------------------------------------------\n",
      "\n",
      "180\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[19 20  3]\n",
      " [21 74 12]\n",
      " [ 2 11 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.719\tKappa: 0.324\tAccuracy: 0.617\n",
      "------------------------------------------------------\n",
      "\n",
      "181\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[28 22  1]\n",
      " [10 67 10]\n",
      " [ 1 21 21]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.773\tKappa: 0.407\tAccuracy: 0.641\n",
      "------------------------------------------------------\n",
      "\n",
      "860\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[17 28  1]\n",
      " [12 61 16]\n",
      " [ 5 12 20]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.707\tKappa: 0.279\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 9 20  2]\n",
      " [ 9 78 13]\n",
      " [ 1 24 16]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.641\tKappa: 0.236\tAccuracy: 0.599\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[19 29  1]\n",
      " [12 61 10]\n",
      " [ 5 21 14]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.7\tKappa: 0.238\tAccuracy: 0.547\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[14 24  4]\n",
      " [ 6 83  5]\n",
      " [ 0 19 17]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.726\tKappa: 0.375\tAccuracy: 0.663\n",
      "------------------------------------------------------\n",
      "\n",
      "172\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[13 26  3]\n",
      " [ 8 68 11]\n",
      " [ 2 23 18]]\n",
      "Scores:\n",
      "\n",
      "AUC:  0.689\tKappa: 0.264\tAccuracy: 0.576\n",
      "------------------------------------------------------\n",
      "\n",
      "Average scores:\n",
      "\n",
      " Average AUC: 0.706\t Average Kappa: 0.289\t Average Accuracy: 0.621\n"
     ]
    }
   ],
   "source": [
    "# pipeline = get_experiment_0_pipeline()\n",
    "\n",
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "scores_array = []\n",
    "\n",
    "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "for dataset_name, dataset in train.items():\n",
    "    pipeline, n = best_classifiers[dataset_name]\n",
    "\n",
    "    # ejecutamos el pipeline sobre el dataset\n",
    "    classifier, learned_labels, scores = run2(dataset, dataset_name, pipeline)\n",
    "\n",
    "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # guardamos las labels aprendidas por el clasificador\n",
    "    learned_labels_array.append(learned_labels)\n",
    "\n",
    "    # guardamos los scores obtenidos\n",
    "    scores_array.append(scores)\n",
    "\n",
    "# print avg scores\n",
    "print(\"Average scores:\\n\\n\",\n",
    "  \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "  .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    },
    "colab_type": "text",
    "id": "Jp3JB-NYXK2q"
   },
   "source": [
    "### Predecir los target set y crear la submission\n",
    "\n",
    "Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.392097Z",
     "start_time": "2020-04-07T15:44:21.386114Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ggKp_CXhXK2r"
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.588573Z",
     "start_time": "2020-04-07T15:44:21.394094Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GdvtcxN6XK2u",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer())])),\n",
      "                ('clf', MultinomialNB())])\n",
      "1\n",
      "Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer())])),\n",
      "                ('clf', MultinomialNB())])\n",
      "2\n",
      "Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer())])),\n",
      "                ('clf', MultinomialNB())])\n",
      "3\n",
      "Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('bow',\n",
      "                                                 CountVectorizer(ngram_range=(1,\n",
      "                                                                              4),\n",
      "                                                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc526b426d0>>)),\n",
      "                                                ('chars_count',\n",
      "                                                 CharsCountTransformer())])),\n",
      "                ('clf', MultinomialNB())])\n"
     ]
    }
   ],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.exists('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    print(idx)\n",
    "    print(classifiers[idx])\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivos separados. \n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FJ3Ok3aXK2z"
   },
   "source": [
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm7IJKL3XK20"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxqiGgyMs3Ao"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
