{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    },
    "colab_type": "text",
    "id": "PkfIPpmVXK1J"
   },
   "source": [
    "# Tarea 1 NLP : Competencia de Clasificación de Texto\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssXRUJcBXK1L"
   },
   "source": [
    "- **Nombre:** Jou-Hui Ho Ku, Martín Valderrama\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** PibJou\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUUQEnHxXK1M"
   },
   "source": [
    "## Objetivo e Instrucciones:\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificación de tweets según su intensidad de emoción. Específicamente: \n",
    "\n",
    "Tendrán 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deberán crear un clasificador que indique la intensidad de dicha emoción en sus tweets (`low`, `medium`, `high`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVvANhT6XK1N"
   },
   "source": [
    "###  Fecha de Entrega: \n",
    "\n",
    "Por ser anunciada una vez termine el paro. Se publicará la fecha en ucursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:34:38.796217Z",
     "start_time": "2020-04-07T14:34:38.782255Z"
    },
    "colab_type": "text",
    "id": "CG1T7EWnXK1O"
   },
   "source": [
    "### Detalles e instrucciones de la competencia:\n",
    "\n",
    "- La competencia consiste en resolver 4 problemas de clasificación distintos, cada uno de tres clases. Por cada problema deberán crear un clasificador distinto. La evaluación de la competencia se realiza en base a 4 métricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada ítem serán los que ganen.\n",
    "\n",
    "- Para comenzar se les entregará en este notebook el baseline y la estructura del reporte. El baseline es el código que realiza creación de features y clasificación básica. Los puntajes de este serán ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
    "\n",
    "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
    "\n",
    "- **Es requisito entregar el reporte con el código y haber participado en la competencia para ser evaluado.**\n",
    "\n",
    "- Pueden hacer grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y después cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
    "\n",
    "- En total pueden hacer un **máximo de 4 envíos/submissions** (tanto para equipos como para envíos indivuales).\n",
    "\n",
    "- Hagan varios experimentos haciendo cross-validation o evaluación sobre una sub-partición antes de enviar sus predicciones a Codalab. Asegúrense que la distribución de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les será evaluado incorrectamente.\n",
    "\n",
    "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n",
    "\n",
    "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. ¡Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
    "\n",
    "- Todas las dudas escríbanlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUE0Y0f3XK1P"
   },
   "source": [
    "### Reporte\n",
    "\n",
    "Este debe cumplir la siguiente estructura:\n",
    "\n",
    "1.\t**Introducción**: Presentar brevemente el problema a resolver, los métodos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano. Mas abajo encontrarán una lista útil de estos que les podrá ser de utilidad. (1.5 puntos)\n",
    "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificación usados. (0.5 puntos)\n",
    "4.\t**Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación indicando que miden y su interpretación. (0.5 puntos)\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partición de evaluación que deben crear (o pueden usar cross-validation). Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    },
    "colab_type": "text",
    "id": "DuCEFL_aXK1Q"
   },
   "source": [
    "### Baseline\n",
    "\n",
    "Por último, el baseline contiene un código básico que:\n",
    "\n",
    "- Obtiene los dataset.\n",
    "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizadas.\n",
    "    - Transforma los dataset a bag of words (BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook. \n",
    "\n",
    "\n",
    "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendrá mas sentido cuando vean el código)\n",
    "\n",
    "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del el enunciado. Investigar también el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aquí les adjuntamos algunos ejemplos:\n",
    "    -\tWord n-grams.\n",
    "    -\tCharacter n-grams. \n",
    "    -\tPart-of-speech tags.\n",
    "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
    "        - Count the number of positive and negative words within a sentence.\n",
    "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
    "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
    "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
    "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
    "    -\tThe number of words with all characters in uppercase.\n",
    "    -\tThe presence and the number of positive or negative emoticons.\n",
    "    -\tThe number of individual negations.\n",
    "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
    "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
    "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
    "\n",
    "- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:25:19.677190Z",
     "start_time": "2020-04-07T15:25:19.671206Z"
    },
    "colab_type": "text",
    "id": "DdS9xmSWXK1Q"
   },
   "source": [
    "(Pueden eliminar cualquier celda con instrucciones...)\n",
    "\n",
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFhbF8WvXK1R"
   },
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    },
    "colab_type": "text",
    "id": "pmH9j-eYXK1S"
   },
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_d8xLu1XK1T"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    },
    "colab_type": "text",
    "id": "NJ2NBZFSXK1U"
   },
   "source": [
    "## 2. Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    },
    "colab_type": "text",
    "id": "Bl1LMOz6XK1W"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_H6tUAGPXK1Y"
   },
   "source": [
    "## 3. Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1P0yclvXK1Z"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    },
    "colab_type": "text",
    "id": "KkGR_Y9qXK1a"
   },
   "source": [
    "## 4. Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvQGaiqaXK1b"
   },
   "source": [
    "- AUC: ...\n",
    "- Kappa: ...\n",
    "- Accuracy: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBu_sP0KXK1b"
   },
   "source": [
    "## 5. Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGBQNys8XK1c"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    },
    "colab_type": "text",
    "id": "ul8vukH2XK1d"
   },
   "source": [
    "### Importar librerías y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.587160Z",
     "start_time": "2020-04-07T15:44:19.319386Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "68Lt-v5IXK1e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# !pip install -U emojis\n",
    "# !pip install emosent-py\n",
    "# !pip install emoji --upgrade\n",
    "import emoji\n",
    "import emojis\n",
    "from emosent import get_emoji_sentiment_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPtqQdif8BC3"
   },
   "outputs": [],
   "source": [
    "negative_words = pd.read_csv('https://raw.githubusercontent.com/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/master/data_t1/opinion-lexicon-English/negative-words.csv',\n",
    "                             sep='\\n', header=None, error_bad_lines=False)\n",
    "positive_words = pd.read_csv('https://raw.githubusercontent.com/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/master/data_t1/opinion-lexicon-English/positive-words.csv',\n",
    "                             sep='\\n', header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = np.hstack((list(positive_words),\n",
    "                            'hugging_face',\n",
    "                            'popcorn'))\n",
    "negative_words = np.hstack((list(negative_words),\n",
    "                            'spider',\n",
    "                            'face_with_head-bandage',\n",
    "                            'upside-down_face',\n",
    "                            'face_with_rolling_eyes',\n",
    "                            'zipper-mouth_face',\n",
    "                            'slightly_frowning_face'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sib6XRh6XK1u"
   },
   "source": [
    "### Definir métodos de evaluación\n",
    "\n",
    "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.604066Z",
     "start_time": "2020-04-07T15:44:20.589106Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "N4WVlJ3UXK1v"
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C622AbJeXK1z"
   },
   "source": [
    "### Datos\n",
    "\n",
    "Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.068137Z",
     "start_time": "2020-04-07T15:44:20.606061Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rne8BE61XK10"
   },
   "outputs": [],
   "source": [
    "# Datasets de entrenamiento.\n",
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "# Datasets que deberán predecir para la competencia.\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.088707Z",
     "start_time": "2020-04-07T15:44:21.069757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "X5x06HPdXK14",
    "outputId": "bfbf69ca-c292-4998-f5c3-85c92da0c52f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>30295</td>\n",
       "      <td>Use your smile to change the world. Don't let ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>30506</td>\n",
       "      <td>Twitter is a font of endless hilarity.</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>30256</td>\n",
       "      <td>@LFScott57 Miss Cookie sends her thanks! She's...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30015</td>\n",
       "      <td>@iamTinaDatta love you so much #smile 😊😊</td>\n",
       "      <td>joy</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>30755</td>\n",
       "      <td>@rubeseatsinfo from here, it's but a short ste...</td>\n",
       "      <td>joy</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>30600</td>\n",
       "      <td>Lil reminder that my private account is a deli...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>30408</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @__.mi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>30788</td>\n",
       "      <td>You don't smile and you say you're in love, bu...</td>\n",
       "      <td>joy</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>30714</td>\n",
       "      <td>@EngrNabeeel then rejoicing now is a lil' bit ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>30890</td>\n",
       "      <td>A lifetime of laughter at the expense of the d...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet class  \\\n",
       "295  30295  Use your smile to change the world. Don't let ...   joy   \n",
       "506  30506             Twitter is a font of endless hilarity.   joy   \n",
       "256  30256  @LFScott57 Miss Cookie sends her thanks! She's...   joy   \n",
       "15   30015           @iamTinaDatta love you so much #smile 😊😊   joy   \n",
       "755  30755  @rubeseatsinfo from here, it's but a short ste...   joy   \n",
       "600  30600  Lil reminder that my private account is a deli...   joy   \n",
       "408  30408  Watch this amazing live.ly broadcast by @__.mi...   joy   \n",
       "788  30788  You don't smile and you say you're in love, bu...   joy   \n",
       "714  30714  @EngrNabeeel then rejoicing now is a lil' bit ...   joy   \n",
       "890  30890  A lifetime of laughter at the expense of the d...   joy   \n",
       "\n",
       "    sentiment_intensity  \n",
       "295              medium  \n",
       "506              medium  \n",
       "256              medium  \n",
       "15                 high  \n",
       "755                 low  \n",
       "600              medium  \n",
       "408              medium  \n",
       "788                 low  \n",
       "714                 low  \n",
       "890              medium  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias:\n",
    "train['joy'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBfMKfpPXK17"
   },
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento. Noten que las clases están desbalanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.117633Z",
     "start_time": "2020-04-07T15:44:21.090703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "ZNibpIBYXK18",
    "outputId": "fdf38c5b-34d5-43fe-848a-2fc549353d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count(),\n",
    "          '\\n---------------------------------------\\n')\n",
    "for dataset_name in train:\n",
    "    get_group_dist(dataset_name, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZY7M2bd7XK2B"
   },
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEwpE-Q1XK2B"
   },
   "outputs": [],
   "source": [
    "train_data = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RW6ZJUt8XK2J"
   },
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "for group_name in train.keys():\n",
    "    df = train[group_name]\n",
    "    \n",
    "    max_size = df.groupby('sentiment_intensity').count()['class'].max()\n",
    "    lst = [df]\n",
    "    for class_index, group in df.groupby('sentiment_intensity'):\n",
    "        oversampled = group.sample(max_size-len(group), replace=True)     \n",
    "        lst.append(oversampled)\n",
    "    frame_new = pd.concat(lst)\n",
    "    \n",
    "    train_data[group_name] = frame_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "dpzXG_tQXK2N",
    "outputId": "b1eef3e7-bcd9-4931-d299-f8eb061e474a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 617    617    617\n",
      "low                  617    617    617\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 699    699    699\n",
      "low                  699    699    699\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 488    488    488\n",
      "low                  488    488    488\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 453    453    453\n",
      "low                  453    453    453\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in train_data:\n",
    "    get_group_dist(dataset_name, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0DXme6NXK2Q"
   },
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta carácteres relevantes ('!', '?', '#') en los tweets.\n",
    "2. Definios una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.128600Z",
     "start_time": "2020-04-07T15:44:21.119624Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CrNM4IewXK2R"
   },
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count('#')\n",
    "        num_exclamations = tweet.count('!')\n",
    "        num_interrogations = tweet.count('?')\n",
    "        uppercases_rate = sum(1 for c in tweet if c.isupper())/len(tweet)      \n",
    "        \n",
    "        return [num_hashtags, num_exclamations, num_interrogations, uppercases_rate]\n",
    "    \n",
    "    def get_emojis(self, tweet):\n",
    "        emojis_list = [c for c in tweet if c in emoji.UNICODE_EMOJI]\n",
    "        positive, negative, neutral, sent_score, type_pos, type_neg = 0, 0, 0, 0, 0, 0\n",
    "        no_pesca = 0\n",
    "        for emoj in emojis_list:\n",
    "            try:\n",
    "                em = get_emoji_sentiment_rank(emoj)\n",
    "#                 positive += em['positive']\n",
    "#                 negative += em['negative']\n",
    "#                 neutral += em['neutral']\n",
    "                sent_score += abs(em['sentiment_score'])\n",
    "                if em['sentiment_score']>0:\n",
    "                    type_pos += 1\n",
    "                else:\n",
    "                    type_neg += 1\n",
    "                    \n",
    "            # if get_emoji_sentiment_rank doesn't recognize the emoji\n",
    "            except:  \n",
    "                emoji_name = emoji.UNICODE_EMOJI.get(emoj)[1:-1]\n",
    "                if emoji_name in positive_words:\n",
    "                    type_pos += 1\n",
    "                    continue\n",
    "                elif emoji_name in negative_words:\n",
    "                    type_neg += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # Evaluate by words\n",
    "                    for idx, w in enumerate(emoji_name.split('_')):\n",
    "                        if w in positive_words:\n",
    "                            type_pos += 1\n",
    "                            break\n",
    "                        elif w in negative_words:\n",
    "                            type_neg += 1\n",
    "                            break\n",
    "        # Igual sigue trucho, porque solo considera la informacion de si el emoji es positivo o negativo\n",
    "        # pero el emoji va a tener sent_score = 0, no seria tan comparable con los emojis que \n",
    "        # sí eran compatibles y no necesitaron esta trucheria, por lo que sí tienen sent_score\n",
    "        return [len(emojis_list), sent_score, type_pos, type_neg]\n",
    "\n",
    "    def get_positive_words(self, tweet):\n",
    "        count = sum([w in positive_words for w in tweet.split(' ')])\n",
    "        return count\n",
    "\n",
    "    def get_negative_words(self, tweet):\n",
    "        count = sum([w in negative_words for w in tweet.split(' ')])\n",
    "        return count   \n",
    "    \n",
    "    def neg_transform(self, X):\n",
    "        converted = []\n",
    "        for tweet in X:\n",
    "            neg = mark_negation(tweet.split(' '))\n",
    "            sentence = ' '.join(word for word in neg)\n",
    "            converted.append(sentence)\n",
    "        return converted\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        X_neg = self.neg_transform(X)\n",
    "        for tweet in X_neg:\n",
    "            features = np.hstack((self.get_relevant_chars(tweet),\n",
    "                                  self.get_positive_words(tweet),\n",
    "                                  self.get_negative_words(tweet),\n",
    "                                  self.get_emojis(tweet)))\n",
    "            chars.append(features)\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwTokenizer(TweetTokenizer, TfidfVectorizer):\n",
    "    def transform(self, X, y=None):\n",
    "        tokens_list = [self.tokenize(tweet) for tweet in X]\n",
    "        vectorized = TfidfVectorizer().fit_transform(np.array(tokens_list))\n",
    "        return np.array(tokens_list)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola bb como estas', 'hola ke tal bb']\n",
      "  (0, 2)\t0.5761523551647353\n",
      "  (0, 1)\t0.5761523551647353\n",
      "  (0, 0)\t0.40993714596036396\n",
      "  (0, 3)\t0.40993714596036396\n",
      "  (1, 5)\t0.5761523551647353\n",
      "  (1, 4)\t0.5761523551647353\n",
      "  (1, 0)\t0.40993714596036396\n",
      "  (1, 3)\t0.40993714596036396\n"
     ]
    }
   ],
   "source": [
    "# a = train['joy'].sample(10)['tweet'].values\n",
    "a = ['hola bb como estas','hola ke tal bb']\n",
    "print(a)\n",
    "print(TfidfVectorizer(tokenizer=TweetTokenizer().tokenize).fit_transform(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bb', 'como', 'estas', 'hola', 'ke', 'tal']\n",
      "  (0, 2)\t0.5761523551647353\n",
      "  (0, 1)\t0.5761523551647353\n",
      "  (0, 0)\t0.40993714596036396\n",
      "  (0, 3)\t0.40993714596036396\n",
      "  (1, 5)\t0.5761523551647353\n",
      "  (1, 4)\t0.5761523551647353\n",
      "  (1, 0)\t0.40993714596036396\n",
      "  (1, 3)\t0.40993714596036396\n"
     ]
    }
   ],
   "source": [
    "ti = TfidfVectorizer()\n",
    "x = ti.fit_transform(a)\n",
    "print(ti.get_feature_names())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.145564Z",
     "start_time": "2020-04-07T15:44:21.131593Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "fxw0rq1bXK2V",
    "outputId": "b1bed430-709a-4feb-cd2a-bae1f1ff0b22",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "# sample = train['joy'].sample(200).tweet\n",
    "# sample = train['fear'].tweet\n",
    "# test = pd.DataFrame(zip(sample, TwTokenizer().transform(sample)))\n",
    "# print(test.loc[0][1].mean())\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4mLu9iYXK2Z"
   },
   "source": [
    "### Definir la representación y el clasificador\n",
    "\n",
    "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n",
    "\n",
    "El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
    "\n",
    "```python\n",
    "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                        ('chars_count',CharsCountTransformer())])),\n",
    "              ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6zo8kmUXK2a"
   },
   "source": [
    "Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "F_qyFk1JXK2b"
   },
   "outputs": [],
   "source": [
    "def get_experiment_0_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KNlXOW4IXK2e"
   },
   "outputs": [],
   "source": [
    "def get_experiment_1_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tk', TwTokenizer()),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_2_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_3_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize))])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_4_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', TfidfVectorizer())])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_5_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer(tokenizer=TweetTokenizer().tokenize))])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOQekJanXK2h"
   },
   "source": [
    "### Ejecutar el pipeline para algún dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.167498Z",
     "start_time": "2020-04-07T15:44:21.157540Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V3gMwCchXK2i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name, pipeline):\n",
    "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
    "\n",
    "    # Dividimos el dataset en train y test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "\n",
    "    # Evaluamos:\n",
    "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFZBa7emXK2l"
   },
   "source": [
    "### Ejecutar el sistema creado por cada train set\n",
    "\n",
    "Este código crea y entrena los 4 sistemas de clasificación y luego los evalua. Para los experimentos, pueden copiar este código variando el pipeline cuantas veces estimen conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.384119Z",
     "start_time": "2020-04-07T15:44:21.170488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GOIXPM_RXK2l",
    "outputId": "b3daaa37-d298-4c4a-d226-de08654bfb33",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[181  15   0]\n",
      " [ 10 185  26]\n",
      " [  0  10 184]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.95      0.92      0.94       196\n",
      "      medium       0.88      0.84      0.86       221\n",
      "        high       0.88      0.95      0.91       194\n",
      "\n",
      "    accuracy                           0.90       611\n",
      "   macro avg       0.90      0.90      0.90       611\n",
      "weighted avg       0.90      0.90      0.90       611\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.968\tKappa: 0.85\tAccuracy: 0.9\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[155  63   5]\n",
      " [ 27 172  31]\n",
      " [  2  53 185]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.84      0.70      0.76       223\n",
      "      medium       0.60      0.75      0.66       230\n",
      "        high       0.84      0.77      0.80       240\n",
      "\n",
      "    accuracy                           0.74       693\n",
      "   macro avg       0.76      0.74      0.74       693\n",
      "weighted avg       0.76      0.74      0.74       693\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.894\tKappa: 0.608\tAccuracy: 0.739\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[124  29   4]\n",
      " [ 19 130  17]\n",
      " [  0  20 141]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.87      0.79      0.83       157\n",
      "      medium       0.73      0.78      0.75       166\n",
      "        high       0.87      0.88      0.87       161\n",
      "\n",
      "    accuracy                           0.82       484\n",
      "   macro avg       0.82      0.82      0.82       484\n",
      "weighted avg       0.82      0.82      0.82       484\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.939\tKappa: 0.724\tAccuracy: 0.816\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[119  36   0]\n",
      " [ 22 121  10]\n",
      " [  3  23 115]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.83      0.77      0.80       155\n",
      "      medium       0.67      0.79      0.73       153\n",
      "        high       0.92      0.82      0.86       141\n",
      "\n",
      "    accuracy                           0.79       449\n",
      "   macro avg       0.81      0.79      0.80       449\n",
      "weighted avg       0.80      0.79      0.79       449\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.913\tKappa: 0.685\tAccuracy: 0.791\n",
      "------------------------------------------------------\n",
      "\n",
      "Experiment 0 Average scores:\n",
      "\n",
      " Average AUC: 0.929\t Average Kappa: 0.717\t Average Accuracy: 0.811\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[194  11   0]\n",
      " [ 29 146  24]\n",
      " [  1  26 180]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.87      0.95      0.90       205\n",
      "      medium       0.80      0.73      0.76       199\n",
      "        high       0.88      0.87      0.88       207\n",
      "\n",
      "    accuracy                           0.85       611\n",
      "   macro avg       0.85      0.85      0.85       611\n",
      "weighted avg       0.85      0.85      0.85       611\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.958\tKappa: 0.776\tAccuracy: 0.851\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[178  53   6]\n",
      " [ 47 150  24]\n",
      " [  9  32 194]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.76      0.75      0.76       237\n",
      "      medium       0.64      0.68      0.66       221\n",
      "        high       0.87      0.83      0.85       235\n",
      "\n",
      "    accuracy                           0.75       693\n",
      "   macro avg       0.76      0.75      0.75       693\n",
      "weighted avg       0.76      0.75      0.75       693\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.893\tKappa: 0.63\tAccuracy: 0.753\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[126  28   4]\n",
      " [ 17 125  23]\n",
      " [  0  31 130]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.88      0.80      0.84       158\n",
      "      medium       0.68      0.76      0.72       165\n",
      "        high       0.83      0.81      0.82       161\n",
      "\n",
      "    accuracy                           0.79       484\n",
      "   macro avg       0.80      0.79      0.79       484\n",
      "weighted avg       0.79      0.79      0.79       484\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.917\tKappa: 0.68\tAccuracy: 0.787\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[119  24  12]\n",
      " [ 25  82  40]\n",
      " [  5  22 120]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.80      0.77      0.78       155\n",
      "      medium       0.64      0.56      0.60       147\n",
      "        high       0.70      0.82      0.75       147\n",
      "\n",
      "    accuracy                           0.71       449\n",
      "   macro avg       0.71      0.71      0.71       449\n",
      "weighted avg       0.71      0.71      0.71       449\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.885\tKappa: 0.572\tAccuracy: 0.715\n",
      "------------------------------------------------------\n",
      "\n",
      "Experiment 1 Average scores:\n",
      "\n",
      " Average AUC: 0.913\t Average Kappa: 0.665\t Average Accuracy: 0.776\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[189  23   0]\n",
      " [ 10 187  13]\n",
      " [  0  22 167]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.95      0.89      0.92       212\n",
      "      medium       0.81      0.89      0.85       210\n",
      "        high       0.93      0.88      0.91       189\n",
      "\n",
      "    accuracy                           0.89       611\n",
      "   macro avg       0.89      0.89      0.89       611\n",
      "weighted avg       0.89      0.89      0.89       611\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.961\tKappa: 0.833\tAccuracy: 0.889\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[172  51   3]\n",
      " [ 29 182  20]\n",
      " [ 10  30 196]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.82      0.76      0.79       226\n",
      "      medium       0.69      0.79      0.74       231\n",
      "        high       0.89      0.83      0.86       236\n",
      "\n",
      "    accuracy                           0.79       693\n",
      "   macro avg       0.80      0.79      0.80       693\n",
      "weighted avg       0.80      0.79      0.80       693\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.917\tKappa: 0.69\tAccuracy: 0.794\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[126  34   0]\n",
      " [ 17 121  17]\n",
      " [  2  23 144]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.87      0.79      0.83       160\n",
      "      medium       0.68      0.78      0.73       155\n",
      "        high       0.89      0.85      0.87       169\n",
      "\n",
      "    accuracy                           0.81       484\n",
      "   macro avg       0.81      0.81      0.81       484\n",
      "weighted avg       0.82      0.81      0.81       484\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.926\tKappa: 0.712\tAccuracy: 0.808\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[115  35   2]\n",
      " [ 13 114  12]\n",
      " [  0  26 132]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.90      0.76      0.82       152\n",
      "      medium       0.65      0.82      0.73       139\n",
      "        high       0.90      0.84      0.87       158\n",
      "\n",
      "    accuracy                           0.80       449\n",
      "   macro avg       0.82      0.80      0.81       449\n",
      "weighted avg       0.82      0.80      0.81       449\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.928\tKappa: 0.707\tAccuracy: 0.804\n",
      "------------------------------------------------------\n",
      "\n",
      "Experiment 2 Average scores:\n",
      "\n",
      " Average AUC: 0.933\t Average Kappa: 0.735\t Average Accuracy: 0.824\n"
     ]
    }
   ],
   "source": [
    "experiments = [get_experiment_3_pipeline(),\n",
    "               get_experiment_5_pipeline(),\n",
    "               get_experiment_4_pipeline()]\n",
    "for n, pipeline in enumerate(experiments):\n",
    "    classifiers = []\n",
    "    learned_labels_array = []\n",
    "    scores_array = []\n",
    "\n",
    "    # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "    for dataset_name, dataset in train_data.items():\n",
    "\n",
    "        # creamos el pipeline\n",
    "#         pipeline = get_experiment_1_pipeline()\n",
    "\n",
    "        # ejecutamos el pipeline sobre el dataset\n",
    "        classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
    "\n",
    "        # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "        # guardamos las labels aprendidas por el clasificador\n",
    "        learned_labels_array.append(learned_labels)\n",
    "\n",
    "        # guardamos los scores obtenidos\n",
    "        scores_array.append(scores)\n",
    "\n",
    "    # print avg scores\n",
    "    print('Experiment', n,\n",
    "      \"Average scores:\\n\\n\",\n",
    "      \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "      .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    },
    "colab_type": "text",
    "id": "Jp3JB-NYXK2q"
   },
   "source": [
    "### Predecir los target set y crear la submission\n",
    "\n",
    "Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.392097Z",
     "start_time": "2020-04-07T15:44:21.386114Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ggKp_CXhXK2r"
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.588573Z",
     "start_time": "2020-04-07T15:44:21.394094Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GdvtcxN6XK2u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.exists('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivos separados. \n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FJ3Ok3aXK2z"
   },
   "source": [
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm7IJKL3XK20"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxqiGgyMs3Ao"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
