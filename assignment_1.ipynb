{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    },
    "colab_type": "text",
    "id": "PkfIPpmVXK1J"
   },
   "source": [
    "# Tarea 1 NLP : Competencia de Clasificaci√≥n de Texto\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssXRUJcBXK1L"
   },
   "source": [
    "- **Nombre:** Jou-Hui Ho Ku, Mart√≠n Valderrama\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** PibJou\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUUQEnHxXK1M"
   },
   "source": [
    "## Objetivo e Instrucciones:\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificaci√≥n de tweets seg√∫n su intensidad de emoci√≥n. Espec√≠ficamente: \n",
    "\n",
    "Tendr√°n 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deber√°n crear un clasificador que indique la intensidad de dicha emoci√≥n en sus tweets (`low`, `medium`, `high`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVvANhT6XK1N"
   },
   "source": [
    "###  Fecha de Entrega: \n",
    "\n",
    "Por ser anunciada una vez termine el paro. Se publicar√° la fecha en ucursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:34:38.796217Z",
     "start_time": "2020-04-07T14:34:38.782255Z"
    },
    "colab_type": "text",
    "id": "CG1T7EWnXK1O"
   },
   "source": [
    "### Detalles e instrucciones de la competencia:\n",
    "\n",
    "- La competencia consiste en resolver 4 problemas de clasificaci√≥n distintos, cada uno de tres clases. Por cada problema deber√°n crear un clasificador distinto. La evaluaci√≥n de la competencia se realiza en base a 4 m√©tricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada √≠tem ser√°n los que ganen.\n",
    "\n",
    "- Para comenzar se les entregar√° en este notebook el baseline y la estructura del reporte. El baseline es el c√≥digo que realiza creaci√≥n de features y clasificaci√≥n b√°sica. Los puntajes de este ser√°n ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
    "\n",
    "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
    "\n",
    "- **Es requisito entregar el reporte con el c√≥digo y haber participado en la competencia para ser evaluado.**\n",
    "\n",
    "- Pueden hacer grupos de m√°ximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y despu√©s cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
    "\n",
    "- En total pueden hacer un **m√°ximo de 4 env√≠os/submissions** (tanto para equipos como para env√≠os indivuales).\n",
    "\n",
    "- Hagan varios experimentos haciendo cross-validation o evaluaci√≥n sobre una sub-partici√≥n antes de enviar sus predicciones a Codalab. Aseg√∫rense que la distribuci√≥n de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les ser√° evaluado incorrectamente.\n",
    "\n",
    "- Estar top 5 en alguna m√©trica equivale a 1 punto extra en la nota final.\n",
    "\n",
    "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. ¬°Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
    "\n",
    "- Todas las dudas escr√≠banlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente ser√°n remitidos a ese medio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUE0Y0f3XK1P"
   },
   "source": [
    "### Reporte\n",
    "\n",
    "Este debe cumplir la siguiente estructura:\n",
    "\n",
    "1.\t**Introducci√≥n**: Presentar brevemente el problema a resolver, los m√©todos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci√≥n agregando m√°s atributos y representaciones dise√±adas a mano. Mas abajo encontrar√°n una lista √∫til de estos que les podr√° ser de utilidad. (1.5 puntos)\n",
    "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificaci√≥n usados. (0.5 puntos)\n",
    "4.\t**M√©tricas de evaluaci√≥n**: Describir brevemente las m√©tricas utilizadas en la evaluaci√≥n indicando que miden y su interpretaci√≥n. (0.5 puntos)\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partici√≥n de evaluaci√≥n que deben crear (o pueden usar cross-validation). Incluyan todo el c√≥digo de sus experimentos aqu√≠. ¬°Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    },
    "colab_type": "text",
    "id": "DuCEFL_aXK1Q"
   },
   "source": [
    "### Baseline\n",
    "\n",
    "Por √∫ltimo, el baseline contiene un c√≥digo b√°sico que:\n",
    "\n",
    "- Obtiene los dataset.\n",
    "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizadas.\n",
    "    - Transforma los dataset a bag of words (BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se est√° ejecutando el notebook. \n",
    "\n",
    "\n",
    "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendr√° mas sentido cuando vean el c√≥digo)\n",
    "\n",
    "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. Tambi√©n, el par√°metro `ngram_range` (Ojo que el clf naive bayes no deber√≠a usarse con n-gramas, ya que rompe el supuesto de independencia). Adem√°s, implementar los atributos que crean √∫tiles desde el listado del el enunciado. Investigar tambi√©n el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la funci√≥n `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aqu√≠ les adjuntamos algunos ejemplos:\n",
    "    -\tWord n-grams.\n",
    "    -\tCharacter n-grams. \n",
    "    -\tPart-of-speech tags.\n",
    "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
    "        - Count the number of positive and negative words within a sentence.\n",
    "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
    "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
    "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
    "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
    "    -\tThe number of words with all characters in uppercase.\n",
    "    -\tThe presence and the number of positive or negative emoticons.\n",
    "    -\tThe number of individual negations.\n",
    "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
    "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
    "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
    "\n",
    "- **Reducci√≥n de dimensionalidad**: Tambi√©n puede serles de ayuda. Referencias [aqu√≠](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por √∫ltimo, pueden encontrar mas referencias de c√≥mo mejorar sus features, el vectorizador y el clasificador [aqu√≠](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:25:19.677190Z",
     "start_time": "2020-04-07T15:25:19.671206Z"
    },
    "colab_type": "text",
    "id": "DdS9xmSWXK1Q"
   },
   "source": [
    "(Pueden eliminar cualquier celda con instrucciones...)\n",
    "\n",
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO ser√°n evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFhbF8WvXK1R"
   },
   "source": [
    "----------------------------------------\n",
    "\n",
    "(borrar este bloque despues)\n",
    "1.\t**Introducci√≥n**: Presentar brevemente el problema a resolver, los m√©todos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci√≥n agregando m√°s atributos y representaciones dise√±adas a mano. Mas abajo encontrar√°n una lista √∫til de estos que les podr√° ser de utilidad. (1.5 puntos)\n",
    "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificaci√≥n usados. (0.5 puntos)\n",
    "4.\t**M√©tricas de evaluaci√≥n**: Describir brevemente las m√©tricas utilizadas en la evaluaci√≥n indicando que miden y su interpretaci√≥n. (0.5 puntos)\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partici√≥n de evaluaci√≥n que deben crear (o pueden usar cross-validation). Incluyan todo el c√≥digo de sus experimentos aqu√≠. ¬°Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    },
    "colab_type": "text",
    "id": "pmH9j-eYXK1S"
   },
   "source": [
    "## 1. Introducci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_d8xLu1XK1T"
   },
   "source": [
    "El problema a abordar en este trabajo es el an√°lisis de sentimientos de tweets. Se tiene un dataset de tweets clasificados por sentimiento (joy, fear, sadness, anger), y en cada sentimiento, a su vez, los tweets se clasifican por intensidad (alto, medio o bajo).\n",
    "\n",
    "Para resolver este problema, se utilizan distintas representaciones de los datos, tales como Bag Of Words, la representaci√≥n Tf-Idf, Word Embeddings, y tambi√©n se extraen caracter√≠sticas relevantes de los textos, como por ejemplo la cantidad de signos de exclamaci√≥n que contengan.\n",
    "\n",
    "Una vez convertidos los textos a su representaci√≥n, se utilizan m√©todos de NaiveBayes y SVM para clasificar el nivel de sentimiento en cada clase.\n",
    "\n",
    "A partir de este trabajo, se concluye que...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    },
    "colab_type": "text",
    "id": "NJ2NBZFSXK1U"
   },
   "source": [
    "## 2. Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    },
    "colab_type": "text",
    "id": "Bl1LMOz6XK1W"
   },
   "source": [
    "Se implementan distintas representaciones de los textos, mediante las siguientes clases:\n",
    "\n",
    "- **CharCountVectorizer**: \\\n",
    "    Esta representaci√≥n considera una serie de conteos \"manuales\" implementados a continuaci√≥n. Esta incluye:\n",
    "    - **Caracteres relevantes dentro del texto**, tales como:\n",
    "        - Cantidad de signos de exclamaci√≥n\n",
    "        - Cantidad de signos de interrogaci√≥n\n",
    "        - Cantidad de letras repetidas m√°s de tres veces\n",
    "        - Cantidad de letras en may√∫scula\n",
    "    - Cantidad de palabras positivas y palabras negativas, esto se hace con el dataset de Bing Liu.\n",
    "    \n",
    "    Adem√°s, se marcan previamente las palabras que siguen a una negaci√≥n, concatenando el string \\_NEG luego de la palabra, para que sea considerada diferente a la misma palabra, pero en un contexto positivo.\n",
    "    \n",
    "- **CountVectorizer** \\\n",
    "    Consiste en una representaci√≥n de los textos en Bag of Words.\n",
    "    \n",
<<<<<<< HEAD
    "- **TfidfVectorizer** \\\n",
    "    Calcula la matriz Tf-Idf asociada al dataset."
=======
    "- **TfidfVectorizer**\n",
    "    Calcula la matriz Tf-Idf asociada al dataset. El algoritmo funciona sobre un tokenizer el cual por defecto se realiza separando solamente las palabras del tweet en este caso. Se puede utilizar con distintos Tokenizers, en esta tarea se prueba un caso con el TweetTokenizer de la librer√≠a nltk.\n",
    "    \n",
    "- **TweetTokenizer**\n",
    "    Es un tokenizer pre-definido de la librer√≠a nltk, el cual reconoce car√°cteres especiales de los mensajes de Twitter como emojis en s√≠mbolos o puntutaci√≥n, incluso hashtags."
>>>>>>> 76398b1d93fcb3f7d7a60edfd8bf61baa2dc9f34
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_H6tUAGPXK1Y"
   },
   "source": [
    "## 3. Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1P0yclvXK1Z"
   },
   "source": [
<<<<<<< HEAD
    "Los algoritmos utilizados en este trabajo son:\n",
    "\n",
    "- **Naive Bayes**\n",
    "\n",
    "- **SVM**"
=======
    "hoasjdkfhkjdsf"
>>>>>>> jouhui
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    },
    "colab_type": "text",
    "id": "KkGR_Y9qXK1a"
   },
   "source": [
    "## 4. M√©tricas de Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvQGaiqaXK1b"
   },
   "source": [
    "- AUC: es una m√©trica que mide la capacidad del modelo de distinguir entre clases variando el threshold de separaci√≥n, es decir, la probabilidad con la que se considera la pertenencia a una clase. En la imagen inferior obtenida de [towardsdatascience.com](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5), se muestra una separaci√≥n entre las distribuciones de probabilidad de cada clase y la ROC resultante al mover el threshold.\n",
    "\n",
    "<img src=\"./img_asg1/roc_sep_td.png\" alt=\"Drawing\" style=\"width: 450px;\" align=\"left\"/><img src=\"./img_asg1/roc_td.png\" alt=\"Drawing\" style=\"width: 210px;\" align=\"left\"/>\n",
    "\n",
    "En este caso, las distribuciones de probabilidad son dadas por el modelo, y el threshold de separaci√≥n es el que se var√≠a para obtener la ROC y su consecuente AUC. Si el threshold se mueve, es un desplazamiento en la curva ROC. Si el threshold se mueve hacia la izquierda, se aumenta en TPR y FPR, es decir, el m√©todo detecta m√°s f√°cilmente los positivos (o toma todos como positivos) pero dejando pasar muchos falsos positivos, ya que se disminuye la exigencia para ser positivo, resultando en subir en la curva ROC. Si se aumenta la exigencia para detectar reales positivos, se desplaza bajando por la ROC, teniendo menor tasa de reales positivos y por lo tanto, tambi√©n de falsos positivos.\n",
    "\n",
    "En este caso, se tiene que la AUC es 0.7, es decir, el modelo tiene un 70% de probabilidad de distinguir entre reales positivos y falsos positivos por clase. En el caso de multiclases, se realiza una heur√≠stica de uno versus todos para calcular las probabilidades.\n",
    "\n",
    "En este caso, para el c√°lculo del modelo, se realiza un promedio ponderado de las AUC para las clases bajo, medio y alto. Luego, se realiza un promedio de los AUC entre las clases de sentimientos.\n",
    "    \n",
    "- Kappa: El coeficiente Cohen's Kappa es un estad√≠stico de la capacidad del modelo de \"estar de acuerdo\" con otro anotador o clasificador, en este caso, con el anotador de los labels que consideramos reales. Para calcular el coeficiente de Kappa, se ve la matriz de confusi√≥n y se realiza un c√°lculo de haber clasificado correctamente solamente por coincidencia. Para ello se calcula el accuracy del modelo, es decir, la suma de la diagonal sobre la traza de la matriz de confusi√≥n, y se le resta las correctas por coincidencia. Este √∫ltimo, seg√∫n el canal [MINT TMS Tutorials by Christian Hollmann](https://youtu.be/fOR_8gkU3UE), se calcula como la cantidad de positivos seg√∫n el modelo, sobre el total de muestras, por la cantidad de positivos seg√∫n nuestros labels, sobre el total de muestras; m√°s la cantidad de negativos seg√∫n el modelo sobre el total de muestras, por la cantidad de negativos seg√∫n los labels sobre el total de muestras. El coeficiente de Kappa se calcula como:\n",
    "\n",
    "$$K = \\frac{0A - AC}{1 - AC}$$\n",
    "\n",
    "Donde $0A$ es el accuracy obtenido, y $AC$ es la probabilida de haberle acertado por coincidencia. Un coeficiente de Cohen Kappa superior a 0.8 indica un buen rendimiento del modelo sobre haberle acertado por coincidencia. Mientras que un coeficiente inferior o negativo, indica un mal rendimiento del modelo.\n",
    "\n",
    "- Accuracy: El accuracy es el coeficiente de evaluaci√≥n m√°s usado para evaluar a un modelo. Con √©l se calcula el nivel de acuerdo que tiene el modelo con los labels considerados reales, se calcula como la suma de la diagonal de la matriz de confusi√≥n sobre el total de las muestras consideradas en la evaluaci√≥n. Mientras m√°s cercano a 1 quire decir que el modelo se parece m√°s a los labels considerados reales, mientras que un accuracy 0, quiere decir que el modelo no pudo predecir correctamente ninguna etiqueta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBu_sP0KXK1b"
   },
   "source": [
    "## 5. Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGBQNys8XK1c"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    },
    "colab_type": "text",
    "id": "ul8vukH2XK1d"
   },
   "source": [
    "### Importar librer√≠as y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.587160Z",
     "start_time": "2020-04-07T15:44:19.319386Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "68Lt-v5IXK1e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# !pip install -U emojis\n",
    "# !pip install emosent-py\n",
    "# !pip install emoji --upgrade\n",
    "import emoji\n",
    "import emojis\n",
    "from emosent import get_emoji_sentiment_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import string\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPtqQdif8BC3"
   },
   "outputs": [],
   "source": [
    "negative_words = pd.read_csv('https://raw.githubusercontent.com/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/master/data_t1/opinion-lexicon-English/negative-words.csv',\n",
    "                             sep='\\n', header=None, error_bad_lines=False)\n",
    "positive_words = pd.read_csv('https://raw.githubusercontent.com/MartinVIllesca/CC6205-Procesamiento-de-Lenguaje-Natural/master/data_t1/opinion-lexicon-English/positive-words.csv',\n",
    "                             sep='\\n', header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = np.hstack((list(positive_words),\n",
    "                            'hugging_face',\n",
    "                            'popcorn'))\n",
    "negative_words = np.hstack((list(negative_words),\n",
    "                            'spider',\n",
    "                            'face_with_head-bandage',\n",
    "                            'upside-down_face',\n",
    "                            'face_with_rolling_eyes',\n",
    "                            'zipper-mouth_face',\n",
    "                            'slightly_frowning_face'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiar puntuaciones y separar por tokens.\n",
    "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29821 | INFO | loading projection weights from /home/jouhui/gensim-data/glove-twitter-25/glove-twitter-25.gz\n",
      "29821 | INFO | loaded (1193514, 25) matrix from /home/jouhui/gensim-data/glove-twitter-25/glove-twitter-25.gz\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec Model\n",
    "glove_twitter = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "# glove_twitter.most_similar(\"cat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sib6XRh6XK1u"
   },
   "source": [
    "### Definir m√©todos de evaluaci√≥n\n",
    "\n",
    "Estas funciones est√°n a cargo de evaluar los resultados de la tarea. No deber√≠an cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.604066Z",
     "start_time": "2020-04-07T15:44:20.589106Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "N4WVlJ3UXK1v"
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C622AbJeXK1z"
   },
   "source": [
    "### Datos\n",
    "\n",
    "Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.068137Z",
     "start_time": "2020-04-07T15:44:20.606061Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rne8BE61XK10"
   },
   "outputs": [],
   "source": [
    "# Datasets de entrenamiento.\n",
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "# Datasets que deber√°n predecir para la competencia.\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.088707Z",
     "start_time": "2020-04-07T15:44:21.069757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "X5x06HPdXK14",
    "outputId": "bfbf69ca-c292-4998-f5c3-85c92da0c52f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>30069</td>\n",
       "      <td>May your day be filled with #peace#love n #lau...</td>\n",
       "      <td>joy</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>30758</td>\n",
       "      <td>I truly believe in my heart right now that Sat...</td>\n",
       "      <td>joy</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>30544</td>\n",
       "      <td>@veggiesausage that happens...u will be ok tom...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>30124</td>\n",
       "      <td>@ArtyBagger With or without cake seeing your w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>30320</td>\n",
       "      <td>@5ftanomaly aside from that i'm all about plea...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>30878</td>\n",
       "      <td>@airtelindia have some issues with my broadban...</td>\n",
       "      <td>joy</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>30586</td>\n",
       "      <td>My ukulele bag has fallen apart. üòê WELLL AT LE...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>30064</td>\n",
       "      <td>Getting my comedic relief w/ @SofiaVergara dur...</td>\n",
       "      <td>joy</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>30209</td>\n",
       "      <td>@crimpoop my adorable and cheerful daughter\\nA...</td>\n",
       "      <td>joy</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>30794</td>\n",
       "      <td>@aradsliff don't know I'm from nj we are the w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet class  \\\n",
       "69   30069  May your day be filled with #peace#love n #lau...   joy   \n",
       "758  30758  I truly believe in my heart right now that Sat...   joy   \n",
       "544  30544  @veggiesausage that happens...u will be ok tom...   joy   \n",
       "124  30124  @ArtyBagger With or without cake seeing your w...   joy   \n",
       "320  30320  @5ftanomaly aside from that i'm all about plea...   joy   \n",
       "878  30878  @airtelindia have some issues with my broadban...   joy   \n",
       "586  30586  My ukulele bag has fallen apart. üòê WELLL AT LE...   joy   \n",
       "64   30064  Getting my comedic relief w/ @SofiaVergara dur...   joy   \n",
       "209  30209  @crimpoop my adorable and cheerful daughter\\nA...   joy   \n",
       "794  30794  @aradsliff don't know I'm from nj we are the w...   joy   \n",
       "\n",
       "    sentiment_intensity  \n",
       "69                 high  \n",
       "758                 low  \n",
       "544              medium  \n",
       "124                high  \n",
       "320              medium  \n",
       "878                 low  \n",
       "586              medium  \n",
       "64                 high  \n",
       "209              medium  \n",
       "794                 low  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias:\n",
    "train['joy'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBfMKfpPXK17"
   },
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, seg√∫n su intensidad de sentimiento. Noten que las clases est√°n desbalanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.117633Z",
     "start_time": "2020-04-07T15:44:21.090703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "ZNibpIBYXK18",
    "outputId": "fdf38c5b-34d5-43fe-848a-2fc549353d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count(),\n",
    "          '\\n---------------------------------------\\n')\n",
    "for dataset_name in train:\n",
    "    get_group_dist(dataset_name, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZY7M2bd7XK2B"
   },
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RW6ZJUt8XK2J"
   },
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "for group_name in train.keys():\n",
    "    df = train[group_name]\n",
    "    \n",
    "    max_size = df.groupby('sentiment_intensity').count()['class'].max()\n",
    "    lst = [df]\n",
    "    for class_index, group in df.groupby('sentiment_intensity'):\n",
    "        oversampled = group.sample(max_size-len(group), replace=True)     \n",
    "        lst.append(oversampled)\n",
    "    frame_new = pd.concat(lst)\n",
    "    \n",
    "    train_data[group_name] = frame_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "dpzXG_tQXK2N",
    "outputId": "b1eef3e7-bcd9-4931-d299-f8eb061e474a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 617    617    617\n",
      "low                  617    617    617\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 699    699    699\n",
      "low                  699    699    699\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 488    488    488\n",
      "low                  488    488    488\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 453    453    453\n",
      "low                  453    453    453\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in train_data:\n",
    "    get_group_dist(dataset_name, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0DXme6NXK2Q"
   },
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta car√°cteres relevantes ('!', '?', '#') en los tweets.\n",
    "2. Definios una funci√≥n c√≥mo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la funci√≥n `transform` en donde iteramos por cada tweet, llamamos a la funci√≥n que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitar√° el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aqu√≠](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.128600Z",
     "start_time": "2020-04-07T15:44:21.119624Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CrNM4IewXK2R"
   },
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count('#')\n",
    "        num_exclamations = tweet.count('!')\n",
    "        num_interrogations = tweet.count('?')\n",
    "        num_uppercases = sum(1 for c in tweet if c.isupper())  \n",
    "        num_repeated_chars = self.count_repeated_chars(tweet)\n",
    "        \n",
    "        return [num_hashtags, num_exclamations, num_interrogations, num_uppercases, num_repeated_chars]\n",
    "    \n",
    "    def get_emojis(self, tweet):\n",
    "        \"\"\"\n",
    "        Get number, sentiment_score, and polarity of emojis.\n",
    "        \"\"\"\n",
    "        emojis_list = [c for c in tweet if c in emoji.UNICODE_EMOJI]\n",
    "        positive, negative, neutral, sent_score, type_pos, type_neg = 0, 0, 0, 0, 0, 0\n",
    "        no_pesca = 0\n",
    "        for emoj in emojis_list:\n",
    "            try:\n",
    "                em = get_emoji_sentiment_rank(emoj)\n",
    "#                 positive += em['positive']\n",
    "#                 negative += em['negative']\n",
    "#                 neutral += em['neutral']\n",
    "                sent_score += abs(em['sentiment_score'])\n",
    "                if em['sentiment_score']>0:\n",
    "                    type_pos += 1\n",
    "                else:\n",
    "                    type_neg += 1\n",
    "                    \n",
    "            # if get_emoji_sentiment_rank doesn't recognize the emoji\n",
    "            except:  \n",
    "                emoji_name = emoji.UNICODE_EMOJI.get(emoj)[1:-1]\n",
    "                if emoji_name in positive_words:\n",
    "                    type_pos += 1\n",
    "                    continue\n",
    "                elif emoji_name in negative_words:\n",
    "                    type_neg += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # Evaluate by words\n",
    "                    for idx, w in enumerate(emoji_name.split('_')):\n",
    "                        if w in positive_words:\n",
    "                            type_pos += 1\n",
    "                            break\n",
    "                        elif w in negative_words:\n",
    "                            type_neg += 1\n",
    "                            break\n",
    "        # Igual sigue trucho, porque solo considera la informacion de si el emoji es positivo o negativo\n",
    "        # pero el emoji va a tener sent_score = 0, no seria tan comparable con los emojis que \n",
    "        # s√≠ eran compatibles y no necesitaron esta trucheria, por lo que s√≠ tienen sent_score\n",
    "        return [len(emojis_list), sent_score, type_pos, type_neg]\n",
    "\n",
    "    def get_positive_words(self, tweet):\n",
    "        \"\"\"\n",
    "        Number of positive words.\n",
    "        \"\"\"\n",
    "        count = sum([w in positive_words for w in tweet.split(' ')])\n",
    "        return count\n",
    "\n",
    "    def get_negative_words(self, tweet):\n",
    "        \"\"\"\n",
    "        Number of negative words.\n",
    "        \"\"\"\n",
    "        count = sum([w in negative_words for w in tweet.split(' ')])\n",
    "        return count   \n",
    "\n",
    "    def neg_transform(self, X):\n",
    "        \"\"\"\n",
    "        Append '_NEG' to all words after a negation.\n",
    "        \"\"\"\n",
    "        converted = []\n",
    "        for tweet in X:\n",
    "            neg = mark_negation(tweet.split(' '))\n",
    "            sentence = ' '.join(word for word in neg)\n",
    "            converted.append(sentence)\n",
    "        return converted\n",
    "    \n",
    "    def count_repeated_chars(self, tweet):\n",
    "        \"\"\"\n",
    "        Count number of more than twice repeated chars ( >= three times).\n",
    "        \"\"\"\n",
    "        c_prev = tweet[0]\n",
    "        counter = 0\n",
    "        twice = False\n",
    "        for c in tweet[1:]:\n",
    "            if c==c_prev:\n",
    "                if twice:\n",
    "                    counter += 1\n",
    "                twice = True\n",
    "            else:\n",
    "                twice = False\n",
    "            c_prev = c\n",
    "        return counter\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        X_neg = self.neg_transform(X)\n",
    "        for tweet in X_neg:\n",
    "            features = np.hstack((self.get_relevant_chars(tweet),\n",
    "                                  self.get_positive_words(tweet),\n",
    "                                  self.get_negative_words(tweet),\n",
    "                                  self.get_emojis(tweet)))\n",
    "            chars.append(features)\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola bb como estas', 'hola ke tal bb']\n",
      "  (0, 2)\t0.5761523551647353\n",
      "  (0, 1)\t0.5761523551647353\n",
      "  (0, 0)\t0.40993714596036396\n",
      "  (0, 3)\t0.40993714596036396\n",
      "  (1, 5)\t0.5761523551647353\n",
      "  (1, 4)\t0.5761523551647353\n",
      "  (1, 0)\t0.40993714596036396\n",
      "  (1, 3)\t0.40993714596036396\n"
     ]
    }
   ],
   "source": [
    "# a = train['joy'].sample(10)['tweet'].values\n",
    "a = ['hola bb como estas','hola ke tal bb']\n",
    "print(a)\n",
    "print(TfidfVectorizer(tokenizer=TweetTokenizer().tokenize).fit_transform(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.145564Z",
     "start_time": "2020-04-07T15:44:21.131593Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "fxw0rq1bXK2V",
    "outputId": "b1bed430-709a-4feb-cd2a-bae1f1ff0b22",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "# sample = train['joy'].sample(200).tweet\n",
    "# sample = train['fear'].tweet\n",
    "# test = pd.DataFrame(zip(sample, TwTokenizer().transform(sample)))\n",
    "# print(test.loc[0][1].mean())\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer().fit_transform(['hola como estas XD ;) <3 </3 #happy @hey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = 'Hola QUE pasa #ah :)'\n",
    "b = doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, aggregation_func, tokenizer):\n",
    "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
    "        self.model = model.wv \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # indicamos la funci√≥n de agregaci√≥n (np.min, np.max, np.mean, np.sum, ...)\n",
    "        self.aggregation_func = aggregation_func\n",
    "\n",
    "    def simple_tokenizer(self, doc, lower=False):\n",
    "        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y \n",
    "           separa el tweet por espacios.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(doc)\n",
    "        final_tokens = []\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                if t[0]=='#':  # Convert hashtags\n",
    "                    t = t[1:]\n",
    "            except:\n",
    "                print('Except', doc)\n",
    "            if t.lower() not in stopwords:\n",
    "                final_tokens.append(t)\n",
    "        \n",
    "        return final_tokens\n",
    "        \n",
    "#         if lower:\n",
    "#             doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "#         return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        doc_embeddings = []\n",
    "        no_embeddings = False\n",
    "        for doc in X:\n",
    "            # tokenizamos el documento. Se llevan todos los tokens a min√∫scula. \n",
    "            # ojo con esto, ya que puede que tokens con min√∫scula y may√∫scula tengan\n",
    "            # distintas representaciones\n",
    "            tokens = self.simple_tokenizer(doc) \n",
    "#             tokens = self.tokenizer.tokenize(doc)\n",
    "    \n",
    "            selected_wv = []\n",
    "            for token in tokens:\n",
    "                token = token.lower()\n",
    "                if token in self.model.vocab:\n",
    "                    selected_wv.append(self.model[token])\n",
    "                    \n",
    "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo a√±adimos.\n",
    "            if len(selected_wv) > 0:\n",
    "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
    "                doc_embeddings.append(doc_embedding)\n",
    "            # si no, a√±adimos un vector de ceros que represente a ese documento.\n",
    "            else: \n",
    "                if not no_embeddings:\n",
    "                    print('Couldnt find any embedding for tweet:')\n",
    "                    no_embeddings = True\n",
    "                print(doc)\n",
    "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
    "\n",
    "        return np.array(doc_embeddings)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jouhui/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# doc2vec_mean = Doc2VecTransformer(glove_twitter, np.mean)\n",
    "doc2vec_sum = Doc2VecTransformer(glove_twitter, np.sum, TweetTokenizer())\n",
    "doc2vec_max = Doc2VecTransformer(glove_twitter, np.max, TweetTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4mLu9iYXK2Z"
   },
   "source": [
    "### Definir la representaci√≥n y el clasificador\n",
    "\n",
    "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as√≠ nuestra programaci√≥n.\n",
    "\n",
    "El pipeline m√°s b√°sico que podemos hacer es transformar el dataset a Bag of Words y despu√©s usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformaci√≥n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar√° los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
    "\n",
    "```python\n",
    "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                        ('chars_count',CharsCountTransformer())])),\n",
    "              ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6zo8kmUXK2a"
   },
   "source": [
    "Recuerden que cada pipeline representa un sistema de clasificaci√≥n distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podr√≠an solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "F_qyFk1JXK2b"
   },
   "outputs": [],
   "source": [
    "def get_experiment_0_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer(ngram_range=(1, 3))),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_1_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer(tokenizer=TweetTokenizer().tokenize, \n",
    "                                                            ngram_range=(1, 3))),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_2_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer(tokenizer=TweetTokenizer().tokenize,\n",
    "                                                            ngram_range=(1, 3)))])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_3_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize)),\n",
    "                                    ('chars_count', CharsCountTransformer())])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_4_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize))])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_5_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize)),\n",
    "                                    ('doc2vec', doc2vec_sum)])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnMg2nhyuQbJ"
   },
   "outputs": [],
   "source": [
    "def get_experiment_6_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('tf-idf', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize)),\n",
    "                                    ('doc2vec', doc2vec_max)])),\n",
    "                     ('clf', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOQekJanXK2h"
   },
   "source": [
    "### Ejecutar el pipeline para alg√∫n dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.167498Z",
     "start_time": "2020-04-07T15:44:21.157540Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V3gMwCchXK2i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name, pipeline):\n",
    "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
    "\n",
    "    # Dividimos el dataset en train y test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "\n",
    "    # Evaluamos:\n",
    "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFZBa7emXK2l"
   },
   "source": [
    "### Ejecutar el sistema creado por cada train set\n",
    "\n",
    "Este c√≥digo crea y entrena los 4 sistemas de clasificaci√≥n y luego los evalua. Para los experimentos, pueden copiar este c√≥digo variando el pipeline cuantas veces estimen conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.384119Z",
     "start_time": "2020-04-07T15:44:21.170488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GOIXPM_RXK2l",
    "outputId": "b3daaa37-d298-4c4a-d226-de08654bfb33",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldnt find any embedding for tweet:\n",
      "I think they may be \n",
      "I think they may be \n",
      "I can't even right now #bb18 \n",
      "I think they may be \n",
      "I think they may be \n",
      "Couldnt find any embedding for tweet:\n",
      "I think they may be \n",
      "I think they may be \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[146  44  20]\n",
      " [ 59  90  53]\n",
      " [ 28  33 138]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.63      0.70      0.66       210\n",
      "      medium       0.54      0.45      0.49       202\n",
      "        high       0.65      0.69      0.67       199\n",
      "\n",
      "    accuracy                           0.61       611\n",
      "   macro avg       0.61      0.61      0.61       611\n",
      "weighted avg       0.61      0.61      0.61       611\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.794\tKappa: 0.418\tAccuracy: 0.612\n",
      "------------------------------------------------------\n",
      "\n",
      "Couldnt find any embedding for tweet:\n",
      "And here we go again üòì \n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "That goes the \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[159  40  27]\n",
      " [ 79  89  60]\n",
      " [ 32  42 165]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.59      0.70      0.64       226\n",
      "      medium       0.52      0.39      0.45       228\n",
      "        high       0.65      0.69      0.67       239\n",
      "\n",
      "    accuracy                           0.60       693\n",
      "   macro avg       0.59      0.59      0.59       693\n",
      "weighted avg       0.59      0.60      0.59       693\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.78\tKappa: 0.394\tAccuracy: 0.596\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[110  24  12]\n",
      " [ 64  64  37]\n",
      " [ 26  29 118]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.55      0.75      0.64       146\n",
      "      medium       0.55      0.39      0.45       165\n",
      "        high       0.71      0.68      0.69       173\n",
      "\n",
      "    accuracy                           0.60       484\n",
      "   macro avg       0.60      0.61      0.59       484\n",
      "weighted avg       0.60      0.60      0.59       484\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.787\tKappa: 0.408\tAccuracy: 0.603\n",
      "------------------------------------------------------\n",
      "\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[89 41 18]\n",
      " [30 72 54]\n",
      " [18 32 95]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.65      0.60      0.62       148\n",
      "      medium       0.50      0.46      0.48       156\n",
      "        high       0.57      0.66      0.61       145\n",
      "\n",
      "    accuracy                           0.57       449\n",
      "   macro avg       0.57      0.57      0.57       449\n",
      "weighted avg       0.57      0.57      0.57       449\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.756\tKappa: 0.356\tAccuracy: 0.57\n",
      "------------------------------------------------------\n",
      "\n",
      "Experiment 0 Average scores:\n",
      "\n",
      " Average AUC: 0.779\t Average Kappa: 0.394\t Average Accuracy: 0.595\n",
      "Couldnt find any embedding for tweet:\n",
      "I can't even right now #bb18 \n",
      "I think they may be \n",
      "I think they may be \n",
      "I think they may be \n",
      "I think they may be \n",
      "I think they may be \n",
      "Couldnt find any embedding for tweet:\n",
      "I think they may be \n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[134  36  24]\n",
      " [ 55  98  60]\n",
      " [ 10  28 166]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.67      0.69      0.68       194\n",
      "      medium       0.60      0.46      0.52       213\n",
      "        high       0.66      0.81      0.73       204\n",
      "\n",
      "    accuracy                           0.65       611\n",
      "   macro avg       0.65      0.65      0.65       611\n",
      "weighted avg       0.65      0.65      0.64       611\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.813\tKappa: 0.478\tAccuracy: 0.651\n",
      "------------------------------------------------------\n",
      "\n",
      "Couldnt find any embedding for tweet:\n",
      "That goes the \n",
      "That goes the \n",
      "Couldnt find any embedding for tweet:\n",
      "And here we go again üòì \n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[146  65  36]\n",
      " [ 55 121  44]\n",
      " [ 34  48 144]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.62      0.59      0.61       247\n",
      "      medium       0.52      0.55      0.53       220\n",
      "        high       0.64      0.64      0.64       226\n",
      "\n",
      "    accuracy                           0.59       693\n",
      "   macro avg       0.59      0.59      0.59       693\n",
      "weighted avg       0.60      0.59      0.59       693\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.771\tKappa: 0.389\tAccuracy: 0.593\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[116  32  12]\n",
      " [ 48  72  34]\n",
      " [ 12  42 116]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.66      0.72      0.69       160\n",
      "      medium       0.49      0.47      0.48       154\n",
      "        high       0.72      0.68      0.70       170\n",
      "\n",
      "    accuracy                           0.63       484\n",
      "   macro avg       0.62      0.62      0.62       484\n",
      "weighted avg       0.63      0.63      0.63       484\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.809\tKappa: 0.442\tAccuracy: 0.628\n",
      "------------------------------------------------------\n",
      "\n",
      "Couldnt find any embedding for tweet:\n",
      "@chelseafc let them know it's the \n",
      "@chelseafc let them know it's the \n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 95  47  14]\n",
      " [ 43  57  41]\n",
      " [ 10  37 105]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.64      0.61      0.62       156\n",
      "      medium       0.40      0.40      0.40       141\n",
      "        high       0.66      0.69      0.67       152\n",
      "\n",
      "    accuracy                           0.57       449\n",
      "   macro avg       0.57      0.57      0.57       449\n",
      "weighted avg       0.57      0.57      0.57       449\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.766\tKappa: 0.358\tAccuracy: 0.572\n",
      "------------------------------------------------------\n",
      "\n",
      "Experiment 1 Average scores:\n",
      "\n",
      " Average AUC: 0.79\t Average Kappa: 0.417\t Average Accuracy: 0.611\n"
     ]
    }
   ],
   "source": [
    "experiments = [get_experiment_5_pipeline(),\n",
    "               get_experiment_6_pipeline()]\n",
    "for n, pipeline in enumerate(experiments):\n",
    "    classifiers = []\n",
    "    learned_labels_array = []\n",
    "    scores_array = []\n",
    "\n",
    "    # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "    for dataset_name, dataset in train_data.items():\n",
    "\n",
    "        # creamos el pipeline\n",
    "#         pipeline = get_experiment_1_pipeline()\n",
    "\n",
    "        # ejecutamos el pipeline sobre el dataset\n",
    "        classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
    "\n",
    "        # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "        # guardamos las labels aprendidas por el clasificador\n",
    "        learned_labels_array.append(learned_labels)\n",
    "\n",
    "        # guardamos los scores obtenidos\n",
    "        scores_array.append(scores)\n",
    "\n",
    "    # print avg scores\n",
    "    print('Experiment', n,\n",
    "      \"Average scores:\\n\\n\",\n",
    "      \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "      .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    },
    "colab_type": "text",
    "id": "Jp3JB-NYXK2q"
   },
   "source": [
    "### Predecir los target set y crear la submission\n",
    "\n",
    "Aqu√≠ predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.392097Z",
     "start_time": "2020-04-07T15:44:21.386114Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ggKp_CXhXK2r"
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.588573Z",
     "start_time": "2020-04-07T15:44:21.394094Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GdvtcxN6XK2u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.exists('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivos separados. \n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FJ3Ok3aXK2z"
   },
   "source": [
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm7IJKL3XK20"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxqiGgyMs3Ao"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
